{"language":{"0":"Batchfile","1":"R","2":"PHP","3":"Ruby","4":"C++","5":"Java","6":"R","7":"Ruby","8":"C#","9":"Ruby","10":"Ruby","11":"R","12":"Jupyter Notebook","13":"HTML","14":"JavaScript","15":"Java","16":"JavaScript","17":"Python","18":"Python","19":"Java","20":"C#","21":"Shell","22":"C#","23":"Jupyter Notebook","24":"Java","25":"Java","26":"Jupyter Notebook","27":"CSS","28":"PowerShell","29":"C++","30":"Python","31":"Python","32":"Objective-C","33":"Jupyter Notebook","34":"PowerShell","35":"C++","36":"Java","37":"Java","38":"Jupyter Notebook","39":"C#","40":"C","41":"Jupyter Notebook","42":"C#","43":"Java","44":"C#","45":"Java","46":"C++","47":"Java","48":"Java","49":"C#","50":"Python","51":"HTML","52":"C++","53":"Mathematica","54":"JavaScript","55":"Python","56":"MATLAB","57":"Ruby","58":"Java","59":"CSS","60":"C#","61":"Jupyter Notebook","62":"HTML","63":"R","64":"Java","65":"Other","66":"Java","67":"Java","68":"JavaScript","69":"JavaScript","70":"C#","71":"Python","72":"C#","73":"JavaScript","74":"C++","75":"Java","76":"Java","77":"JavaScript","78":"Jupyter Notebook","79":"JavaScript","80":"C++","81":"Jupyter Notebook","82":"Java","83":"JavaScript","84":"Java","85":"C","86":"C","87":"Jupyter Notebook","88":"Java","89":"C#","90":"Python","91":"Lua","92":"HTML","93":"TeX","94":"HTML","95":"HTML","96":"Jupyter Notebook","97":"Python","98":"Swift","99":"MATLAB","100":"JavaScript","101":"C","102":"Python","103":"JavaScript","104":"JavaScript","105":"Python","106":"Java","107":"Java","108":"R","109":"JavaScript","110":"Swift","111":"C","112":"MATLAB","113":"Python","114":"C#","115":"R","116":"Jupyter Notebook","117":"Vim script","118":"C++","119":"JavaScript","120":"Java","121":"Python","122":"JavaScript","123":"Python","124":"Python","125":"JavaScript","126":"Kotlin","127":"C++","128":"Python","129":"Python","130":"Go","131":"Shell","132":"R","133":"Vue","134":"JavaScript","135":"Vim script","136":"Batchfile","137":"R","138":"Python","139":"Python","140":"TeX","141":"Stata","142":"CSS","143":"HTML","144":"Python","145":"C","146":"Ruby","147":"JavaScript","148":"Java","149":"JavaScript","150":"HTML","151":"HTML","152":"JavaScript","153":"JavaScript","154":"Java","155":"Vue","156":"Python","157":"Python","158":"Python","159":"TeX","160":"JavaScript","161":"Python","162":"Python","163":"JavaScript","164":"CSS","165":"HTML","166":"HTML","167":"JavaScript","168":"JavaScript","169":"HTML","170":"HTML","171":"PHP","172":"JavaScript","173":"MATLAB","174":"R","175":"HTML","176":"JavaScript","177":"TeX","178":"C++","179":"C#","180":"HTML","181":"JavaScript","182":"TeX","183":"Jupyter Notebook","184":"TypeScript","185":"HTML","186":"R","187":"C++","188":"HTML","189":"HTML","190":"Mathematica","191":"HTML","192":"Python","193":"R","194":"HTML","195":"CoffeeScript","196":"HTML","197":"PHP","198":"Jupyter Notebook","199":"C++","200":"HTML","201":"TypeScript","202":"C++","203":"PHP","204":"Jupyter Notebook","205":"C++","206":"HTML","207":"TypeScript","208":"JavaScript","209":"TeX","210":"C++"},"content":{"0":"Environmental\nEnvironmental quickly build\n\u4f18\u79c0\u7684\u7a0b\u5e8f\u5458\u4e0d\u4ec5\u4ec5\u53ea\u662f\u4f7f\u7528\u5de5\u5177\n","1":"seraphim \nseraphim is a R package for investigating the impact of environmental factors on the dispersal history and dynamics of viral lineages. The package can also be used to estimate dispersal statistics and mapping phylogeographic trees.\nInstallation\nseraphim can be installed with the devtools package:\ninstall.packages(\"devtools\"); library(devtools)\ninstall_github(\"sdellicour\/seraphim\/unix_OS\")\nReferences\n\nDellicour S, Rose R, Faria N, Lemey P, Pybus OG (2016). SERAPHIM: studying environmental rasters and phylogenetically-informed movements. Bioinformatics 32: 3204-3206.\nDellicour S, Rose R, Pybus OG (2016). Explaining the geographic spread of emerging epidemics: a framework for comparing viral phylogenies and environmental landscape data. BMC Bioinformatics 17: 82.\n\n","2":"PHP dotenv\nA .env file parsing and loading library for PHP.\nAutomatically loads variables into a number of contexts:\n\ngetenv() (default)\n$_ENV (default)\n$_SERVER (default)\napache_getenv (optional)\nPHP constants (optional)\nGlobal variables (optional)\nA custom config array (optional)\n\nWhy?\nYou should never store sensitive credentials in your code. Storing configuration in the environment is one of the tenets of a twelve-factor app. Anything that is likely to change between deployment environments \u2013 such as database credentials or credentials for 3rd party services \u2013 should be extracted from the code into environment variables.\nRequirements\n\nPHP 5.4\n\nInstallation\nUsing Composer, run composer require wpscholar\/phpdotenv.\nMake sure you have a line in your code to handle autoloading:\n<?php\n\nrequire __DIR__ . '\/vendor\/autoload.php';\nUsage\nCreate a new loader and use any of the available methods to help customize your configuration:\n<?php\n\n$loader = new wpscholar\\phpdotenv\\Loader(); \/\/ Can also do wpscholar\\phpdotenv\\Loader::create() \n$loader\n    ->config([ \/\/ Must be used to customize adapters, can also be used to set defaults or required variables.\n        'adapters' => [\n            'apache',   \/\/ Uses apache_setenv() \n            'array',    \/\/ Uses a custom array\n            'define',   \/\/ Uses define() to set PHP constants\n            'env',      \/\/ Uses $_ENV\n            'global',   \/\/ Sets global variables\n            'putenv',   \/\/ Uses putenv()\n            'server'    \/\/ Uses $_SERVER\n        ], \n        'defaults' => [\n            'foo' => 'bar' \/\/ Set a default value if not provided in .env  \t\n        ],\n        'required' => [\n            'bar', \/\/ Require that a variable be defined in the .env file. Throws an exception if not defined.\n            'baz',\n        ],\n    ])\n    ->required([ \/\/ Another way to define required variables\n        'bar',\n        'baz',\n        'quux',    \t\n    ])\n    ->setDefaults([ \/\/ Another way to set defaults\n        'foo' => 'bar',\t\n    ])\n    ->parse([ __DIR__ . '\/.env', dirname( __DIR__ ) . '\/.env' ]) \/\/ Array of file paths to check for a .env file. Parses found file and loads vars into memory.\n    ->set( 'qux', $loader->get('foo') ); \/\/ Override variables after loading, but with access to existing variables before they are loaded into the environment.\n    \n\/\/ Validate variable values after parsing the .env file, but before loading the results into the environment.\n$loader->validate('foo')->notEmpty();\n$loader->validate('bar')->isBoolean();\n$loader->validate('baz')->isInteger();\n$loader->validate('qux')->notEmpty()->allowedValues( [ 'bar', 'baz' ] ); \/\/ Validations can be chained together.\n$loader->validate('quux')->assert(function( $value ) { \/\/ Apply your own custom validation assertions.\n    return is_int($value) && $value > 0 && $value <= 10;\t\n});\n\n\/\/ Call load() to load variables into the environment without overwriting existing variables.\n$loader->load();\n\n\/\/ Call overload() to load variables into the environment, overwriting any existing variables.\n$loader->overload();\nIt is possible to create multiple instances of the loader, each loading a different .env file and loading variables into different contexts.\nCustom Configuration Array Example Usage\n<?php\n\n$loader = wpscholar\\phpdotenv\\Loader::create();\n$loader\n    ->config([ 'adapters' => 'array'] ) \/\/ All values are self-contained in an array within the loader.\n    ->required([ 'bar', 'baz', 'quux', ])\n    ->setDefaults([ 'foo' => 'bar' ])\n    ->parse( __DIR__ . '\/.env' )\n    ->set( 'qux', $loader->get('foo') )\n    ->load();\n\n$config = $loader->all(); \/\/ Get an array containing the final values.\n\n$bar = $loader->get('bar'); \/\/ Get a single value.\nWordPress wp-config.php Example Usage\n<?php\n\nrequire __DIR__ . '\/vendor\/autoload.php';\n\nuse wpscholar\\phpdotenv\\Loader;\n\n$loader = new Loader();\n$loader\n\t->config( [ 'adapters' => 'define' ] ) \/\/ Will only set PHP constants\n\t->required( [ \/\/ Requires these be set in the .env file\n\t\t'DB_NAME',\n\t\t'DB_USER',\n\t\t'DB_PASSWORD',\n\t] )\n\t->setDefaults( [ \/\/ Defaults to use if not defined in .env file\n\t\t'ABSPATH'         => __DIR__ . '\/wp',\n\t\t'DB_CHARSET'      => 'utf8',\n\t\t'DB_COLLATE'      => '',\n\t\t'DB_HOST'         => 'localhost',\n\t\t'WP_DEBUG'        => false,\n\t\t'WP_TABLE_PREFIX' => 'wp_',\n\t] )\n\t->parse( __DIR__ . '\/.env' ) \/\/ Parse the .env file\n\t->set( 'WP_HOME', 'https:\/\/' . $_SERVER['HTTP_HOST'] )\n\t->set( 'WP_SITEURL', $loader->get( 'WP_HOME' ) . '\/wp' ) \/\/ Use previously defined values to set other values.\n\t->set( 'WP_CONTENT_DIR', __DIR__ . '\/content' )\n\t->set( 'WP_CONTENT_URL', $loader->get( 'WP_HOME' ) . '\/content' )\n\t->set( 'DISALLOW_FILE_EDIT', true )\n\t->load(); \/\/ We could use overload() here, but we can't overwrite constants in PHP either way.\n\n$table_prefix = WP_TABLE_PREFIX;\n\nrequire_once( ABSPATH . 'wp-settings.php' );\nCreating a .env File\nSample .env file for the wp-config.php example:\nDB_NAME=local\nDB_USER=root\nDB_PASSWORD=root\nWP_DEBUG=true\nSCRIPT_DEBUG=true\nExplore all the features of the .env file parser.\nRules to Follow\nWhen using phpdotenv, you should strive to follow these rules:\n\nAdd your .env file to a .gitignore file to prevent sensitive data from being committed to the project repository.\nUse a .env.example to set a default configuration for your project. This allows your development team to override defaults in a method that works for their local environment.\nAlways set sane defaults when possible.\nWhere necessary, add comments to credentials with information as to what they are, how they are used, and how one might procure new ones.\nAs phpdotenv uses more lax procedures for defining environment variables, ensure your .env files are compatible with your shell. A good way to test this is to run the following:\n\n# Source in your .env file\nsource .env\n# Check an environmental variable\nfoo\n\nWhen possible, avoid running phpdotenv in production settings. Instead, set environment variables in your webserver, process manager or in bash before your app loads.\n\n","3":"\n\n\nMersea\nRequirements\n\nRuby MRI 2.5.x (rbenv recommended)\nBundler\nRails 5.x\nPostgres 9.5+ configuration file\nImageMagick(for thumbnails)\n\nSetup\nrbenv install\ngem install bundler\nDevelopment\nClone repository.\n# Install and configure db\n$ bundle install\n$ bundle exec rails db:create\n$ bundle exec rails db:migrate\n\n# Add static pages\n$ bundle exec rails db:seed\n\n# Launch app\n$ bundle exec rails s\nIncrease inotify watchers.\nReCaptcha is disabled in development. Configure key if needed using RECAPTCHA_SECRET_KEY and RECAPTCHA_SITE_KEY.\nCreate an admin account\nWithin a Rails console:\nbundle exec rails console\nAdmin.create(name: 'myname', email: 'myemail@email.local', password: 'mypassword')\nAdmin section is reachable at \/admin.\nFrontend\nTo setup frontend, see readme in .\/frontend.\nProduction with Docker\nConfigure your reCaptcha keys as environment variables\n\nStart server via Docker Compose\n\n$ cd \/path\/to\/mersea\n$ docker-compose up -d\n\nFeel free to modifies the provided docker-compose.yml to your needs.\n\nhttp:\/\/localhost:3000\n\nStart a Rails console\n\n# mersea_mersea_1 is the container name defined by docker-compose\n$ docker exec -it mersea_mersea_1 bundle exec rails c\nTo set any environment variable in the container, use one or more -e flags:\n\nJWT_SECRET \u2192 the JWT secret\nMERSEA_NAMESPACE \u2192 namespace the url\nRAILS_SERVE_STATIC_FILES \u2192 the webapp serves all the assets instead of NGINX\nMERSEA_DATABASE_POOL \u2192 database connection pool size\nMERSEA_DATABASE_HOST \u2192 database host (IP address or URL)\nMERSEA_DATABASE_PORT \u2192 database port (by default 5432)\nMERSEA_DATABASE_USERNAME \u2192 database credential\nMERSEA_DATABASE_PASSWORD \u2192 database credential\nRECAPTCHA_SITE_KEY \u2192 Google reCaptcha key\nRECAPTCHA_SECRET_KEY \u2192 Google reCaptcha secret\nBUGSNAG_API_KEY \u2192 Bugsnag key (leave empty to disable error reporting)\n\nLicense\nMIT. See the LICENSE for more details.\nAbout\n\nFor more information about the project checkout the about and information section on oceanplastictracker.com\n\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am 'Add some feature')\nEnsure specs and Rubocop pass\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\nSpecial thanks\nWe would like to thanks the following companies for their open source plans and support\n\n\n\n\n\n\n\n\n\n\nThanks to Bugsnag we can monitor and investigate errors on our application\n\n\n\nThank you to Mapbox for their mapping services and tools\n\n\n\nThank you to BrowserStack for their testing platform. It allows us to seamlessly test our web application on different devices and browser\n\n\n\nThanks to Circleci we can build efficient ci\/cd pipelines\n\n\n\nThanks to all the open source tools we are using to make our application (gemfile, package.json)\n","4":"\nWifi and Bluetooth environmental sensors built with ESP32 dev boards.\nThese firmwares are providing both a WiFi web inteface and a documented BLE API to use with WatchFlower.\nThey use standard libraries and are easy to customize to your own needs.\nProjects\nHiGrow\nCustom firmware for HiGrow ESP32 boards. Check it out!\n\nGeiger Counter\nESP32 board with a \"CAJOE\" Geiger Counter module. Check it out!\n\nAir Quality Monitor\nESP32 board with a handful of sensors for air quality monitoring. Check it out!\nGet involved!\nDevelopers\nYou can browse the code on the GitHub page, submit patches and pull requests! Your help would be greatly appreciated ;-)\nUsers\nYou can help us find and report bugs, suggest new features, help with translation, documentation and more! Visit the Issues section of the GitHub page to start!\n","5":"\nWifi and Bluetooth environmental sensors built with ESP32 dev boards.\nThese firmwares are providing both a WiFi web inteface and a documented BLE API to use with WatchFlower.\nThey use standard libraries and are easy to customize to your own needs.\nProjects\nHiGrow\nCustom firmware for HiGrow ESP32 boards. Check it out!\n\nGeiger Counter\nESP32 board with a \"CAJOE\" Geiger Counter module. Check it out!\n\nAir Quality Monitor\nESP32 board with a handful of sensors for air quality monitoring. Check it out!\nGet involved!\nDevelopers\nYou can browse the code on the GitHub page, submit patches and pull requests! Your help would be greatly appreciated ;-)\nUsers\nYou can help us find and report bugs, suggest new features, help with translation, documentation and more! Visit the Issues section of the GitHub page to start!\n","6":"osd-environmental-data\nScope\nA repository for creating and maintaining environmental data layers for OSD analysis.\nPlease note, this repository only links to or hosts contextual data sets in their rawest form. That is, no manipulations other than those that are absolutely needed for workability are performed and only when needed. Code and\/or documentation related to these manipulations will be recorded here. Please see this repository for information and code related to downstream analysis steps.\nThe Wiki\nDescriptions of the contextual data sets gathered for OSD will be present on this repository's wiki. Please read about the data sets' properties and any assumptions used (e.g. for satellite data or calculated geographic distances).\nCode\nTo be added.\nIssues\nIf you'd like to see what's in our TODO queue check out our issue tracker. You're free to create issues or add comments whereever you feel you can contribute! You'll need a GitHub account to do this, register for one here.\n","7":"\u6982\u8981\nUnix\u74b0\u5883\u3067\u5fc5\u8981\u3068\u306a\u308b\u958b\u767a\u30c4\u30fc\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u30b3\u30f3\u30d1\u30a4\u30eb\u3059\u308b\u3068\u3044\u3046\u8ab0\u5f97\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002\n\u500b\u4eba\u7684\u306b\u65b0\u3057\u3044\u74b0\u5883\u306b\u5f15\u3063\u8d8a\u3057\u3057\u305f\u6642\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002\n\u5185\u5bb9\u7269\n","8":"EnvironmentalDefense\nTuck is not in the haus\n","9":"emitter\nBase module for Brighter Planet's emitters. See the Brighter Planet developer page for details on Brighter Planet's development environment.\nInstallation\n$ gem install emitter\n\nUsage\n# my_emitter.rb\nrequire 'emitter'\n\nclass MyEmitter\n  include BrighterPlanet::Emitter\nend\n\nRequired modules\nYour emitter must define some modules under lib\/my_emitter\/, they are:\n\nlib\/my_emitter\/impact_model.rb - module MyEmitter::ImpactModel - defines the Leap decisions that calculate an impact.\nlib\/my_emitter\/characterization.rb - module MyEmitter::Characterization - defines the Characteristics that describe the model's inputs.\nlib\/my_emitter\/data.rb - module MyEmitter::Data - defines the schema definitions for the emitter and DataMiner processes that fetch and store data for the emitter.\nlib\/my_emitter\/relationships.rb - module MyEmitter::Relationships - defines the ActiveRecord relationships between the emitter and other Earth models.\nlib\/my_emitter\/summarization.rb - moduel MyEmitter::Summarization - defines phrases that describe various attributes (SummaryJudgement descriptors) about an emitter that are displayed on CM1's methodology pages.\n\nTools\nYou can use the bp gem to generate the skeleton for a new emitter.\nMagic\nEmitter, once included, will perform a couple tasks automatically:\n\nIt converts committees into characterizations that are not already characterized.\nIt adds auto_upgrade! and run_data_miner_on_parent_associations! tasks to DataMiner so that a call to MyEmitter.run_data_miner! will update the emitter's schema and run DataMiner tasks on any belongs_to associations.\n\nNote on Patches\/Pull Requests\n\nFork the project.\nMake your feature addition or bug fix.\nAdd tests for it. This is important so I don't break it in a\nfuture version unintentionally.\nCommit, do not mess with rakefile, version, or history.\n(if you want to have your own version, that is fine but bump version in a commit by itself I can ignore when I pull)\nSend me a pull request. Bonus points for topic branches.\n\nCopyright\nCopyright 2010, 2011 Brighter Planet, Inc. See LICENSE and LICENSE-PREAMBLE for details.\n","10":"AppConfiguration\n\n\n\n\nAppConfiguration is a very simple gem that helps you configure your Ruby applications. AppConfiguration uses YAML config files or environmental variales to set\nthe configuration parameters.\nInstallation\nAdd this line to your application's Gemfile:\ngem 'app_configuration'\n\nAnd then execute:\n$ bundle\n\nOr install it yourself as:\n$ gem install app_configuration\n\nUsage\nAppConfiguration comes with great default values. So if you want to setup a new config all you need to do is\nconfig = AppConfiguration.new\nmy_configurable_variable = config.foo\nmy_other_variable = config['bar']\nBy default, when getting the variable foo AppConfiguration will look for the environmental variable FOO.\nIf it cannot find it, AppConfiguration will look for the .config.yml file in the current working directory.\nIf there is no config file there, it will try to find the .config.yml in your home directory.\nA possible .config.yml for this example could look like this\nfoo: 'This is the foo variable'\nbar: 'This is the bar variable'\nCustomize your configuration\nAppConfiguration can be customized to fit your needs. Here is an example\nconfig = AppConfiguration.new('.setup.yml') do\n  base_local_path '\/usr\/local'\n  base_global_path '\/config'\n  use_env_variables true\n  prefix 'my_app'\nend\nYou can set the configuration file name by passing the name to the new method or you can use the config_file_name\nmethod inside the configuration block.\n\nconfig_file_name Sets the name of the config file. Default .config.yml.\nbase_local_path Sets the base path for the local configuration file. If there is no config file in this path it will\nlook in the global configuration path. Default .\/\nbase_global_path Sets the base path for the global configuration file. Default ~\/\nuse_env_variables Flag that activates the use of enviromental variable. Default true\nprefix A prefix to be appended when looking for environmental variables. For example if prefix is set to my_app,\nwhen the foo variable is fetched, the MY_APP_FOO environmental variable will be checked.\nThis is used to avoid name collitions. Default nil\n\nVariable lookup\nYou can retrieve a variable from a AppConfiguration::Config object by doing\nfoo = config.foo\nfoo = config['foo']\nEnvironmental variables will be checked first, adding the necesary prefix if provided. If there is no environmental\nvariable, the local config file will be checked. If there is no local file or a value has not been defined for\nthe given variable, the global config file will be checked. Otherwise it returns nil.\nConfiguration registry\nIf you create a new config object by using AppConfiguration.new, then you must keep the reference to this configuration.\nInstead you can registers a configuration by using AppConfiguration.for. Then you can obtain a configuration by using\nAppConfiguration[]. For example\nAppConfiguration.for :github\n# ... Then somewhere else ...\ngithub = AppConfiguration[:github]\ngithub.api_key\nIn the previous example the name of the configuration file is assumed to be .github.yml and all the environmental variables\nwill be prefixed with GITHUB_. You can change this behaviour by passing a configuration block to the for method.\nFor example if you want to change the local path you can register the configuration as follows\nAppConfiguration.for :github do\n  base_local_path Rails.root\nend\nDefault values\nTo change the default local path and the default global path for all the AppConfiguration::Config objects all you\nneed to do is\nAppConfiguration::Config.default_local_path = Rails.root\nAppConfiguration::Config.default_global_path = '\/usr\/configs'\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am 'Add some feature')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\nPlease add specs for all new features. If you find a bug and an spec probing that the bug exists and in a separate commit\nadd the bug fix.\nLicense\nCopyright (c) 2013 Guido Marucci Blas\nMIT License\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and\/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","11":"IndoPacific-Corals\nAnalysis of Indo-Pacific coral life histories: socio-environmental drivers and strategic management\n","12":"IndoPacific-Corals\nAnalysis of Indo-Pacific coral life histories: socio-environmental drivers and strategic management\n","13":"IndoPacific-Corals\nAnalysis of Indo-Pacific coral life histories: socio-environmental drivers and strategic management\n","14":"Grassberry High is an environmental controller for your home grow project. This repository contains the code for the backend Node.js server. Grassberry High is an open source software project. All contributions and the support is community based.\nIMPORTANT NOTICE \/ PROJECT STATUS\nI realized that I don't have the time and energy to develop this project in my free time further. I will still support the project as a maintainer, with guidance and support. But this project needs active developers and further maintainers! I cannot provide enough time to fix all bugs or add additional features at the moment.\nHelp\nIf you find any bugs, please use the issue tracker.\nIn case you need personal assistance you can either use:\n\nour reddit channel (how to use)\nStackOverflow (coding)\nGit-Start (coding)\n\nYou are missing some feature? There are three ways to create new features:\n\nContribute, read our contribution guidelines\nFind someone to code it for you, (a bounty might help)\nSupport development via the official Patreon Campaign\n\nWe are looking for people to translate documents into other languages!\nUseful information\nI2C Setup\ndetect on raspberry pi:\ni2cdetect -y 1 (alias: checki2c)\n0x20 (32) Relais Controller\n0x21 (33), 0x22 (34) Chirp Water Sensor\n0x40 (64), 0x43 (67) HDC1000\n0x4d (77) MHZ16\nDevelopment Slack\nDrop an email to hello <> grassberry-high.com with the title Invite me to Slack\nto get invited to the Slack channel.\nRun\nOn the Pi3\nFollow the instructions here:\nBuild your own tutorial\nShort Version\n\nPlugin all sensors & controllers into the i2c bus and add the power supply\nConnect to gh-config wifi hotspot\nEnter http:\/\/grassberry.local\nEnter your wifi credentials into the configuration, let the device reboot automatically\nDone\n\nConfiguration  Variables\nBasic\n\nNODE_ENV: sets the environment (devlopment, test, production)\n\nApi\n\nAPI_TOKEN: bearer token for api access, for future use\n\nDatabase\n\nMONGODB_URL: url to the database\nMONGODB_ADMIN: admin db\n\nSimulation\n\nSEED: automatically seed diff. collections e.g. \"chambers sensors outputs rules cronjobs\"\nUSER_SEED_TOKEN: give a default token to every user for test reasons\nON_SHOW_MODE_BLOCKED: block c\/u of crud to prevent users to mess around with the system on a fair\/exibition\nOS: turns on certain simulation functions e.g. I2C Bus, \"MAC OSX\"\nSIMULATION: turns on simulation mode for sensors\n\nDebug\n\nLONG_ERROR_TRACES: enables long error traces in prod mode (in dev always on)\nHEAP_SNAPSHOPT: turn heap snapshots on with 'true'\nDEBUG: enable debug loggers e.g. 'sensor*'\n\nOther\n\nNO_CRONS: Disables cronjobs\n\nCreate a simulation set:\n\nmongoexport --db LOC_gh --collection sensordatas -f value --query '{detectorType: \"temperature\"}' | head -1000 > temperature-simulation.json\nreplace \/{ \"value\" : (.*)\\, .* }\/ with $1,\nadd brackets and ,\n\nCoding Guidlines\n..1. Use separate branches for separate problems, feel free to push to master afterwards\n..2. More code is read than written, be specific in variable names\n..3. Small commits, commit the smallest unit\n..4. Write unit tests on critical functions, write unit tests where they are missing\n..5. No duplicate code, if code is needed twice, use classes, functions etc.\n..6. Write comments, readme if something is not 100% self explaining\n..7. SignOff your commits with git commit --signoff\nDebugging\nRemote debugging tutorial\nLicense\nThe project is licensed under MIT license.\nBy using\/contributing to the project you accept the license.\n","15":"Grassberry High is an environmental controller for your home grow project. This repository contains the code for the backend Node.js server. Grassberry High is an open source software project. All contributions and the support is community based.\nIMPORTANT NOTICE \/ PROJECT STATUS\nI realized that I don't have the time and energy to develop this project in my free time further. I will still support the project as a maintainer, with guidance and support. But this project needs active developers and further maintainers! I cannot provide enough time to fix all bugs or add additional features at the moment.\nHelp\nIf you find any bugs, please use the issue tracker.\nIn case you need personal assistance you can either use:\n\nour reddit channel (how to use)\nStackOverflow (coding)\nGit-Start (coding)\n\nYou are missing some feature? There are three ways to create new features:\n\nContribute, read our contribution guidelines\nFind someone to code it for you, (a bounty might help)\nSupport development via the official Patreon Campaign\n\nWe are looking for people to translate documents into other languages!\nUseful information\nI2C Setup\ndetect on raspberry pi:\ni2cdetect -y 1 (alias: checki2c)\n0x20 (32) Relais Controller\n0x21 (33), 0x22 (34) Chirp Water Sensor\n0x40 (64), 0x43 (67) HDC1000\n0x4d (77) MHZ16\nDevelopment Slack\nDrop an email to hello <> grassberry-high.com with the title Invite me to Slack\nto get invited to the Slack channel.\nRun\nOn the Pi3\nFollow the instructions here:\nBuild your own tutorial\nShort Version\n\nPlugin all sensors & controllers into the i2c bus and add the power supply\nConnect to gh-config wifi hotspot\nEnter http:\/\/grassberry.local\nEnter your wifi credentials into the configuration, let the device reboot automatically\nDone\n\nConfiguration  Variables\nBasic\n\nNODE_ENV: sets the environment (devlopment, test, production)\n\nApi\n\nAPI_TOKEN: bearer token for api access, for future use\n\nDatabase\n\nMONGODB_URL: url to the database\nMONGODB_ADMIN: admin db\n\nSimulation\n\nSEED: automatically seed diff. collections e.g. \"chambers sensors outputs rules cronjobs\"\nUSER_SEED_TOKEN: give a default token to every user for test reasons\nON_SHOW_MODE_BLOCKED: block c\/u of crud to prevent users to mess around with the system on a fair\/exibition\nOS: turns on certain simulation functions e.g. I2C Bus, \"MAC OSX\"\nSIMULATION: turns on simulation mode for sensors\n\nDebug\n\nLONG_ERROR_TRACES: enables long error traces in prod mode (in dev always on)\nHEAP_SNAPSHOPT: turn heap snapshots on with 'true'\nDEBUG: enable debug loggers e.g. 'sensor*'\n\nOther\n\nNO_CRONS: Disables cronjobs\n\nCreate a simulation set:\n\nmongoexport --db LOC_gh --collection sensordatas -f value --query '{detectorType: \"temperature\"}' | head -1000 > temperature-simulation.json\nreplace \/{ \"value\" : (.*)\\, .* }\/ with $1,\nadd brackets and ,\n\nCoding Guidlines\n..1. Use separate branches for separate problems, feel free to push to master afterwards\n..2. More code is read than written, be specific in variable names\n..3. Small commits, commit the smallest unit\n..4. Write unit tests on critical functions, write unit tests where they are missing\n..5. No duplicate code, if code is needed twice, use classes, functions etc.\n..6. Write comments, readme if something is not 100% self explaining\n..7. SignOff your commits with git commit --signoff\nDebugging\nRemote debugging tutorial\nLicense\nThe project is licensed under MIT license.\nBy using\/contributing to the project you accept the license.\n","16":"Environmental Impact of COVID-19\nThis project examines the \"potential impacts of reduced human traffic\" in protected environments, such as beaches, parks, marine monuments and other wilderness areas and was created as part of the NASA SpaceApps COVID-19 Challenge. These impacts could manifest in a number of forms such as:\n\nReduction in land degradation\nChange in water quality\nChange in vegetation growth\/density\n\nThis project focuses on the Long Island area, and tracks EVI (Enhanced Vegetation Index), Surface Albedo (proportion of incident light or radiation is reflected by a surface), and SST (sea surface temperature) in this region. Check out our team here!\nHow was this created?\nThis project was created using NASA remote sensing data from NASA Goddard Earth Sciences, NASA aqua MODIS, and NASA terra MODIS. Data was downloaded from NASA Giovanni and NASA Earthdata search in HDF format (for EVI) and netCDF format (for SST and surface albedo) for May 2016-2020 and then visualized using Panoply. The visualization showed the level of EVI, SST, and surface albedo on a 9-color scale, with more vibrant colors indicating higher levels of the given variable and paler colors indicating lower levels. After saving the visualization image as a png, it was run through an image processing python script which counted the frequency of each color on the scale and saved the results as a JSON. This information was then plotted as interactive graphs using ReactJS and the Material-UI library that show the frequency of each color versus time for all 3 data sets. The site also contains additional information about the variables used.\nSneak peek\nHere is a graph that shows EVI over time\n\n\n...created from the images above that range from May 2016 to May 2020.\nSo what can we deduce from this?\n\nThere is a slight increase in vegetation greenness between May 2019 and May 2020\nThere are drastic fluctuations in EVI prior to 2019 meaning that the change in 2020 may not be statistically significant\n\nWe plan to increase out dataset to include years prior to 2016 to get a better idea of the trends in EVI and whether or not 2020 marks a significant deviation. If you clone and run this project, you can interact with the EVI chart yourself along with charts for Surface Albedo and SST to draw your own conclusions!\nHow do I run this project?\nStart by installing yarn\nThen run the following commands in your terminal:\n\ngit clone https:\/\/github.com\/SpaceApps2020\/EnvironmentalImpacts.git\ncd EnvironmentalImpacts\/enviroviz\nyarn install\nyarn start\n\nThis should automatically open http:\/\/localhost:3000\/ on your computer where the webapp will be running! A demo of the website can be found here.\n","17":"EnvironmentalCS\nThis program was created on Feb.20\/2020 and after 6 months of work, it was completed on Jun.22\/2020\nThe program is a comparison algorithm for people working towards environmental benefits and are monitoring the plant life growth in a\ncertain area. They can upload and save images of landscapes to our directory and compare them overtime with new images of the area. This\nwill provide them with a thorough analysis of how much plant life change there is in the location of interest.\n","18":"scadasim\nSCADA Simulator encompassing things from PLCs to devices such as valves, pumps, tanks, etc. and environmental properties such as water pH levels\nRequirements\n\npython-pip\n\nInstallation\n$ git clone https:\/\/github.com\/sintax1\/scadasim.git\n$ cd scadasim\n$ make\nRunning Tests\n$ make test\nRunning a simulation using a configuration file\n# Run default config\n$ make run\n-Or-\n# Run custom config\n$ python -i run.py -c [YAML config]\nINFO:root:[e660148d][reservoir][reservoir1]: Initialized\nINFO:root:[0efd0900][valve][valve1]: Initialized\nINFO:root:[db91f04a][pump][pump1]: Initialized\nINFO:root:[1d81d76b][valve][valve2]: Initialized\nINFO:root:[f41bb0d9][tank][tank1]: Initialized\nINFO:root:[0efd0900][valve][valve1]: Added input <- [e660148d][reservoir][reservoir1]\nINFO:root:[e660148d][reservoir][reservoir1]: Added output -> [0efd0900][valve][valve1]\nINFO:root:[db91f04a][pump][pump1]: Added input <- [0efd0900][valve][valve1]\nINFO:root:[0efd0900][valve][valve1]: Added output -> [db91f04a][pump][pump1]\nINFO:root:[f41bb0d9][tank][tank1]: Added input <- [1d81d76b][valve][valve2]\nINFO:root:[1d81d76b][valve][valve2]: Added output -> [f41bb0d9][tank][tank1]\nINFO:root:[1d81d76b][valve][valve2]: Added input <- [db91f04a][pump][pump1]\nINFO:root:[db91f04a][pump][pump1]: Added output -> [1d81d76b][valve][valve2]\nINFO:root:[1d81d76b][valve][valve2]: Active\nINFO:root:[f41bb0d9][tank][tank1]: Active\nINFO:root:[0efd0900][valve][valve1]: Active\nINFO:root:[e660148d][reservoir][reservoir1]: Active\nINFO:root:[db91f04a][pump][pump1]: Active\n>>> sim.devices\n{'valve2': [1d81d76b][valve][valve2], 'tank1': [f41bb0d9][tank][tank1], 'valve1': [0efd0900][valve][valve1], 'reservoir1': [e660148d][reservoir][reservoir1], 'pump1': [db91f04a][pump][pump1]}\n>>> sim.settings\n{'speed': 1}\n>>> sim.devices['tank1'].volume\n79\n>>> sim.devices['pump1'].turn_off()\n>>> sim.devices['tank1'].volume\n103\n>>> sim.devices['tank1'].volume\n103\n>>> sim.pause()\nINFO:root:[dd153bbb][valve][valve2]: Inactive\nINFO:root:[16960a58][tank][tank1]: Inactive\nINFO:root:[d4d9302d][valve][valve1]: Inactive\nINFO:root:[a2f02e65][reservoir][reservoir1]: Inactive\nINFO:root:[f1afd77b][pump][pump1]: Inactive\n>>> sim.start()\nINFO:root:[dd153bbb][valve][valve2]: Active\nINFO:root:[16960a58][tank][tank1]: Active\nINFO:root:[d4d9302d][valve][valve1]: Active\nINFO:root:[a2f02e65][reservoir][reservoir1]: Active\nINFO:root:[f1afd77b][pump][pump1]: Active\n>>> sim.stop()\n$\nExample YAML Config\nsettings:\n  speed: 1\n\ndevices:\n  - !reservoir\n    label: reservoir1\n    volume: 1000\n    fluid: !water {}\n  - !valve\n    label: valve1\n    state: 'open'\n  - !pump\n    label: pump1\n    state: 'on'\n  - !valve\n    label: valve2\n    state: 'open'\n  - !tank\n    label: tank1\n\nconnections:\n  reservoir1:\n    outputs: \n     - valve1\n  valve1:\n    outputs:\n     - pump1\n  pump1:\n    outputs:\n     - valve2\n  valve2:\n    outputs:\n     - tank1\n\n\n  \/*\n    slaveid\n\n        Each slave in a network is assigned a unique unit address from 1 to 247. When the master requests data, \n        the first byte it sends is the Slave address. This way each slave knows after the first byte \n        whether or not to ignore the message.\n\n    register_type & data_address\n\n    \u2018d\u2019 - Discrete Inputs initializer \u2018c\u2019 - Coils initializer \u2018h\u2019 - Holding Register initializer \u2018i\u2019 - Input Registers iniatializer\n\n    Coil\/Register Numbers   Data Addresses  Type        Table Name                          Use\n    1-9999                  0000 to 270E    Read-Write  Discrete Output Coils               on\/off read\/write   c\n    10001-19999             0000 to 270E    Read-Only   Discrete Input Contacts             on\/off readonly     d\n    30001-39999             0000 to 270E    Read-Only   Analog Input Registers              analog readonly     i\n    40001-49999             0000 to 270E    Read-Write  Analog Output Holding Registers     analog read\/write   h\n\n    Each coil or contact is 1 bit and assigned a data address between 0000 and 270E.\n    Each register is 1 word = 16 bits = 2 bytes\n\n  *\/\n\nplcs:\n  plc1:\n    slaveid: 1                  # valid range: 1-247\n    sensors:\n      reservoirsensor:\n          register_type: i       # Valid values: (d)iscretes,(i)inputs,(h)oliding,(c)oils\n          data_address: 0x0000    # Valid values: 0x0000 - 0x270e\n      pump1sensor:\n          register_type: d\n          data_address: 0x0000\n      valve1sensor:\n          register_type: d\n          data_address: 0x0001\n\nRunning a simulation within your own python script\n# Import a fluid with properties\nfrom scadasim.fluids import Water\n# Import the devices\nfrom scadasim.devices import Valve, Pump, Tank, Reservoir\n\n# Instantiate the fluid and devices\nwater = Water()\nreservoir1 = Reservoir(label=\"Reservoir1\", fluid=water, volume=100000000)\ntank2 = Tank(label=\"Tank2\")\npump1 = Pump(label=\"Pump1\")\nvalve1 = Valve(label=\"Valve1\")\nvalve2 = Valve(label=\"Valve2\")\n\n# Connect the devices\nreservoir1.add_output(valve1)\nvalve1.add_output(pump1)\npump1.add_output(valve2)\nvalve2.add_output(tank2)\n\n# Activate the devices\nreservoir1.activate()\nvalve1.activate()\npump1.activate()\nvalve2.activate()\n\n# Manipulate the devices\nvalve1.open()\nvalve2.open()\npump1.turn_on()\nExtending the simulator by adding your own device, sensor, or fluid\nfrom scadasim.devices import Device\n\nclass MyCustomDevice(Device):\n    yaml_tag = u'!mycustomdevice' # So it can be used within YAML configs\n    \n    def __init__(self, myvariable=0, **kwargs):\n        # Add your custom variables here\n        self.myvariable = myvariable\n        super(MyCustomDevice, self).__init__(device_type=\"tank\", **kwargs)\n\n    def input(self, fluid, volume):\n        \"\"\"Receive `volume` amount of `fluid` and return the amount your device is willing to receive\n            accepted_volume = 0: Don't accept any fluid\n            accepted_volume = volume: Accept it all\n            accepted_volume = volume \/ 2: Restrict flow by accepting a fraction of the volume\n        \"\"\"\n        ...\n        Do something here with the fluid that the connected devices send to your device's input\n        ...\n        return accepted_volume\n\n    def output(self, to_device, volume):\n        \"\"\" `to_device` is pulling this device's output (sucking fluid) in the mount of `volume`\n        \"\"\"\n        # If you accept the request, send the fluid to the requesting devices input\n        accepted_volume = to_device.input(self.fluid, volume)\n\n    def worker(self):\n        \"\"\"Do something each cycle of `worker_frequency`\n            Update fluid, pull inputs, push outputs, etc.\n            If your device only performs work based on input and output stimulation, \n            there may be no need to have this worker. Such as a valve.\n        \"\"\"\n        pass\n        \n# Use it\nmydevice = MyCustomDevice(fluid=water, myvariable=10) \n        \nReferences\nhttp:\/\/www.simplymodbus.ca\/faq.htm#Stored\nhttps:\/\/dbus.freedesktop.org\/doc\/dbus-python\/doc\/tutorial.html#basic-type-conversions\nhttp:\/\/home.isr.uc.pt\/~lino\/AIR\/Arquivo\/PLC_Tutor\/registers.htm\n","19":"scadasim\nSCADA Simulator encompassing things from PLCs to devices such as valves, pumps, tanks, etc. and environmental properties such as water pH levels\nRequirements\n\npython-pip\n\nInstallation\n$ git clone https:\/\/github.com\/sintax1\/scadasim.git\n$ cd scadasim\n$ make\nRunning Tests\n$ make test\nRunning a simulation using a configuration file\n# Run default config\n$ make run\n-Or-\n# Run custom config\n$ python -i run.py -c [YAML config]\nINFO:root:[e660148d][reservoir][reservoir1]: Initialized\nINFO:root:[0efd0900][valve][valve1]: Initialized\nINFO:root:[db91f04a][pump][pump1]: Initialized\nINFO:root:[1d81d76b][valve][valve2]: Initialized\nINFO:root:[f41bb0d9][tank][tank1]: Initialized\nINFO:root:[0efd0900][valve][valve1]: Added input <- [e660148d][reservoir][reservoir1]\nINFO:root:[e660148d][reservoir][reservoir1]: Added output -> [0efd0900][valve][valve1]\nINFO:root:[db91f04a][pump][pump1]: Added input <- [0efd0900][valve][valve1]\nINFO:root:[0efd0900][valve][valve1]: Added output -> [db91f04a][pump][pump1]\nINFO:root:[f41bb0d9][tank][tank1]: Added input <- [1d81d76b][valve][valve2]\nINFO:root:[1d81d76b][valve][valve2]: Added output -> [f41bb0d9][tank][tank1]\nINFO:root:[1d81d76b][valve][valve2]: Added input <- [db91f04a][pump][pump1]\nINFO:root:[db91f04a][pump][pump1]: Added output -> [1d81d76b][valve][valve2]\nINFO:root:[1d81d76b][valve][valve2]: Active\nINFO:root:[f41bb0d9][tank][tank1]: Active\nINFO:root:[0efd0900][valve][valve1]: Active\nINFO:root:[e660148d][reservoir][reservoir1]: Active\nINFO:root:[db91f04a][pump][pump1]: Active\n>>> sim.devices\n{'valve2': [1d81d76b][valve][valve2], 'tank1': [f41bb0d9][tank][tank1], 'valve1': [0efd0900][valve][valve1], 'reservoir1': [e660148d][reservoir][reservoir1], 'pump1': [db91f04a][pump][pump1]}\n>>> sim.settings\n{'speed': 1}\n>>> sim.devices['tank1'].volume\n79\n>>> sim.devices['pump1'].turn_off()\n>>> sim.devices['tank1'].volume\n103\n>>> sim.devices['tank1'].volume\n103\n>>> sim.pause()\nINFO:root:[dd153bbb][valve][valve2]: Inactive\nINFO:root:[16960a58][tank][tank1]: Inactive\nINFO:root:[d4d9302d][valve][valve1]: Inactive\nINFO:root:[a2f02e65][reservoir][reservoir1]: Inactive\nINFO:root:[f1afd77b][pump][pump1]: Inactive\n>>> sim.start()\nINFO:root:[dd153bbb][valve][valve2]: Active\nINFO:root:[16960a58][tank][tank1]: Active\nINFO:root:[d4d9302d][valve][valve1]: Active\nINFO:root:[a2f02e65][reservoir][reservoir1]: Active\nINFO:root:[f1afd77b][pump][pump1]: Active\n>>> sim.stop()\n$\nExample YAML Config\nsettings:\n  speed: 1\n\ndevices:\n  - !reservoir\n    label: reservoir1\n    volume: 1000\n    fluid: !water {}\n  - !valve\n    label: valve1\n    state: 'open'\n  - !pump\n    label: pump1\n    state: 'on'\n  - !valve\n    label: valve2\n    state: 'open'\n  - !tank\n    label: tank1\n\nconnections:\n  reservoir1:\n    outputs: \n     - valve1\n  valve1:\n    outputs:\n     - pump1\n  pump1:\n    outputs:\n     - valve2\n  valve2:\n    outputs:\n     - tank1\n\n\n  \/*\n    slaveid\n\n        Each slave in a network is assigned a unique unit address from 1 to 247. When the master requests data, \n        the first byte it sends is the Slave address. This way each slave knows after the first byte \n        whether or not to ignore the message.\n\n    register_type & data_address\n\n    \u2018d\u2019 - Discrete Inputs initializer \u2018c\u2019 - Coils initializer \u2018h\u2019 - Holding Register initializer \u2018i\u2019 - Input Registers iniatializer\n\n    Coil\/Register Numbers   Data Addresses  Type        Table Name                          Use\n    1-9999                  0000 to 270E    Read-Write  Discrete Output Coils               on\/off read\/write   c\n    10001-19999             0000 to 270E    Read-Only   Discrete Input Contacts             on\/off readonly     d\n    30001-39999             0000 to 270E    Read-Only   Analog Input Registers              analog readonly     i\n    40001-49999             0000 to 270E    Read-Write  Analog Output Holding Registers     analog read\/write   h\n\n    Each coil or contact is 1 bit and assigned a data address between 0000 and 270E.\n    Each register is 1 word = 16 bits = 2 bytes\n\n  *\/\n\nplcs:\n  plc1:\n    slaveid: 1                  # valid range: 1-247\n    sensors:\n      reservoirsensor:\n          register_type: i       # Valid values: (d)iscretes,(i)inputs,(h)oliding,(c)oils\n          data_address: 0x0000    # Valid values: 0x0000 - 0x270e\n      pump1sensor:\n          register_type: d\n          data_address: 0x0000\n      valve1sensor:\n          register_type: d\n          data_address: 0x0001\n\nRunning a simulation within your own python script\n# Import a fluid with properties\nfrom scadasim.fluids import Water\n# Import the devices\nfrom scadasim.devices import Valve, Pump, Tank, Reservoir\n\n# Instantiate the fluid and devices\nwater = Water()\nreservoir1 = Reservoir(label=\"Reservoir1\", fluid=water, volume=100000000)\ntank2 = Tank(label=\"Tank2\")\npump1 = Pump(label=\"Pump1\")\nvalve1 = Valve(label=\"Valve1\")\nvalve2 = Valve(label=\"Valve2\")\n\n# Connect the devices\nreservoir1.add_output(valve1)\nvalve1.add_output(pump1)\npump1.add_output(valve2)\nvalve2.add_output(tank2)\n\n# Activate the devices\nreservoir1.activate()\nvalve1.activate()\npump1.activate()\nvalve2.activate()\n\n# Manipulate the devices\nvalve1.open()\nvalve2.open()\npump1.turn_on()\nExtending the simulator by adding your own device, sensor, or fluid\nfrom scadasim.devices import Device\n\nclass MyCustomDevice(Device):\n    yaml_tag = u'!mycustomdevice' # So it can be used within YAML configs\n    \n    def __init__(self, myvariable=0, **kwargs):\n        # Add your custom variables here\n        self.myvariable = myvariable\n        super(MyCustomDevice, self).__init__(device_type=\"tank\", **kwargs)\n\n    def input(self, fluid, volume):\n        \"\"\"Receive `volume` amount of `fluid` and return the amount your device is willing to receive\n            accepted_volume = 0: Don't accept any fluid\n            accepted_volume = volume: Accept it all\n            accepted_volume = volume \/ 2: Restrict flow by accepting a fraction of the volume\n        \"\"\"\n        ...\n        Do something here with the fluid that the connected devices send to your device's input\n        ...\n        return accepted_volume\n\n    def output(self, to_device, volume):\n        \"\"\" `to_device` is pulling this device's output (sucking fluid) in the mount of `volume`\n        \"\"\"\n        # If you accept the request, send the fluid to the requesting devices input\n        accepted_volume = to_device.input(self.fluid, volume)\n\n    def worker(self):\n        \"\"\"Do something each cycle of `worker_frequency`\n            Update fluid, pull inputs, push outputs, etc.\n            If your device only performs work based on input and output stimulation, \n            there may be no need to have this worker. Such as a valve.\n        \"\"\"\n        pass\n        \n# Use it\nmydevice = MyCustomDevice(fluid=water, myvariable=10) \n        \nReferences\nhttp:\/\/www.simplymodbus.ca\/faq.htm#Stored\nhttps:\/\/dbus.freedesktop.org\/doc\/dbus-python\/doc\/tutorial.html#basic-type-conversions\nhttp:\/\/home.isr.uc.pt\/~lino\/AIR\/Arquivo\/PLC_Tutor\/registers.htm\n","20":"scadasim\nSCADA Simulator encompassing things from PLCs to devices such as valves, pumps, tanks, etc. and environmental properties such as water pH levels\nRequirements\n\npython-pip\n\nInstallation\n$ git clone https:\/\/github.com\/sintax1\/scadasim.git\n$ cd scadasim\n$ make\nRunning Tests\n$ make test\nRunning a simulation using a configuration file\n# Run default config\n$ make run\n-Or-\n# Run custom config\n$ python -i run.py -c [YAML config]\nINFO:root:[e660148d][reservoir][reservoir1]: Initialized\nINFO:root:[0efd0900][valve][valve1]: Initialized\nINFO:root:[db91f04a][pump][pump1]: Initialized\nINFO:root:[1d81d76b][valve][valve2]: Initialized\nINFO:root:[f41bb0d9][tank][tank1]: Initialized\nINFO:root:[0efd0900][valve][valve1]: Added input <- [e660148d][reservoir][reservoir1]\nINFO:root:[e660148d][reservoir][reservoir1]: Added output -> [0efd0900][valve][valve1]\nINFO:root:[db91f04a][pump][pump1]: Added input <- [0efd0900][valve][valve1]\nINFO:root:[0efd0900][valve][valve1]: Added output -> [db91f04a][pump][pump1]\nINFO:root:[f41bb0d9][tank][tank1]: Added input <- [1d81d76b][valve][valve2]\nINFO:root:[1d81d76b][valve][valve2]: Added output -> [f41bb0d9][tank][tank1]\nINFO:root:[1d81d76b][valve][valve2]: Added input <- [db91f04a][pump][pump1]\nINFO:root:[db91f04a][pump][pump1]: Added output -> [1d81d76b][valve][valve2]\nINFO:root:[1d81d76b][valve][valve2]: Active\nINFO:root:[f41bb0d9][tank][tank1]: Active\nINFO:root:[0efd0900][valve][valve1]: Active\nINFO:root:[e660148d][reservoir][reservoir1]: Active\nINFO:root:[db91f04a][pump][pump1]: Active\n>>> sim.devices\n{'valve2': [1d81d76b][valve][valve2], 'tank1': [f41bb0d9][tank][tank1], 'valve1': [0efd0900][valve][valve1], 'reservoir1': [e660148d][reservoir][reservoir1], 'pump1': [db91f04a][pump][pump1]}\n>>> sim.settings\n{'speed': 1}\n>>> sim.devices['tank1'].volume\n79\n>>> sim.devices['pump1'].turn_off()\n>>> sim.devices['tank1'].volume\n103\n>>> sim.devices['tank1'].volume\n103\n>>> sim.pause()\nINFO:root:[dd153bbb][valve][valve2]: Inactive\nINFO:root:[16960a58][tank][tank1]: Inactive\nINFO:root:[d4d9302d][valve][valve1]: Inactive\nINFO:root:[a2f02e65][reservoir][reservoir1]: Inactive\nINFO:root:[f1afd77b][pump][pump1]: Inactive\n>>> sim.start()\nINFO:root:[dd153bbb][valve][valve2]: Active\nINFO:root:[16960a58][tank][tank1]: Active\nINFO:root:[d4d9302d][valve][valve1]: Active\nINFO:root:[a2f02e65][reservoir][reservoir1]: Active\nINFO:root:[f1afd77b][pump][pump1]: Active\n>>> sim.stop()\n$\nExample YAML Config\nsettings:\n  speed: 1\n\ndevices:\n  - !reservoir\n    label: reservoir1\n    volume: 1000\n    fluid: !water {}\n  - !valve\n    label: valve1\n    state: 'open'\n  - !pump\n    label: pump1\n    state: 'on'\n  - !valve\n    label: valve2\n    state: 'open'\n  - !tank\n    label: tank1\n\nconnections:\n  reservoir1:\n    outputs: \n     - valve1\n  valve1:\n    outputs:\n     - pump1\n  pump1:\n    outputs:\n     - valve2\n  valve2:\n    outputs:\n     - tank1\n\n\n  \/*\n    slaveid\n\n        Each slave in a network is assigned a unique unit address from 1 to 247. When the master requests data, \n        the first byte it sends is the Slave address. This way each slave knows after the first byte \n        whether or not to ignore the message.\n\n    register_type & data_address\n\n    \u2018d\u2019 - Discrete Inputs initializer \u2018c\u2019 - Coils initializer \u2018h\u2019 - Holding Register initializer \u2018i\u2019 - Input Registers iniatializer\n\n    Coil\/Register Numbers   Data Addresses  Type        Table Name                          Use\n    1-9999                  0000 to 270E    Read-Write  Discrete Output Coils               on\/off read\/write   c\n    10001-19999             0000 to 270E    Read-Only   Discrete Input Contacts             on\/off readonly     d\n    30001-39999             0000 to 270E    Read-Only   Analog Input Registers              analog readonly     i\n    40001-49999             0000 to 270E    Read-Write  Analog Output Holding Registers     analog read\/write   h\n\n    Each coil or contact is 1 bit and assigned a data address between 0000 and 270E.\n    Each register is 1 word = 16 bits = 2 bytes\n\n  *\/\n\nplcs:\n  plc1:\n    slaveid: 1                  # valid range: 1-247\n    sensors:\n      reservoirsensor:\n          register_type: i       # Valid values: (d)iscretes,(i)inputs,(h)oliding,(c)oils\n          data_address: 0x0000    # Valid values: 0x0000 - 0x270e\n      pump1sensor:\n          register_type: d\n          data_address: 0x0000\n      valve1sensor:\n          register_type: d\n          data_address: 0x0001\n\nRunning a simulation within your own python script\n# Import a fluid with properties\nfrom scadasim.fluids import Water\n# Import the devices\nfrom scadasim.devices import Valve, Pump, Tank, Reservoir\n\n# Instantiate the fluid and devices\nwater = Water()\nreservoir1 = Reservoir(label=\"Reservoir1\", fluid=water, volume=100000000)\ntank2 = Tank(label=\"Tank2\")\npump1 = Pump(label=\"Pump1\")\nvalve1 = Valve(label=\"Valve1\")\nvalve2 = Valve(label=\"Valve2\")\n\n# Connect the devices\nreservoir1.add_output(valve1)\nvalve1.add_output(pump1)\npump1.add_output(valve2)\nvalve2.add_output(tank2)\n\n# Activate the devices\nreservoir1.activate()\nvalve1.activate()\npump1.activate()\nvalve2.activate()\n\n# Manipulate the devices\nvalve1.open()\nvalve2.open()\npump1.turn_on()\nExtending the simulator by adding your own device, sensor, or fluid\nfrom scadasim.devices import Device\n\nclass MyCustomDevice(Device):\n    yaml_tag = u'!mycustomdevice' # So it can be used within YAML configs\n    \n    def __init__(self, myvariable=0, **kwargs):\n        # Add your custom variables here\n        self.myvariable = myvariable\n        super(MyCustomDevice, self).__init__(device_type=\"tank\", **kwargs)\n\n    def input(self, fluid, volume):\n        \"\"\"Receive `volume` amount of `fluid` and return the amount your device is willing to receive\n            accepted_volume = 0: Don't accept any fluid\n            accepted_volume = volume: Accept it all\n            accepted_volume = volume \/ 2: Restrict flow by accepting a fraction of the volume\n        \"\"\"\n        ...\n        Do something here with the fluid that the connected devices send to your device's input\n        ...\n        return accepted_volume\n\n    def output(self, to_device, volume):\n        \"\"\" `to_device` is pulling this device's output (sucking fluid) in the mount of `volume`\n        \"\"\"\n        # If you accept the request, send the fluid to the requesting devices input\n        accepted_volume = to_device.input(self.fluid, volume)\n\n    def worker(self):\n        \"\"\"Do something each cycle of `worker_frequency`\n            Update fluid, pull inputs, push outputs, etc.\n            If your device only performs work based on input and output stimulation, \n            there may be no need to have this worker. Such as a valve.\n        \"\"\"\n        pass\n        \n# Use it\nmydevice = MyCustomDevice(fluid=water, myvariable=10) \n        \nReferences\nhttp:\/\/www.simplymodbus.ca\/faq.htm#Stored\nhttps:\/\/dbus.freedesktop.org\/doc\/dbus-python\/doc\/tutorial.html#basic-type-conversions\nhttp:\/\/home.isr.uc.pt\/~lino\/AIR\/Arquivo\/PLC_Tutor\/registers.htm\n","21":"Albiorix\nCode used for configuration and administration of the bioinformatics computer cluster Albiorix at the Department of Biological and Environmental Sciences, University of Gothenburg.\n","22":"Albiorix\nCode used for configuration and administration of the bioinformatics computer cluster Albiorix at the Department of Biological and Environmental Sciences, University of Gothenburg.\n","23":"EnvironmentalModeling\nLectures Environmental Science Modeling\n","24":"Environment Monitoring\nA sensitive healthcare equipment needs suitable operating conditions.\nThe equipment's operating environment\nis the responsibility of the customer.\nTo assist the customer in maintaining a suitable environment,\nwe install an environment monitoring device at the customer premises\nand monitor the data.\nThis project is about simulating data from a monitoring device\nand issuing alerts and warnings.\nDecomposition\nAt a top level, the program runs in two processes - the sender and the receiver.\n\nThe Sender is responsible for simulating the monitoring device.\nThe Receiver analyzes the data.\nThe Sender sends data to the Receiver using console redirection.\nRun them on the command line as follows:\nsender-executable | receiver-executable\nThis would make the console-writes of the sender\nbecome the console-reads of the receiver.\nDecomposition of responsibility within the Sender and Receiver\nThe naming of source files within the Sender and within the Receiver\ngive their internal decomposition.\nThe Code\nThis project follows the practices and tools\nlisted here.\nThe Interface\nWe document the interface between the Sender and the Receiver as test cases.\nThe Sender and Receiver are testable on their own:\n\nThe Sender is testable without the Receiver - so we can develop\nfor another data-source, test and be confident about integration.\nThe Receiver is testable without the Sender - so we can enhance\nwithout re-testing against all Receivers again.\n\nMinimum Functionality\nFor simulating different sequences of data from the monitor,\nthe Sender takes a CSV file as input.\nThis file contains temperature and humidity data that the Sender\nsends periodically.\nThe Receiver outputs warnings and alerts on the console when environmental\nconditions breach these limits:\n\nTemperature warning levels: High: 37 C; Low: 4 C\nTemperature error levels: High: 40C; Low: 0 C\nHumidity warning level: High: 70%\nHumidity error level: High: 90%\n\nExtended Functionality\nThe Sender needs to send the data every 5 minutes. When the Receiver doesn't\nreceive any data for half an hour, the Receiver outputs an alert\nEvaluation Criteria\nSee here\nfor the evaluation criteria of this exercise.\n","25":"EnvironmentalSensors\ncode for adafruit and maxbotix sensors to track the vernal window.\n","26":"EnvironmentalSensors\ncode for adafruit and maxbotix sensors to track the vernal window.\n","27":"EnvironmentalSensors\ncode for adafruit and maxbotix sensors to track the vernal window.\n","28":"EnvironmentalSensors\ncode for adafruit and maxbotix sensors to track the vernal window.\n","29":"EnvironmentalMonitor\nArduino+python temperature\/humidity\/etc. monitor and logger.\n","30":"Enviro-monitor\nIndoor\/outdoor environmental monitor project for the Enviro+ environmental monitoring board.\nUses OpenWeather api to get the current weather and show it on the display.\nAlso, the display can be turned on\/off by passing your finger near the light sensor to reduce energy consumption.\n\nPython library and more steps to install can be found on pimoroni\/enviroplus-python\nInstalling\nInstall and configure dependencies from GitHub:\n\ngit clone https:\/\/github.com\/pimoroni\/enviroplus-python\ncd enviroplus-python\nsudo .\/install.sh && cd ..\n\nNote Raspbian Lite users may first need to install git: sudo apt install git\n\ngit clone https:\/\/github.com\/cesnietor\/enviro-monitor.git\ncd enviro-monitor\n\nAdd environment variables:\n\n\nMake sure you have a valid APPID from OpenWeather on:\ncurl https:\/\/api.openweathermap.org\/data\/2.5\/weather?id=2172797&APPID=<UniqueUUID>&units=imperial\n\n\n\nDefine city id and APPID as environment variables:\nexport OPENWEATHERMAP_CITY_ID=<YourCityID>\nexport OPENWEATHERMAP_APPID=<UniqueUUID>\n\n\n\nChange time_zone and city_name timezone and city for python timezone on enviro-monitor.py\n\n\nRunning\npython3 enviro-monitory.py\n\nIf you want to run it as a background process and your are using ssh on your raspberry pi\nmake sure the process is re-parented by init.\nsetsid python3 enviro-monitor.py < \/dev\/zero &> error.log &\n\nIf yout want to run it when the pi boots refer to rc.local\n","31":"ETD_EnvironmentalSensor\nProject in Python consisting of smaller apps talking to each other (micro-services) that enables reading measurements from Hum&Temp sensor and displaying them on a webpage.\nSteps for creating project\nDownload repo\ngit clone <adres_url>\nCreate venv and install required packages\npython3 -m venv venv\nsource .\/venv\/bin\/activate\npip install --upgrade pip\npip3 install -r .\/requirements.txt\npermanent alias\nnano ~\/.bashrc\nalias venv=\"source ~\/EnvironmentalSensors\/venv\/bin\/activate\"\n#reboot or type source ~\/.bashrc\nCAN config (for temp&hum sensor) \/\/add or uncomment\ndtparam=spi=on\ndtoverlay=mcp2515-can0,oscillator=8000000,interrupt=25,spimaxfrequency=1000000\nsudo apt update\nsudo modprobe mcp251x\nsudo apt install can-utils   #worth rebooting now\nsudo ip link set can0 up type can bitrate 460800\ncandump can0 -t A\nsome bug fix\nsudo apt-get install libatlas-base-dev\nrunning app\n.\/start.sh\nsigoihy\n","32":"EnvironmentalMonitoring\n\u914d\u7535\u623f\u73af\u5883\u8fdc\u7a0b\u63a7\u5236\u7cfb\u7edf\n1.\u767b\u5f55\n2.\u9996\u9875\u83b7\u53d6\u914d\u7535\u623f\u73af\u5883\u76d1\u63a7\u8bbe\u5907\u5305\u62ec\u5728\u7ebf\u60c5\u51b5\u3001\u521b\u5efa\u65f6\u95f4\u4ee5\u53ca\u540d\u79f0\u3001ID\u7b49\n3.\u5373\u65f6\u6570\u636e\u9875\u9762\u4e5f\u83b7\u53d6\u67d0\u4e2a\u8bbe\u5907\u6700\u65b0\u91c7\u96c6\u7684\u6570\u636e\n4.\u53ef\u4ee5\u641c\u7d22\u8bbe\u5907\uff0c\u6839\u636e\u8bbe\u5907\u7684\u4fe1\u606f\n5.\u9996\u9875\u6709Peek & Pop\u529f\u80fd\n6.\u4fee\u6539\u5bc6\u7801\u548c\u9000\u51fa\u767b\u5f55\n7.\u6570\u636e\u8be6\u60c5\u53ef\u4ee5\u89c2\u5bdf\u67d0\u4e2a\u8bbe\u5907\u6700\u65b0\u91c7\u96c6\u6570\u636e\u7684\u534a\u4e2a\u5c0f\u65f6\u5185\u7684\u6e29\u6e7f\u5ea6\u6298\u7ebf\u56fe\n8.\u5237\u65b0\u65b9\u5f0f\u6709\u4e0a\u4e0b\u62c9\u5237\u65b0\u548c\u81ea\u52a8\u5237\u65b0\n9.\u53ef\u4ee5\u67e5\u8be2\u5386\u53f2\u6570\u636e\n10.\u53ef\u4ee5\u5bfc\u51faExcel\u65e5\u5fd7\n11.\u5982\u679c\u6570\u636e\u5f02\u5e38\u80fd\u63a5\u6536\u5230\u63a8\u9001\n12.\u91cc\u9762\u5c11\u4e86\u4e00\u4e2aLibXL\u7684\u5e93 \u53ef\u4ee5\u81ea\u884c\u4e0b\u8f7d\u5bfc\u5165\u5373\u53ef\n","33":"EnvironmentalS\n","34":"EnvironmentalReadings\nA PowerShell module for reading the temperature and humidity from the webpages of a sensor made using the instructions linked below.  The data is then appended to a CSV file.\nhttps:\/\/randomnerdtutorials.com\/esp32-dht11-dht22-temperature-humidity-web-server-arduino-ide\/\n","35":"EnvironmentalSensor\n","36":"EnvironmentalModeling\n","37":"EnvironmentalModeling\n","38":"EnvironmentalModeling\n","39":"EnvironmentalModeling\n","40":"##Environmental Data Logger\nThe design files and the software for a small low-power temperature and humidity logger\nProbably the most remarkable feature is the power consumption of just 3.2\u00b5A or less\na better documentation can be found on Hackaday.io\nCredits:\n-HD44780 Library by SA Development\n-I\u00b2C Library by Peter Fleury\n","41":"##Environmental Data Logger\nThe design files and the software for a small low-power temperature and humidity logger\nProbably the most remarkable feature is the power consumption of just 3.2\u00b5A or less\na better documentation can be found on Hackaday.io\nCredits:\n-HD44780 Library by SA Development\n-I\u00b2C Library by Peter Fleury\n","42":"##Environmental Data Logger\nThe design files and the software for a small low-power temperature and humidity logger\nProbably the most remarkable feature is the power consumption of just 3.2\u00b5A or less\na better documentation can be found on Hackaday.io\nCredits:\n-HD44780 Library by SA Development\n-I\u00b2C Library by Peter Fleury\n","43":"EnvironmentalInterface\nUnderlying middleware for ServIoTicy.\nRequirements\n\nServIoTicy APIKEY inside \/servioticy\/res directory.\nsecrets.example must be fulfilled, instructions inside.\n\nDeletion Requirements\nIn order to use the delete scripts to clean Service Objects, additional tools are needed:\n\ncURL\nPostgreSQL client\nsudo apt-get install postgresql-common postgresql-client-<version>\nFile .pgpass in root directory with the following database info content\n\ncd ~\necho \"hostname:port:dbname:dbuser:dbpassword\" > .pgpass\n\nPreparation Phase\nIn order to create a valid \/servioticy folder, several files must be created before executing. Firstly, models for room sensors must be created. To do so, make use of the generator inside \/servioticy\/models\njavac modelGenerator.java\njava modelGenerator #rooms\n\nEach room is currently composed of 5 sensors (XM1000, Light, Power, Presence and Air quality).\nAdditionally, actuators can also be created by making use of the generator inside \/servioticy\/actuators\njavac actuatorGenerator.java\njava actuatorGenerator #rooms\n\nCurrently, each room contains Computer, Light and HVAC actuators. In order to push the actuators to ServIoTicy and store the IDS into the database, additional scripts are created.\n.\/push_actuators.sh\n\nThe script firstly uses the create_SO.sh script to communicate with ServIoTicy and obtain the IDs. Finally, it uses the script db.py to store the IDs into the database.\nUsage\nThere are several options to launch the Interface. Currently, virtual, real or both sensor servers can be run. If the real sensor server is needed, it is firstly required to start the serial forwarder. At the moment, only XM1000 motes are supported. For more driver and specs information refer to http:\/\/www.advanticsys.com\/shop\/asxm1000-p-24.html\n.\/start_sf.sh (only if the serial port is being used)\njava -jar EnvIface.jar [-comm <source>] [-port <port>]\n\nOnce the servers are running, if virtual sensors are needed, they are executed as follows:\njava sensors [-port <port>] [-ids <#sensors>]\n\nEither of those commands will execute the sensors, sending 2 messages for each of them.\n","44":"EnvironmentalInterface\nUnderlying middleware for ServIoTicy.\nRequirements\n\nServIoTicy APIKEY inside \/servioticy\/res directory.\nsecrets.example must be fulfilled, instructions inside.\n\nDeletion Requirements\nIn order to use the delete scripts to clean Service Objects, additional tools are needed:\n\ncURL\nPostgreSQL client\nsudo apt-get install postgresql-common postgresql-client-<version>\nFile .pgpass in root directory with the following database info content\n\ncd ~\necho \"hostname:port:dbname:dbuser:dbpassword\" > .pgpass\n\nPreparation Phase\nIn order to create a valid \/servioticy folder, several files must be created before executing. Firstly, models for room sensors must be created. To do so, make use of the generator inside \/servioticy\/models\njavac modelGenerator.java\njava modelGenerator #rooms\n\nEach room is currently composed of 5 sensors (XM1000, Light, Power, Presence and Air quality).\nAdditionally, actuators can also be created by making use of the generator inside \/servioticy\/actuators\njavac actuatorGenerator.java\njava actuatorGenerator #rooms\n\nCurrently, each room contains Computer, Light and HVAC actuators. In order to push the actuators to ServIoTicy and store the IDS into the database, additional scripts are created.\n.\/push_actuators.sh\n\nThe script firstly uses the create_SO.sh script to communicate with ServIoTicy and obtain the IDs. Finally, it uses the script db.py to store the IDs into the database.\nUsage\nThere are several options to launch the Interface. Currently, virtual, real or both sensor servers can be run. If the real sensor server is needed, it is firstly required to start the serial forwarder. At the moment, only XM1000 motes are supported. For more driver and specs information refer to http:\/\/www.advanticsys.com\/shop\/asxm1000-p-24.html\n.\/start_sf.sh (only if the serial port is being used)\njava -jar EnvIface.jar [-comm <source>] [-port <port>]\n\nOnce the servers are running, if virtual sensors are needed, they are executed as follows:\njava sensors [-port <port>] [-ids <#sensors>]\n\nEither of those commands will execute the sensors, sending 2 messages for each of them.\n","45":"EnvironmentalInterface\nUnderlying middleware for ServIoTicy.\nRequirements\n\nServIoTicy APIKEY inside \/servioticy\/res directory.\nsecrets.example must be fulfilled, instructions inside.\n\nDeletion Requirements\nIn order to use the delete scripts to clean Service Objects, additional tools are needed:\n\ncURL\nPostgreSQL client\nsudo apt-get install postgresql-common postgresql-client-<version>\nFile .pgpass in root directory with the following database info content\n\ncd ~\necho \"hostname:port:dbname:dbuser:dbpassword\" > .pgpass\n\nPreparation Phase\nIn order to create a valid \/servioticy folder, several files must be created before executing. Firstly, models for room sensors must be created. To do so, make use of the generator inside \/servioticy\/models\njavac modelGenerator.java\njava modelGenerator #rooms\n\nEach room is currently composed of 5 sensors (XM1000, Light, Power, Presence and Air quality).\nAdditionally, actuators can also be created by making use of the generator inside \/servioticy\/actuators\njavac actuatorGenerator.java\njava actuatorGenerator #rooms\n\nCurrently, each room contains Computer, Light and HVAC actuators. In order to push the actuators to ServIoTicy and store the IDS into the database, additional scripts are created.\n.\/push_actuators.sh\n\nThe script firstly uses the create_SO.sh script to communicate with ServIoTicy and obtain the IDs. Finally, it uses the script db.py to store the IDs into the database.\nUsage\nThere are several options to launch the Interface. Currently, virtual, real or both sensor servers can be run. If the real sensor server is needed, it is firstly required to start the serial forwarder. At the moment, only XM1000 motes are supported. For more driver and specs information refer to http:\/\/www.advanticsys.com\/shop\/asxm1000-p-24.html\n.\/start_sf.sh (only if the serial port is being used)\njava -jar EnvIface.jar [-comm <source>] [-port <port>]\n\nOnce the servers are running, if virtual sensors are needed, they are executed as follows:\njava sensors [-port <port>] [-ids <#sensors>]\n\nEither of those commands will execute the sensors, sending 2 messages for each of them.\n","46":"Environmental Monitor\nData captured\nData is published to serial bus once per second as a comma delimited string.\nExample:\n26.70,35,736,1013,-5.91,26421\n\n\nTemperature - Celsius\nHumidity - Percent\nLight - Analog value between 0 and 1023\nPressure - Pa\nAltitude  - m (Not accurate)\nCRC-16 - checksum of data fields\n\nSensors\n\nDHT11 - temperature + humidity\nBMP085 - temperature + pressure\nLDR - light\n\nDependencies\npip3 install paho-mqtt pyserial crcmod\nConfig\n\nSerial - 9600\nLDR - A0\nDHT11 - 4\nBMP085 - 0x77\n\nPublishing to MQTT\nserToMQTT.py takes the data published by the Arduino and forwards it to an MQTT broker.\nIt takes the name of the serial port and name of the room it is in as parameters.\npython3 serToMQTT.py \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nAn alternative to python is the q versionserToMQTT.q\nq serToMQTT.q -q \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nYou can subscribe from the command line to confirm your data is publishing to your broker:\nmosquitto_sub -h 192.168.1.111 -t \"hassio\/#\"\nAdd as sensors to Home Assistant\nAdd to configuration.yaml:\nsensor:\n  platform: mqtt\n  name: \"Living Room Temperature\"\n  state_topic: \"hassio\/livingroom\/temperature\"\n  qos: 0\n  unit_of_measurement: \"\u00baC\"\n\nsensor 2:\n  platform: mqtt\n  name: \"Living Room Humidity\"\n  state_topic: \"hassio\/livingroom\/humidity\"\n  qos: 0\n  unit_of_measurement: \"%\"\n\nsensor 3:\n  platform: mqtt\n  name: \"Living Room Pressure\"\n  state_topic: \"hassio\/livingroom\/pressure\"\n  qos: 0\n  unit_of_measurement: \"hPa\"\n\nsensor 4:\n  platform: mqtt\n  name: \"Living Room Light\"\n  state_topic: \"hassio\/livingroom\/light\"\n  qos: 0\n  unit_of_measurement: \"\/1024\"\nAdding sensors to Lovelace Dashboard\nTo add to a Lovelace dashboard:\nentities:\n  - entity: sensor.living_room_temperature\n  - entity: sensor.living_room_humidity\n  - entity: sensor.living_room_pressure\n  - entity: sensor.living_room_light\nshow_icon: true\nshow_name: false\nshow_state: true\ntitle: Living Room\ntype: glance\n","47":"Environmental Monitor\nData captured\nData is published to serial bus once per second as a comma delimited string.\nExample:\n26.70,35,736,1013,-5.91,26421\n\n\nTemperature - Celsius\nHumidity - Percent\nLight - Analog value between 0 and 1023\nPressure - Pa\nAltitude  - m (Not accurate)\nCRC-16 - checksum of data fields\n\nSensors\n\nDHT11 - temperature + humidity\nBMP085 - temperature + pressure\nLDR - light\n\nDependencies\npip3 install paho-mqtt pyserial crcmod\nConfig\n\nSerial - 9600\nLDR - A0\nDHT11 - 4\nBMP085 - 0x77\n\nPublishing to MQTT\nserToMQTT.py takes the data published by the Arduino and forwards it to an MQTT broker.\nIt takes the name of the serial port and name of the room it is in as parameters.\npython3 serToMQTT.py \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nAn alternative to python is the q versionserToMQTT.q\nq serToMQTT.q -q \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nYou can subscribe from the command line to confirm your data is publishing to your broker:\nmosquitto_sub -h 192.168.1.111 -t \"hassio\/#\"\nAdd as sensors to Home Assistant\nAdd to configuration.yaml:\nsensor:\n  platform: mqtt\n  name: \"Living Room Temperature\"\n  state_topic: \"hassio\/livingroom\/temperature\"\n  qos: 0\n  unit_of_measurement: \"\u00baC\"\n\nsensor 2:\n  platform: mqtt\n  name: \"Living Room Humidity\"\n  state_topic: \"hassio\/livingroom\/humidity\"\n  qos: 0\n  unit_of_measurement: \"%\"\n\nsensor 3:\n  platform: mqtt\n  name: \"Living Room Pressure\"\n  state_topic: \"hassio\/livingroom\/pressure\"\n  qos: 0\n  unit_of_measurement: \"hPa\"\n\nsensor 4:\n  platform: mqtt\n  name: \"Living Room Light\"\n  state_topic: \"hassio\/livingroom\/light\"\n  qos: 0\n  unit_of_measurement: \"\/1024\"\nAdding sensors to Lovelace Dashboard\nTo add to a Lovelace dashboard:\nentities:\n  - entity: sensor.living_room_temperature\n  - entity: sensor.living_room_humidity\n  - entity: sensor.living_room_pressure\n  - entity: sensor.living_room_light\nshow_icon: true\nshow_name: false\nshow_state: true\ntitle: Living Room\ntype: glance\n","48":"Environmental Monitor\nData captured\nData is published to serial bus once per second as a comma delimited string.\nExample:\n26.70,35,736,1013,-5.91,26421\n\n\nTemperature - Celsius\nHumidity - Percent\nLight - Analog value between 0 and 1023\nPressure - Pa\nAltitude  - m (Not accurate)\nCRC-16 - checksum of data fields\n\nSensors\n\nDHT11 - temperature + humidity\nBMP085 - temperature + pressure\nLDR - light\n\nDependencies\npip3 install paho-mqtt pyserial crcmod\nConfig\n\nSerial - 9600\nLDR - A0\nDHT11 - 4\nBMP085 - 0x77\n\nPublishing to MQTT\nserToMQTT.py takes the data published by the Arduino and forwards it to an MQTT broker.\nIt takes the name of the serial port and name of the room it is in as parameters.\npython3 serToMQTT.py \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nAn alternative to python is the q versionserToMQTT.q\nq serToMQTT.q -q \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nYou can subscribe from the command line to confirm your data is publishing to your broker:\nmosquitto_sub -h 192.168.1.111 -t \"hassio\/#\"\nAdd as sensors to Home Assistant\nAdd to configuration.yaml:\nsensor:\n  platform: mqtt\n  name: \"Living Room Temperature\"\n  state_topic: \"hassio\/livingroom\/temperature\"\n  qos: 0\n  unit_of_measurement: \"\u00baC\"\n\nsensor 2:\n  platform: mqtt\n  name: \"Living Room Humidity\"\n  state_topic: \"hassio\/livingroom\/humidity\"\n  qos: 0\n  unit_of_measurement: \"%\"\n\nsensor 3:\n  platform: mqtt\n  name: \"Living Room Pressure\"\n  state_topic: \"hassio\/livingroom\/pressure\"\n  qos: 0\n  unit_of_measurement: \"hPa\"\n\nsensor 4:\n  platform: mqtt\n  name: \"Living Room Light\"\n  state_topic: \"hassio\/livingroom\/light\"\n  qos: 0\n  unit_of_measurement: \"\/1024\"\nAdding sensors to Lovelace Dashboard\nTo add to a Lovelace dashboard:\nentities:\n  - entity: sensor.living_room_temperature\n  - entity: sensor.living_room_humidity\n  - entity: sensor.living_room_pressure\n  - entity: sensor.living_room_light\nshow_icon: true\nshow_name: false\nshow_state: true\ntitle: Living Room\ntype: glance\n","49":"Environmental Monitor\nData captured\nData is published to serial bus once per second as a comma delimited string.\nExample:\n26.70,35,736,1013,-5.91,26421\n\n\nTemperature - Celsius\nHumidity - Percent\nLight - Analog value between 0 and 1023\nPressure - Pa\nAltitude  - m (Not accurate)\nCRC-16 - checksum of data fields\n\nSensors\n\nDHT11 - temperature + humidity\nBMP085 - temperature + pressure\nLDR - light\n\nDependencies\npip3 install paho-mqtt pyserial crcmod\nConfig\n\nSerial - 9600\nLDR - A0\nDHT11 - 4\nBMP085 - 0x77\n\nPublishing to MQTT\nserToMQTT.py takes the data published by the Arduino and forwards it to an MQTT broker.\nIt takes the name of the serial port and name of the room it is in as parameters.\npython3 serToMQTT.py \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nAn alternative to python is the q versionserToMQTT.q\nq serToMQTT.q -q \"192.168.1.111\" \"\/dev\/ttyACM0\" \"livingroom\"\nYou can subscribe from the command line to confirm your data is publishing to your broker:\nmosquitto_sub -h 192.168.1.111 -t \"hassio\/#\"\nAdd as sensors to Home Assistant\nAdd to configuration.yaml:\nsensor:\n  platform: mqtt\n  name: \"Living Room Temperature\"\n  state_topic: \"hassio\/livingroom\/temperature\"\n  qos: 0\n  unit_of_measurement: \"\u00baC\"\n\nsensor 2:\n  platform: mqtt\n  name: \"Living Room Humidity\"\n  state_topic: \"hassio\/livingroom\/humidity\"\n  qos: 0\n  unit_of_measurement: \"%\"\n\nsensor 3:\n  platform: mqtt\n  name: \"Living Room Pressure\"\n  state_topic: \"hassio\/livingroom\/pressure\"\n  qos: 0\n  unit_of_measurement: \"hPa\"\n\nsensor 4:\n  platform: mqtt\n  name: \"Living Room Light\"\n  state_topic: \"hassio\/livingroom\/light\"\n  qos: 0\n  unit_of_measurement: \"\/1024\"\nAdding sensors to Lovelace Dashboard\nTo add to a Lovelace dashboard:\nentities:\n  - entity: sensor.living_room_temperature\n  - entity: sensor.living_room_humidity\n  - entity: sensor.living_room_pressure\n  - entity: sensor.living_room_light\nshow_icon: true\nshow_name: false\nshow_state: true\ntitle: Living Room\ntype: glance\n","50":"Environmental Sensing with AWS IoT\nA project for environmental sensing - pH, various gases, particulate matter, sound and capturing images\nThis is multi-part project to demonstrate environmental monitoring using several types of sensors and AWS services. The first part focuses on building the hardware and monitoring environmental parameters such as CO2, particulate matter, sound and capturing images. This project can be used for environmental monitoring in several use cases such as Industrial Manufacturing, Distribution Warehouses etc.\nThe first version of the environmental sensor box and the software along with it is capable of the following features:\n\nSense temperature, humidity, pressure, CO2, tvoc, proximity, and range and publish them to AWS IoT Core.\nCapture and upload images to an Amazon S3 bucket.\nThe project also uses the AWS Systems Manager to enable remote access to the Raspberry Pi.\nHow to use AWS IoT Rules to log data to Amazon Elasticsearch Service\n\nA second version of the project retains the temperature, humidity, pressure, CO2, tvoc, proximity, and replaces the camera and the range sensor with a pH sensor from Atlas Scientific in a smaller enclosure. The source code remains the same.\nApplications\npH\nThe pH sensor can be used to monitor several industrial and agricultural parameters:\n\nDough Fermentation\nSoil pH - different crops and plants need specific levels of pH in the soil.\n\nHardware and Mechanical\nList of off the shelf hardware used to build the environmental sensing unit:\n\nRaspberry Pi Zero W\nSparkFun Qwiic Kit for Raspberry Pi\nUltrasonic Range Finder - HRXL-MaxSonar-VR\n\nThe unit supports a camera with Raspberry Pi to capture images and load them to AWS S3.\nThe camera used:\n\nArducam Lens Board SKU B0031\n\nYou could use any other compatible camera as well.\nHere is the information on the pH sensor, pH reading circuit and the smaller enclosure:\n\nAtlas Scientific Spear Tip pH Probe\nEZO pH Circuit\nEZO Carrier Board\nSmaller Enclosure\n\nPlease note that the code was tested with an earlier version of the Spear Tip probe and EZO circuits - however the new probe and circuit will work fine. Also important to note that by default the carrier board is in UART mode, and has to be re-programmed to support the I2C mode which is used in this project. The carrier board interfaces with a Qwiic cable to the Qwiic hat.\nA baseplate was designed and 3D printed to house the components in the following case bought from Amazon.com:\n\nUniversal Project Enclosure\n\nHere is an image of the assembled unit:\n\nThe first iteration of this does not support the PM2.5 (Particulate Matter 2.5) sensing - but you can see the sensor PMS7003 below the camera in the image.\nHere is an image of the version with pH support:\n\nArchitecture\nHere is a high level architecture of the first iteration\/part of this project:\n\nConfigure & Setup AWS IoT\nRead through Setting up AWS IoT and Create a Thing. Once you have created a thing for the Raspberry Pi - make sure that the keys are present in the 'keys' subfolder. Also copy the sampleenv file as .env and provide the specifics - you will need these\nLogging data to AWS IoT Core\nPrior to logging data - you should take the sampleenv file - copy it to .env and then make sure that al parameters are set correctly.\nRun the script rpiQwiicAWSIoT.py to read sensor data and log to AWS IoT Core. The script calls two helper modules:\n\nimageCapture.py - to capture an image using hte picamera package, and upload it to S3. Note that the S3 bucket name has to be specified in the .env file.\nultraSonic.py - this script reads the value from the Range Finder sensor if connected.\n\nTwo flags are used to control the import and execution of the above modules - S3_ENABLE and ULTRA_ENABLE in the .env file.\n\nIf you want to exclude both or one of them - set the flag to an emptry string.\nIf the module is to be included then set it to 'True'\n\nSee example below:\nS3_ENABLE='True'\nULTRA_ENABLE=''\nOnce the .env file is configured correctly - you can start tbe execution as follows:\npython3 rpiQwiicAWSIoT.py\nIf you want to run the program in the background, and ensure it keeps running when you disconnect your remote session in to the Raspberry Pi then execute the following:\nnohup python3 -u .\/rpiQwiicAWSIoT.py > output.log &\n\nYou can now go to the \"Test\" secion of the AWS IoT Console, and subscribe to the topic that you are using. The topic used for testing was the following:\ntelemetry\/<thing_name>\nSample output from the unit:\n{\n  \"timestamp\": 1583361438,\n  \"time\": \"03-04-2020 17:37:17\",\n  \"tempc\": 27.63,\n  \"tempf\": 81.752,\n  \"humidity\": 34.475,\n  \"pressure\": 106.863,\n  \"tvoc\": 0,\n  \"co2\": 400,\n  \"proximity\": 2556,\n  \"ambient\": 128,\n  \"image\": \"pzb827ebed3f9a-1583361438\"\n}\nThe data above is not using the Ultrasonic sensor readings and does not have them. The data packet also provides the image that was captured and the filename used to store in the S3 bucket.\nStoring data with Amazon Elasticsearch\nWe will use Amazon Elasticsearch to store transformed data and later on use it for visualization. Setup a Amazon Elasticsearch in the same region as you have used to create the IoT thing. Once your Elasticsearch is setup, go to the AWS IoT console, and setup an IoT rule. See the image below on how it is setup.\nThis the specific IoT rule being used to transform the incoming packets:\nSELECT topic(2) as thing_name, timestamp, parse_time(\"yyyy-MM-dd'T'HH:mm:ssZZ\", timestamp(), \"America\/New_York\" ) as ts, tempf, tempc, humidity, pressure, co2, ambient, tvoc, proximity FROM 'telemetry\/+'\nThis rule does minor transformation of the incoming data message, and the output should look as follows:\n{\n  \"thing_name\": \"pzb827ebed3f9a\",\n  \"timestamp\": 1583361502,\n  \"ts\": \"2020-03-04T17:38:26-05:00\",\n  \"tempf\": 82.058,\n  \"tempc\": 27.81,\n  \"humidity\": 34.288,\n  \"pressure\": 106.548,\n  \"co2\": 400,\n  \"ambient\": 125,\n  \"tvoc\": 0,\n  \"proximity\": 2555\n}\n","51":"EnvironmentalPollution\n","52":"EnvironmentalMonitoring\narduino code\n","53":"EnvironmentalMonitoring\narduino code\n","54":"EnvironmentalMonitoring\narduino code\n","55":"Environmental Monitor\nArduino code for monitoring environmental conditions.\nBased on an Adafruit Feather M0 WiFi - ATSAMD21 + ATWINC1500 with a Bosch BME280 temperature\/humidity\/pressure sensor and a Modern Device Wind Sensor Rev. C.\nSimple data acquisition writes a JSON object over the USB serial port. Client software reads the JSON messages and writes them to disk as well as publishing them over ZeroMQ to a PyQt GUI.\nAdded reader for an Elitech RC-5 temperature logger based on the elitech-datareader.\n","56":"EnvironmentalWebsite\nStatic Website\n","57":" ,-----.,--.                  ,--. ,---.   ,--.,------.  ,------.\n'  .--.\/|  | ,---. ,--.,--. ,-|  || o   \\  |  ||  .-.  \\ |  .---'\n|  |    |  || .-. ||  ||  |' .-. |`..'  |  |  ||  |  \\  :|  `--, \n'  '--'\\|  |' '-' ''  ''  '\\ `-' | .'  \/   |  ||  '--'  \/|  `---.\n `-----'`--' `---'  `----'  `---'  `--'    `--'`-------' `------'\n----------------------------------------------------------------- \n\nWelcome to your Rails project on Cloud9 IDE!\nTo get started, just do the following:\n\nRun the project with the \"Run Project\" button in the menu bar on top of the IDE.\nPreview your new app by clicking on the URL that appears in the Run panel below (https:\/\/HOSTNAME\/).\n\nHappy coding!\nThe Cloud9 IDE team\nSupport & Documentation\nVisit http:\/\/docs.c9.io for support, or to learn more about using Cloud9 IDE.\nTo watch some training videos, visit http:\/\/www.youtube.com\/user\/c9ide\n","58":" ,-----.,--.                  ,--. ,---.   ,--.,------.  ,------.\n'  .--.\/|  | ,---. ,--.,--. ,-|  || o   \\  |  ||  .-.  \\ |  .---'\n|  |    |  || .-. ||  ||  |' .-. |`..'  |  |  ||  |  \\  :|  `--, \n'  '--'\\|  |' '-' ''  ''  '\\ `-' | .'  \/   |  ||  '--'  \/|  `---.\n `-----'`--' `---'  `----'  `---'  `--'    `--'`-------' `------'\n----------------------------------------------------------------- \n\nWelcome to your Rails project on Cloud9 IDE!\nTo get started, just do the following:\n\nRun the project with the \"Run Project\" button in the menu bar on top of the IDE.\nPreview your new app by clicking on the URL that appears in the Run panel below (https:\/\/HOSTNAME\/).\n\nHappy coding!\nThe Cloud9 IDE team\nSupport & Documentation\nVisit http:\/\/docs.c9.io for support, or to learn more about using Cloud9 IDE.\nTo watch some training videos, visit http:\/\/www.youtube.com\/user\/c9ide\n","59":" ,-----.,--.                  ,--. ,---.   ,--.,------.  ,------.\n'  .--.\/|  | ,---. ,--.,--. ,-|  || o   \\  |  ||  .-.  \\ |  .---'\n|  |    |  || .-. ||  ||  |' .-. |`..'  |  |  ||  |  \\  :|  `--, \n'  '--'\\|  |' '-' ''  ''  '\\ `-' | .'  \/   |  ||  '--'  \/|  `---.\n `-----'`--' `---'  `----'  `---'  `--'    `--'`-------' `------'\n----------------------------------------------------------------- \n\nWelcome to your Rails project on Cloud9 IDE!\nTo get started, just do the following:\n\nRun the project with the \"Run Project\" button in the menu bar on top of the IDE.\nPreview your new app by clicking on the URL that appears in the Run panel below (https:\/\/HOSTNAME\/).\n\nHappy coding!\nThe Cloud9 IDE team\nSupport & Documentation\nVisit http:\/\/docs.c9.io for support, or to learn more about using Cloud9 IDE.\nTo watch some training videos, visit http:\/\/www.youtube.com\/user\/c9ide\n","60":"EnvironmentalConfiguration\n\n\nA lightweight configuration library for .NET projects, built to support configuration for multiple environments from within the same configuration file.\n","61":"Environmental_Sound_Classification_ESC-50\nClassification of Environmental Sound using Deep Learning.\n","62":"EnvironmentalViolence\nEnvironmentalViolence\n","63":"BODO\nPetroleum analysis in Niger Delta\n","64":"DAT257\/DIT257- Agile software project management - Team Phish\nUN sustainability project for course in agile management.\nThe goal selected was Goal 13 Climate Action\nTeam members\nUsernames  - Real name\nPontare25\nDarclander\nadrianhak\nCladnic\nmunchgar\nHersiZa\nMotivation\nThis project was created as part of a course (DAT257\/DIT257- Agile software project management) taught at Chalmers University in collaboration with the University of Gothenburg. The primary motivateion behind the project was to learn to use Agile (and SCRUM) as a developer framework.\nInstallation\nTo run the application we recommend using IntelliJ.\n\nClone the repository\nRun the application using the Maven tab, scroll down to JavaFX and use the javaFX:run option (In IntelliJ the Maven tab is located at the top right).\n\n\nOBS! This is a Maven project using dependencies from JavaFX. Large portions of the program will run using the regular run button, but some dependencies require the maven support.\nHow to use\nFirst follow the steps in Installation. Once you have run the application using the Maven tab the application will urge you to either sign in or register.\n\nRegister: Register by entering the prompts.\nLog in: Either log in using the credentials you entered in the register section or use the Demo credentials. Username: Demo, Password: pass.\n\nOnce logged in you are greted with a homescreen with some information about the application and the UN goal 13\n\nAt the top you will find the global navigator where the main sectins are: Home, Calculator, Statistics, Vehicles, and All Emissions.\nIn the Calculator section you will be able to log all the relevant activity data for your personal emissions, for example, food and personal transport\n\n\nFrom which you can add both vehicles and activities. You need to have registered a vehicle to add a transport activity.\nAt the bottom you will find the Results section which summarises the different emission activities and allows for filteering of categories and dates.\n\nFor more in depth knowledge of the emissions we turn to the global navigator Statistics section\n\nHere we can more graphically break down exactly which activities are most responsible for the emissions.\n\n\nAPI Reference\n\nThis project was developed using java version 14.01. There have been some issues with using older versions so if there are issues with running we recommend using this version or later. These are set in the pom.xml file and the .iml file.\nMaven 4.0.0\nDateAxis\n\nBelow are a list of the external dependencies handled by Maven (see pom.xml file)\n\nMaven compiler version 3.8.0\njavafx-maven-plugin version 0.0.4\nFor JavaFX we use org.openjfx version 14\nFor SQLite Database we use version 3.32.3\n\nFrameworks used\n\nGUI: JavaFX (openjfx) 14\nDatabase: SQLite, SQLite dependency for Maven\nFor managing dependencies Maven has been used.\n\nCredits\n\nDateAxis for charts: Christian Schudt and Diego Cirujano\nSetting up JavaFX and Maven: ByteSmyth Easiest JavaFX Setup, IntelliJ + Maven with Debugger (2020)\nOrganising JavaFX windows: Software Development Tutorials GUI Application Development using JavaFX with Scene Builder\nUN Sustainability goal 13 graphics\n\n","65":"DAT257\/DIT257- Agile software project management - Team Phish\nUN sustainability project for course in agile management.\nThe goal selected was Goal 13 Climate Action\nTeam members\nUsernames  - Real name\nPontare25\nDarclander\nadrianhak\nCladnic\nmunchgar\nHersiZa\nMotivation\nThis project was created as part of a course (DAT257\/DIT257- Agile software project management) taught at Chalmers University in collaboration with the University of Gothenburg. The primary motivateion behind the project was to learn to use Agile (and SCRUM) as a developer framework.\nInstallation\nTo run the application we recommend using IntelliJ.\n\nClone the repository\nRun the application using the Maven tab, scroll down to JavaFX and use the javaFX:run option (In IntelliJ the Maven tab is located at the top right).\n\n\nOBS! This is a Maven project using dependencies from JavaFX. Large portions of the program will run using the regular run button, but some dependencies require the maven support.\nHow to use\nFirst follow the steps in Installation. Once you have run the application using the Maven tab the application will urge you to either sign in or register.\n\nRegister: Register by entering the prompts.\nLog in: Either log in using the credentials you entered in the register section or use the Demo credentials. Username: Demo, Password: pass.\n\nOnce logged in you are greted with a homescreen with some information about the application and the UN goal 13\n\nAt the top you will find the global navigator where the main sectins are: Home, Calculator, Statistics, Vehicles, and All Emissions.\nIn the Calculator section you will be able to log all the relevant activity data for your personal emissions, for example, food and personal transport\n\n\nFrom which you can add both vehicles and activities. You need to have registered a vehicle to add a transport activity.\nAt the bottom you will find the Results section which summarises the different emission activities and allows for filteering of categories and dates.\n\nFor more in depth knowledge of the emissions we turn to the global navigator Statistics section\n\nHere we can more graphically break down exactly which activities are most responsible for the emissions.\n\n\nAPI Reference\n\nThis project was developed using java version 14.01. There have been some issues with using older versions so if there are issues with running we recommend using this version or later. These are set in the pom.xml file and the .iml file.\nMaven 4.0.0\nDateAxis\n\nBelow are a list of the external dependencies handled by Maven (see pom.xml file)\n\nMaven compiler version 3.8.0\njavafx-maven-plugin version 0.0.4\nFor JavaFX we use org.openjfx version 14\nFor SQLite Database we use version 3.32.3\n\nFrameworks used\n\nGUI: JavaFX (openjfx) 14\nDatabase: SQLite, SQLite dependency for Maven\nFor managing dependencies Maven has been used.\n\nCredits\n\nDateAxis for charts: Christian Schudt and Diego Cirujano\nSetting up JavaFX and Maven: ByteSmyth Easiest JavaFX Setup, IntelliJ + Maven with Debugger (2020)\nOrganising JavaFX windows: Software Development Tutorials GUI Application Development using JavaFX with Scene Builder\nUN Sustainability goal 13 graphics\n\n","66":"DAT257\/DIT257- Agile software project management - Team Phish\nUN sustainability project for course in agile management.\nThe goal selected was Goal 13 Climate Action\nTeam members\nUsernames  - Real name\nPontare25\nDarclander\nadrianhak\nCladnic\nmunchgar\nHersiZa\nMotivation\nThis project was created as part of a course (DAT257\/DIT257- Agile software project management) taught at Chalmers University in collaboration with the University of Gothenburg. The primary motivateion behind the project was to learn to use Agile (and SCRUM) as a developer framework.\nInstallation\nTo run the application we recommend using IntelliJ.\n\nClone the repository\nRun the application using the Maven tab, scroll down to JavaFX and use the javaFX:run option (In IntelliJ the Maven tab is located at the top right).\n\n\nOBS! This is a Maven project using dependencies from JavaFX. Large portions of the program will run using the regular run button, but some dependencies require the maven support.\nHow to use\nFirst follow the steps in Installation. Once you have run the application using the Maven tab the application will urge you to either sign in or register.\n\nRegister: Register by entering the prompts.\nLog in: Either log in using the credentials you entered in the register section or use the Demo credentials. Username: Demo, Password: pass.\n\nOnce logged in you are greted with a homescreen with some information about the application and the UN goal 13\n\nAt the top you will find the global navigator where the main sectins are: Home, Calculator, Statistics, Vehicles, and All Emissions.\nIn the Calculator section you will be able to log all the relevant activity data for your personal emissions, for example, food and personal transport\n\n\nFrom which you can add both vehicles and activities. You need to have registered a vehicle to add a transport activity.\nAt the bottom you will find the Results section which summarises the different emission activities and allows for filteering of categories and dates.\n\nFor more in depth knowledge of the emissions we turn to the global navigator Statistics section\n\nHere we can more graphically break down exactly which activities are most responsible for the emissions.\n\n\nAPI Reference\n\nThis project was developed using java version 14.01. There have been some issues with using older versions so if there are issues with running we recommend using this version or later. These are set in the pom.xml file and the .iml file.\nMaven 4.0.0\nDateAxis\n\nBelow are a list of the external dependencies handled by Maven (see pom.xml file)\n\nMaven compiler version 3.8.0\njavafx-maven-plugin version 0.0.4\nFor JavaFX we use org.openjfx version 14\nFor SQLite Database we use version 3.32.3\n\nFrameworks used\n\nGUI: JavaFX (openjfx) 14\nDatabase: SQLite, SQLite dependency for Maven\nFor managing dependencies Maven has been used.\n\nCredits\n\nDateAxis for charts: Christian Schudt and Diego Cirujano\nSetting up JavaFX and Maven: ByteSmyth Easiest JavaFX Setup, IntelliJ + Maven with Debugger (2020)\nOrganising JavaFX windows: Software Development Tutorials GUI Application Development using JavaFX with Scene Builder\nUN Sustainability goal 13 graphics\n\n","67":"EnvironmentalSensors\nhttps:\/\/user-images.githubusercontent.com\/12248815\/29771433-53faa10c-8c11-11e7-82ce-36de73631b2d.png\n","68":"Environmental Monitor (Raspberry Pi Zero W)\n\nDocumentation on Wordpress\nIntro\nThe final assignment is to create a environmental monitor that is connected to internet and send data to a serve\/database.\n","69":"Environmental Monitor (Raspberry Pi Zero W)\n\nDocumentation on Wordpress\nIntro\nThe final assignment is to create a environmental monitor that is connected to internet and send data to a serve\/database.\n","70":"EnvironmentalEducationGame\n","71":"EnvironmentalDataPredict\nEnvironmental Data Predict\n","72":"EnvironmentalDataPredict\nEnvironmental Data Predict\n","73":"umi project\nGetting Started\nInstall dependencies,\n$ yarn\nStart the dev server,\n$ yarn start\n","74":"Indoor Environmental Monitoring System\nOverview\nOur project is an Indoor environment monitor. The project's goal is to monitor, record,\nand display time series data for a given indoor environment. The sensors used in\nthis project will include a thermometer, humidity sensor, pressure sensor,\nlight detector, carbon dioxide, accelerometer, microphone and an airborne particle\ndetector. This data will be collected, recorded and displayed for the client on a central\nhub and a basic Android application as well as a REST interface for external integration.\nImplementation Details\nWe'll be using multiple microcontrollers to handle data collection and pre-processing.\nOur main microcontroller will handle the data collection and communication with the hub.\nAny secondary microcontrollers will be used to preprocess data in the event of a sampling rate\nfar higher than the communication rate, such as acoustic data and the accelerometer. This data\nwill be sent over WiFi\/HTTP over to the server which will store the data in MongoDB. MongoDB has\nbuilt in support for time series data, as well as a non-relational structure and low read\/write overhead\nwhich makes it ideal for IoT style applications such as this. This data is then exposed over a REST\nAPI, for easy querying from our Android application or any other service the user wants to integrate\nour system with.\nResources\nAnticipated cost is around $400 CAD. The bulk of this will be in the microcontroller, we expect to\nspend around $100 CAD on the microcontrollers, with most of the rest being sensors and other hardware.\nThe sensors, depending on their sensitivities and sampling rates, could be as expensive as $50 CAD,\nhowever the average will be far less than that. The balance will be tied up in packaging and\nother smaller items.\n","75":"Indoor Environmental Monitoring System\nOverview\nOur project is an Indoor environment monitor. The project's goal is to monitor, record,\nand display time series data for a given indoor environment. The sensors used in\nthis project will include a thermometer, humidity sensor, pressure sensor,\nlight detector, carbon dioxide, accelerometer, microphone and an airborne particle\ndetector. This data will be collected, recorded and displayed for the client on a central\nhub and a basic Android application as well as a REST interface for external integration.\nImplementation Details\nWe'll be using multiple microcontrollers to handle data collection and pre-processing.\nOur main microcontroller will handle the data collection and communication with the hub.\nAny secondary microcontrollers will be used to preprocess data in the event of a sampling rate\nfar higher than the communication rate, such as acoustic data and the accelerometer. This data\nwill be sent over WiFi\/HTTP over to the server which will store the data in MongoDB. MongoDB has\nbuilt in support for time series data, as well as a non-relational structure and low read\/write overhead\nwhich makes it ideal for IoT style applications such as this. This data is then exposed over a REST\nAPI, for easy querying from our Android application or any other service the user wants to integrate\nour system with.\nResources\nAnticipated cost is around $400 CAD. The bulk of this will be in the microcontroller, we expect to\nspend around $100 CAD on the microcontrollers, with most of the rest being sensors and other hardware.\nThe sensors, depending on their sensitivities and sampling rates, could be as expensive as $50 CAD,\nhowever the average will be far less than that. The balance will be tied up in packaging and\nother smaller items.\n","76":"Indoor Environmental Monitoring System\nOverview\nOur project is an Indoor environment monitor. The project's goal is to monitor, record,\nand display time series data for a given indoor environment. The sensors used in\nthis project will include a thermometer, humidity sensor, pressure sensor,\nlight detector, carbon dioxide, accelerometer, microphone and an airborne particle\ndetector. This data will be collected, recorded and displayed for the client on a central\nhub and a basic Android application as well as a REST interface for external integration.\nImplementation Details\nWe'll be using multiple microcontrollers to handle data collection and pre-processing.\nOur main microcontroller will handle the data collection and communication with the hub.\nAny secondary microcontrollers will be used to preprocess data in the event of a sampling rate\nfar higher than the communication rate, such as acoustic data and the accelerometer. This data\nwill be sent over WiFi\/HTTP over to the server which will store the data in MongoDB. MongoDB has\nbuilt in support for time series data, as well as a non-relational structure and low read\/write overhead\nwhich makes it ideal for IoT style applications such as this. This data is then exposed over a REST\nAPI, for easy querying from our Android application or any other service the user wants to integrate\nour system with.\nResources\nAnticipated cost is around $400 CAD. The bulk of this will be in the microcontroller, we expect to\nspend around $100 CAD on the microcontrollers, with most of the rest being sensors and other hardware.\nThe sensors, depending on their sensitivities and sampling rates, could be as expensive as $50 CAD,\nhowever the average will be far less than that. The balance will be tied up in packaging and\nother smaller items.\n","77":"This app had been deployed to get some data (such tempreture) using nodeMCU and send them to server to drow graphs and show them to owner client.\n","78":"This app had been deployed to get some data (such tempreture) using nodeMCU and send them to server to drow graphs and show them to owner client.\n","79":"YAPEC\nYet Another Parser for Environmental Configuration\nBecause there just aren't enough npm modules for getting config values from your enviroment already!\nTravis Status\n\n\nInstallation\nBe sane, use npm\n$ npm install yapec\notherwise clone this repo via git\n$ git clone https:\/\/github.com\/sandfox\/node-yapec.git\nUsage\nvar yapec = require('yapec');\nvar config = yapec(['PRE_FIX'], configSpec, process.env, opts);\nyapec takes a spec in the form of an object (which can be nested to your heart's content) where the leaf of every path must be a string which dictates how to parse the corresponding ENV_VAR string.\nThe path itself is converted into UPPERCASE, and dot seperators exchanged for underscores*.\nAn optional prefix may be supplied as the first arg which will act as a mask when searching the env object. An optional options object may be supplied, so far the only option is 'ignoreMissing' that accepts a bool, is false by default, and when true rather than throwing an exception if an ENV VAR is missing, instead returns a null for that value.\n*yes I realise this is probably not the clearest way to describe what it does but my brain is failing me at this point in time\nExamples\nSome of these examples can also be found in the examples folder inside this project.\nvar yapec = require('yapec');\n\n\/\/Represents something we could expect process.env to return\nvar env = {\n    APP_PATH: '\/opt\/appy',\n    APP_NAME: 'super server',\n    APP_SERVER_ENABLED: 'true',\n    APP_SERVER_PROCS: '8',\n    APP_SERVER_MAGIC: '2e2'\n}\n\nvar configSpec = {\n    app : {\n        path: 'string',\n        name: 'string',\n        server: {\n            enabled: 'bool',\n            procs: 'int',\n            magic: 'float'\n        }\n    }\n}\n\nvar config = yapec(configSpec, env);\n\nconsole.log(config)\n\/\/outputs the following\n{ app:\n   { path: '\/opt\/appy',\n     name: 'super server',\n     server: {\n        enabled: true,\n        procs: 8,\n        magic: 200 }\n    }\n}\nOptionally a prefix can be supplied as the first arguement which acts as a mask when looking through the enviroment variables.\nExample\nvar yapec = require('yapec');\n\nvar env = {\n    FALLOVER: 'true',\n    MY_APP_FALLOVER: 'false'\n}\n\nvar spec = {\n    fallover: 'bool'\n}\n\nvar config = yapec('MY_APP_', spec, env);\n\nconsole.log(config);\n\n{fallover:false}\nCaveats\nDue to the way this modules works certain combinations of ENV VAR strings are forbidden, for example the following would fail because it could not be resolved into an object in any sane way because app could not be both a string and object at the same time.\nAPP=\"super app\"\nAPP_DB_NAME=\"megadb\"\nAPP_DB_PORT=\"8000\"\nHelpers\nyapec also comes with helpers for creating configs from process.env style objects and for creating ENV VAR strings from a config object. Checkout the examples folder as it should be pretty self explanatory. todo - document this better\nyapec.getSpec([prefix], process.env)\nand\nyapec.getEnvStrings([prefix], config)\nStability Index\nBased up on node.js stability index\nStability: 2 - Unstable\nTesting\nCode is tested with mocha + should, just run npm test as usual.\nThe tests aren't bad, but they could be more complete. There are travis tests too!\nUpgrades, fixes, ideas\nAll ideas, bug fixes, suggestions etc are gladly excepted so feel free to raise pull requests and issues.\nLicense\n(The MIT License)\nCopyright (c) 2013 James Edward Butler AKA sandfox\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","80":"YAPEC\nYet Another Parser for Environmental Configuration\nBecause there just aren't enough npm modules for getting config values from your enviroment already!\nTravis Status\n\n\nInstallation\nBe sane, use npm\n$ npm install yapec\notherwise clone this repo via git\n$ git clone https:\/\/github.com\/sandfox\/node-yapec.git\nUsage\nvar yapec = require('yapec');\nvar config = yapec(['PRE_FIX'], configSpec, process.env, opts);\nyapec takes a spec in the form of an object (which can be nested to your heart's content) where the leaf of every path must be a string which dictates how to parse the corresponding ENV_VAR string.\nThe path itself is converted into UPPERCASE, and dot seperators exchanged for underscores*.\nAn optional prefix may be supplied as the first arg which will act as a mask when searching the env object. An optional options object may be supplied, so far the only option is 'ignoreMissing' that accepts a bool, is false by default, and when true rather than throwing an exception if an ENV VAR is missing, instead returns a null for that value.\n*yes I realise this is probably not the clearest way to describe what it does but my brain is failing me at this point in time\nExamples\nSome of these examples can also be found in the examples folder inside this project.\nvar yapec = require('yapec');\n\n\/\/Represents something we could expect process.env to return\nvar env = {\n    APP_PATH: '\/opt\/appy',\n    APP_NAME: 'super server',\n    APP_SERVER_ENABLED: 'true',\n    APP_SERVER_PROCS: '8',\n    APP_SERVER_MAGIC: '2e2'\n}\n\nvar configSpec = {\n    app : {\n        path: 'string',\n        name: 'string',\n        server: {\n            enabled: 'bool',\n            procs: 'int',\n            magic: 'float'\n        }\n    }\n}\n\nvar config = yapec(configSpec, env);\n\nconsole.log(config)\n\/\/outputs the following\n{ app:\n   { path: '\/opt\/appy',\n     name: 'super server',\n     server: {\n        enabled: true,\n        procs: 8,\n        magic: 200 }\n    }\n}\nOptionally a prefix can be supplied as the first arguement which acts as a mask when looking through the enviroment variables.\nExample\nvar yapec = require('yapec');\n\nvar env = {\n    FALLOVER: 'true',\n    MY_APP_FALLOVER: 'false'\n}\n\nvar spec = {\n    fallover: 'bool'\n}\n\nvar config = yapec('MY_APP_', spec, env);\n\nconsole.log(config);\n\n{fallover:false}\nCaveats\nDue to the way this modules works certain combinations of ENV VAR strings are forbidden, for example the following would fail because it could not be resolved into an object in any sane way because app could not be both a string and object at the same time.\nAPP=\"super app\"\nAPP_DB_NAME=\"megadb\"\nAPP_DB_PORT=\"8000\"\nHelpers\nyapec also comes with helpers for creating configs from process.env style objects and for creating ENV VAR strings from a config object. Checkout the examples folder as it should be pretty self explanatory. todo - document this better\nyapec.getSpec([prefix], process.env)\nand\nyapec.getEnvStrings([prefix], config)\nStability Index\nBased up on node.js stability index\nStability: 2 - Unstable\nTesting\nCode is tested with mocha + should, just run npm test as usual.\nThe tests aren't bad, but they could be more complete. There are travis tests too!\nUpgrades, fixes, ideas\nAll ideas, bug fixes, suggestions etc are gladly excepted so feel free to raise pull requests and issues.\nLicense\n(The MIT License)\nCopyright (c) 2013 James Edward Butler AKA sandfox\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","81":"GreenFerries\nImprove passenger ships environmental impact transparency\nLive website: greenferries.org\n\n\nEcosystem\n\n\nPublic Website: The\nmain public-facing static website. Uses the EleventyJS framework and TailwindCSS\n\n\nData platform:\ndatasets, APIs and iPython notebooks to explore the different original data\nsources that were used to create the GreenFerries database\n\n\nScrapers:\nScrapers used to populate data from ferries booking websites\n\n\nScreenshot\n\n","82":"EnvironmentalNewsAp\n","83":"Mhav env check on Jenkins\n","84":"Mhav env check on Jenkins\n","85":"Mhav env check on Jenkins\n","86":"Mhav env check on Jenkins\n","87":"GreenFerries\nImprove passenger ships environmental impact transparency\nLive website: greenferries.org\n\n\nEcosystem\n\n\nPublic Website: The\nmain public-facing static website. Uses the EleventyJS framework and TailwindCSS\n\n\nData platform:\ndatasets, APIs and iPython notebooks to explore the different original data\nsources that were used to create the GreenFerries database\n\n\nScrapers:\nScrapers used to populate data from ferries booking websites\n\n\nScreenshot\n\n","88":"EnvironmentalSampleLogger\nAn application which helps environmental consultants visualize the distribution of subsurface soil\nwhen drilling boreholes. The user can create a new borehole location, and add samples specify\nparameters such as colour, stratigraphy, and whether or not the sample is odourous. Save your\nprogress and come back to it later.\n\n\n","89":"AI_EnvironmentalLighting\nThis is a bit of a niche mod.  It is mainly intended for people that like to use DHH in the main game, but hate that it messes with the in game day\/night light cycle.  DHH replaces some of the ambient\/direct light that changes depending on time of day and position of the sun\/moon with a static light source.  This causes the night, and especially the moon light to be way too bright.  The mod restores the in game lighting to its normal behavior, but lets you use all of the other really neat features of DHH like color balancing.\nThis mod also adds some other features that work with or without DHH.  These features give you more control over the environmental lighting to adjust them as you see fit.  These features include:\nAutomatically blends the day light and moon light settings.  Normally in game there is a harsh transition from day light to moon light that you can see, this blends the transition so it looks like a more natural sunrise\/sunset.\nAllow you to control what style of ambient light to use, so you can swith to Skybox or Trilight (default) if you choose.  Skybox mode automatically calculates ambient light based off of the time of day.\nIncreases the number of skybox calculations during dawn\/dusk.  The default settings don't update rapidly enough during this transition period.\nIndependet contols of daytime and nighttime ambient light with blending between the day\/night cycles.\nAllows modifications to the direct sun and moon light settings.  Unlike DHH, these modify the base values instead of replace so it still follows the in game light calculations.\n","90":"ESResNet\nEnvironmental Sound Classification Based on Visual Domain Models\nThis repository contains implementation of the models described in the paper arXiv:2004.07301 (submitted to ICPR 2020).\nAbstract\nEnvironmental Sound Classification (ESC) is an active research area in the audio domain and has seen a lot of progress in the past years. However, many of the existing approaches achieve high accuracy by relying on domain-specific features and architectures, making it harder to benefit from advances in other fields (e.g., the image domain). Additionally, some of the past successes have been attributed to a discrepancy of how results are evaluated (i.e., on unofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall progression of the field.\nThe contribution of this paper is twofold. First, we present a model that is inherently compatible with mono and stereo sound inputs. Our model is based on simple log-power Short-Time Fourier Transform (STFT) spectrograms and combines them with several well-known approaches from the image domain (i.e., ResNet, Siamese-like networks and attention). We investigate the influence of cross-domain pre-training, architectural changes, and evaluate our model on standard datasets. We find that our model out-performs all previously known approaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10), 91.5 % (ESC-50) and 84.2 % \/ 85.4 % (US8K mono \/ stereo).\nSecond, we provide a comprehensive overview of the actual state of the field, by differentiating several previously reported results on the US8K dataset between official or unofficial splits. For better reproducibility, our code (including any re-implementations) is made available.\nHow to run the model\nThe required Python version is >= 3.7.\nESResNet\nOn the ESC-10 dataset\npython main.py --config protocols\/esc10\/esresnet-esc10-cv1.json --Dataset.args.root \/path\/to\/ESC10\n\nOn the ESC-50 dataset\npython main.py --config protocols\/esc50\/esresnet-esc50-cv1.json --Dataset.args.root \/path\/to\/ESC50\n\nOn the UrbanSound8K dataset (stereo)\npython main.py --config protocols\/us8k\/esresnet-us8k-stereo-cv1.json --Dataset.args.root \/path\/to\/UrbanSound8K\n\nReproduced results\nLMCNet on the UrbanSound8K dataset\npython main.py --config protocols\/us8k\/lmcnet-us8k-cv1.json --Dataset.args.root \/path\/to\/UrbanSound8K\n\n","91":"ESResNet\nEnvironmental Sound Classification Based on Visual Domain Models\nThis repository contains implementation of the models described in the paper arXiv:2004.07301 (submitted to ICPR 2020).\nAbstract\nEnvironmental Sound Classification (ESC) is an active research area in the audio domain and has seen a lot of progress in the past years. However, many of the existing approaches achieve high accuracy by relying on domain-specific features and architectures, making it harder to benefit from advances in other fields (e.g., the image domain). Additionally, some of the past successes have been attributed to a discrepancy of how results are evaluated (i.e., on unofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall progression of the field.\nThe contribution of this paper is twofold. First, we present a model that is inherently compatible with mono and stereo sound inputs. Our model is based on simple log-power Short-Time Fourier Transform (STFT) spectrograms and combines them with several well-known approaches from the image domain (i.e., ResNet, Siamese-like networks and attention). We investigate the influence of cross-domain pre-training, architectural changes, and evaluate our model on standard datasets. We find that our model out-performs all previously known approaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10), 91.5 % (ESC-50) and 84.2 % \/ 85.4 % (US8K mono \/ stereo).\nSecond, we provide a comprehensive overview of the actual state of the field, by differentiating several previously reported results on the US8K dataset between official or unofficial splits. For better reproducibility, our code (including any re-implementations) is made available.\nHow to run the model\nThe required Python version is >= 3.7.\nESResNet\nOn the ESC-10 dataset\npython main.py --config protocols\/esc10\/esresnet-esc10-cv1.json --Dataset.args.root \/path\/to\/ESC10\n\nOn the ESC-50 dataset\npython main.py --config protocols\/esc50\/esresnet-esc50-cv1.json --Dataset.args.root \/path\/to\/ESC50\n\nOn the UrbanSound8K dataset (stereo)\npython main.py --config protocols\/us8k\/esresnet-us8k-stereo-cv1.json --Dataset.args.root \/path\/to\/UrbanSound8K\n\nReproduced results\nLMCNet on the UrbanSound8K dataset\npython main.py --config protocols\/us8k\/lmcnet-us8k-cv1.json --Dataset.args.root \/path\/to\/UrbanSound8K\n\n","92":"ESResNet\nEnvironmental Sound Classification Based on Visual Domain Models\nThis repository contains implementation of the models described in the paper arXiv:2004.07301 (submitted to ICPR 2020).\nAbstract\nEnvironmental Sound Classification (ESC) is an active research area in the audio domain and has seen a lot of progress in the past years. However, many of the existing approaches achieve high accuracy by relying on domain-specific features and architectures, making it harder to benefit from advances in other fields (e.g., the image domain). Additionally, some of the past successes have been attributed to a discrepancy of how results are evaluated (i.e., on unofficial splits of the UrbanSound8K (US8K) dataset), distorting the overall progression of the field.\nThe contribution of this paper is twofold. First, we present a model that is inherently compatible with mono and stereo sound inputs. Our model is based on simple log-power Short-Time Fourier Transform (STFT) spectrograms and combines them with several well-known approaches from the image domain (i.e., ResNet, Siamese-like networks and attention). We investigate the influence of cross-domain pre-training, architectural changes, and evaluate our model on standard datasets. We find that our model out-performs all previously known approaches in a fair comparison by achieving accuracies of 97.0 % (ESC-10), 91.5 % (ESC-50) and 84.2 % \/ 85.4 % (US8K mono \/ stereo).\nSecond, we provide a comprehensive overview of the actual state of the field, by differentiating several previously reported results on the US8K dataset between official or unofficial splits. For better reproducibility, our code (including any re-implementations) is made available.\nHow to run the model\nThe required Python version is >= 3.7.\nESResNet\nOn the ESC-10 dataset\npython main.py --config protocols\/esc10\/esresnet-esc10-cv1.json --Dataset.args.root \/path\/to\/ESC10\n\nOn the ESC-50 dataset\npython main.py --config protocols\/esc50\/esresnet-esc50-cv1.json --Dataset.args.root \/path\/to\/ESC50\n\nOn the UrbanSound8K dataset (stereo)\npython main.py --config protocols\/us8k\/esresnet-us8k-stereo-cv1.json --Dataset.args.root \/path\/to\/UrbanSound8K\n\nReproduced results\nLMCNet on the UrbanSound8K dataset\npython main.py --config protocols\/us8k\/lmcnet-us8k-cv1.json --Dataset.args.root \/path\/to\/UrbanSound8K\n\n","93":"meds-demo\nTeaching demonstration for UCSB Masters in Environmental Data Science\nPlease visit for more on background, prerequisites and setup:\nhttps:\/\/benbestphd.com\/meds-demo\nwebsite\nThe website was generated using bookdown.\n","94":"Greening a PostIndustrial City:  Applying keyword extractor methods to monitor a fast-changing environmental narrative\nWorcester, Massachusetts (population just under 200,000) is the second largest city after Boston in Massachusetts, USA.  It was important in the American industrial revolution; factories dominated its landscape until the mid-20th century when the city slid physically, and mentally, into post-industrial decline.\nAt first glance, Worcester might present an all-too-familiar global story of gentrification, the loss of city identity and community, but does even a small survey of the historic record of local public discourse confirm this view? Or does a more complicated local story surface?\nTo answer these questions,  literary geographer Sarah Luria teams up with computer scientist Ricardo Campos, developer of the keyword extractor YAKE! to discover if it can create a helpful survey or image of stories told about a neighborhood over time. We believe this interdisciplinary work can play a crucial role by showing how Artificial Intelligence (AI) can track this fast-developing story of urban revitalization and environmental cleanup.\nFrom our survey we curated a small corpus of 26 English-language texts that described the Canal District\/Green Island over time. A majority of the texts are  from 2018-2019, but examples from 1862, 1917, the 1980s and 1990s provide some historic range. The corpus includes descriptions of the Canal District before the industrial revolution altered it dramatically, the building of its canal and railroad, the peak of its industrialization and identity as an Irish working class neighborhood,  its post-industrial decline, and its stages of revitalization. While a majority of texts come from the local major newspaper The Worcester Telegram Gazette, the corpus includes articles from New York Times, Boston Globe, and National Public Radio; also included are  Worcester-born poet Mary Fell\u2019s 1984 poem The Prophecy and historian Roy Rosenzweig\u2019s acclaimed history of Worcester\u2019s working class Eight Hours for What We Will (1985). A description of each one is given below.\nDataset details\nBelow we give some details of each text (sorted by date).\nFilename: 1 - 1862_Lincoln, from History of Worcester (archive.org) landscape description\nSource: William Lincoln, from History of Worcester Lincoln, William. History of Worcester, Massachusetts, from its earliest settlement to September, 1836 : with various notices relating to the history of Worcester County. Worcester: M.D. Phillips & Co., 1862.\nURL: https:\/\/archive.org\/details\/historyofworcest1836linc\/page\/n6\nPublication Date: 1862\nDescription: Worcester\u2019s landscape before the industrial revolution.\nTokens: 256\n\nFilename: 2 - 1879_Abijah Perkins Marvin, History of Worcester (archive.org), railroad descriptionn\nSource: Abijah Perikins Marvin, History of Worcester County, Massachusetts, embracing a comprehensive history of the country from its first settlement to the present time.  Boston; Jewett & Co, 1879, pp. 82-83.\nURL: https:\/\/archive.org\/details\/historyofworcest03marv\/page\/82\nPublication Date: 1879\nDescription: A description of the beginning of the Blackstone canal and the Worcester railroad.\nTokens: 638\n\nFilename: 3 - 1917_Washburn, Industrial History of Worcester (archive.org) re canal\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 23.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/n6\nPublication Date: 1917\nDescription: Description of the creation of the Blackstone canal and its importance\nTokens: 199\n\nFilename: 4 - 1917_Washburn, Industrial History of Worcester (archive.org) re entrepreneurial spirit of city\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 31.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/60\nPublication Date: 1917\nDescription: The introduction of steam-power to Worcester industries.\nTokens: 72\n\nFilename: 5 - 1917_Washburn, Industrial History, re importance of steam power\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 300.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/60\nPublication Date: 1917\nDescription: Entrepreneurial spirit of city.\nTokens: 150\n\nFilename: 6 - 1983-25-9_Worcester Shedding Smokestack Image (New York Times)\nSource: \u201cWorcester Shedding Smokestack Image,\u201d New York Times. Sept. 25, 1983.\nPublication Date: 1983-25-9\nDescription:\nTokens: 911\n\nFilename: 7 - 1984_Mary Fell, The Prophecy (Poem)\nSource: Mary Fell, \u201cThe Prophecy\u201d, from the Persistence of Memory. 1984.\nURL: http:\/\/capa.conncoll.edu\/fell.persistence.html\nPublication Date: 1984\nDescription: Poem collection about Worcester.\nTokens: 328\n\nFilename: 8 - 1985_Roy Rosenzweig, Eight Hours For What We Will (history of recreation for industrial labor)\nSource: Roy Rosenzweig, Eight Hours For What We Will. Cambridge, UK: Cambridge, 1985.\nPublication Date: 1985\nDescription: A history of recreation for Worcester\u2019s industrial labor force.\nTokens: 287\n\nFilename: 9 - 1989-10-12_A New Look for an Old Area (New York Times) (Lafayette Place\/Green Island)\nSource: \u201cWorcester, Mass.: A New Focus for an Old Area,\u201d New York Times.\nPublication Date: 1989-10-12\nDescription: The building of new senior affordable housing in Green Island.\nTokens: 450\n\nFilename: 10 - 1997-11-18_Bureau urges liability relief for brownfields, WTG\nSource: Bronislaus B. Kush, \u201cBureau urges liability relief for brownfields,\u201d Worcester Telegram Gazette.\nPublication Date: 1997-11-18\nDescription:\nTokens: 585\n\nFilename: 11 - 1998-16-7_Green Island Businesses Say City Help Is Killing Them, WTG\nSource: Bronislaus B. Kush, \u201cGreen Island Businesses Say City Help Is Killing Them,\u201d Worcester Telegram Gazette.\nPublication Date: 1998-16-7\nDescription:\nTokens: 597\n\nFilename: 12 - 1999-12-1_Vacant Industrial Sites of No Use to Neighborhood, WTG\nSource: Winston W. Wiley, \u201cVacant Industrial Sites of No Use to Neighborhood,\u201d Worcester Telegram Gazette.\nPublication Date: 1999-12-01\nDescription:\nTokens: 436\n\nFilename: 13 - 2000-29-6_Green Island Revitalization Plan Dropped, WTG\nSource: Lisa Eckelbecker, \u201cGreen Island Revitalization Plan Dropped,\u201d Worcester Telegram Gazette.\nPublication Date: 2000-29-6\nDescription:\nTokens: 446\n\nFilename: 14 - 2007-25-9_Canal District Shapes Up: Old Buildings, New Life on Green Street, WTG\nSource: Shaun Sutner, \u201cCanal District Shapes Up: Old Buildings, New Life on Green Street,\u201d Worcester Telegram Gazette.\nPublication Date: 2007-25-09\nDescription:\nTokens: 645\n\nFilename: 15 - 2011-23-8_Life in Green Island: We have Hope. In City Times (local alternative newspaper)\nSource: Maureen Schwab, \u201cLife in Green Island: We have Hope,\u201d In City Times (local alternative newspaper).\nPublication Date: 2011-23-08\nDescription:\nTokens: 1149\n\nFilename: 16 - 2018-8-17_WooSox Ball Park Has Long History. Boston Globe\nSource: Tim Logan, \u201cNew Home for WooSox Has Long History,\u201d Boston Globe.\nPublication Date: 2018-8-17\nDescription:\nTokens: 632\n\nFilename: 17 - 2018-10-11_Time to Talk About Gentrification in Worcester, Worcester Mag.\nSource: Bill Shaner, \u201cIt\u2019s Time to Talk About Gentrification in Worcester,\u201d Worcester Magazine.\nPublication Date: 2018-10-11\nDescription:\nTokens: 4165\n\nFilename: 18 - 2018-10-23_Worcester: The New  \u201cIt Town.\u201d National Public Radio\nSource: Aaron Schachter, \u201cWorcester: The New\u2018It\u2019 Town.\u201d National Public Radio: WGBH.\nPublication Date :2018-10-23\nDescription:\nTokens: 1184\n\nFilename: 19 - 2018_Worcester, A City Reclaimed.  Vitality Magazine\nSource: Bernard Whitmore, \u201cA Mayor, A Manager, A City Reclaimed.\u201d  Vitality Magazine.\nPublication Date: 2018-11\nDescription:\nTokens: 2077\n\nFilename: 20 - 2019-2-27_Worcester Organizers Hear from Nashville, Buffalo on Tips for WooSox CBA Push, Worcester Mag.\nSource: Bill Shaner, Worcester Organizers Hear from Nashville, Buffalo on Tips for WooSox CBA Push,\u201d Worcester Magazine.\nPublication Date: 2019-02-27\nDescription:\nTokens: 540\n\nFilename: 21_2019-4-10_A Totally Cool Place to Live. MassLive?\nSource: Aviva Luttrell, \u201cA Totally Cool Place to Live,\u201d MassLive.com.\nPublication Date: 2019-04-10\nDescription:\nTokens: 497\n\nFilename: 22 - 2019-6-1_New Shine for Old Building: Former Walker Shoe Factory to be Converted into Studios. WTG\nSource: Scott O\u2019Connell, \u201cNew Shine for Old Building: Former Walker Shoe Factory to be Converted into Studios,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-01\nDescription:\nTokens: 468\n\nFilename: 23 - 2019-6-20_Construction, WooSox, and Regulation are Killing Canal District Dreams. Worcester Business Journal\nSource: Renee Diaz, \u201cConstruction, WooSox, and Regulation are Killing Canal District Dreams,\u201d Worcester Business Journal.\nPublication Date: 2019-06-20\nDescription:\nTokens: 739\n\nFilename: 24 - 2019-6-24_Worcester Gets Brownfield Funds. WTG\nSource: \u201cWorcester Gets Brownfield Funds,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-24\nDescription:\nTokens: 517\n\nFilename: 25 - 2019-6-24_Worcester Pledges $3M to Green Island. WTG\nSource: Kim Ring, \u201cWorcester Pledges $3M to Green Island,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-24\nDescription:\nTokens: 490\n\nFilename: 26 - 2019-7-6_An Away Game for Businesses; Property Owners Near Ballpark Site Make Way for Development. WTG\nSource: \u201cAn Away Game for Businesses; Property Owners Near Ballpark Site Make Way for Development,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-07-06\nDescription:\nTokens: 1322\n\n","95":"Greening a PostIndustrial City:  Applying keyword extractor methods to monitor a fast-changing environmental narrative\nWorcester, Massachusetts (population just under 200,000) is the second largest city after Boston in Massachusetts, USA.  It was important in the American industrial revolution; factories dominated its landscape until the mid-20th century when the city slid physically, and mentally, into post-industrial decline.\nAt first glance, Worcester might present an all-too-familiar global story of gentrification, the loss of city identity and community, but does even a small survey of the historic record of local public discourse confirm this view? Or does a more complicated local story surface?\nTo answer these questions,  literary geographer Sarah Luria teams up with computer scientist Ricardo Campos, developer of the keyword extractor YAKE! to discover if it can create a helpful survey or image of stories told about a neighborhood over time. We believe this interdisciplinary work can play a crucial role by showing how Artificial Intelligence (AI) can track this fast-developing story of urban revitalization and environmental cleanup.\nFrom our survey we curated a small corpus of 26 English-language texts that described the Canal District\/Green Island over time. A majority of the texts are  from 2018-2019, but examples from 1862, 1917, the 1980s and 1990s provide some historic range. The corpus includes descriptions of the Canal District before the industrial revolution altered it dramatically, the building of its canal and railroad, the peak of its industrialization and identity as an Irish working class neighborhood,  its post-industrial decline, and its stages of revitalization. While a majority of texts come from the local major newspaper The Worcester Telegram Gazette, the corpus includes articles from New York Times, Boston Globe, and National Public Radio; also included are  Worcester-born poet Mary Fell\u2019s 1984 poem The Prophecy and historian Roy Rosenzweig\u2019s acclaimed history of Worcester\u2019s working class Eight Hours for What We Will (1985). A description of each one is given below.\nDataset details\nBelow we give some details of each text (sorted by date).\nFilename: 1 - 1862_Lincoln, from History of Worcester (archive.org) landscape description\nSource: William Lincoln, from History of Worcester Lincoln, William. History of Worcester, Massachusetts, from its earliest settlement to September, 1836 : with various notices relating to the history of Worcester County. Worcester: M.D. Phillips & Co., 1862.\nURL: https:\/\/archive.org\/details\/historyofworcest1836linc\/page\/n6\nPublication Date: 1862\nDescription: Worcester\u2019s landscape before the industrial revolution.\nTokens: 256\n\nFilename: 2 - 1879_Abijah Perkins Marvin, History of Worcester (archive.org), railroad descriptionn\nSource: Abijah Perikins Marvin, History of Worcester County, Massachusetts, embracing a comprehensive history of the country from its first settlement to the present time.  Boston; Jewett & Co, 1879, pp. 82-83.\nURL: https:\/\/archive.org\/details\/historyofworcest03marv\/page\/82\nPublication Date: 1879\nDescription: A description of the beginning of the Blackstone canal and the Worcester railroad.\nTokens: 638\n\nFilename: 3 - 1917_Washburn, Industrial History of Worcester (archive.org) re canal\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 23.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/n6\nPublication Date: 1917\nDescription: Description of the creation of the Blackstone canal and its importance\nTokens: 199\n\nFilename: 4 - 1917_Washburn, Industrial History of Worcester (archive.org) re entrepreneurial spirit of city\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 31.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/60\nPublication Date: 1917\nDescription: The introduction of steam-power to Worcester industries.\nTokens: 72\n\nFilename: 5 - 1917_Washburn, Industrial History, re importance of steam power\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 300.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/60\nPublication Date: 1917\nDescription: Entrepreneurial spirit of city.\nTokens: 150\n\nFilename: 6 - 1983-25-9_Worcester Shedding Smokestack Image (New York Times)\nSource: \u201cWorcester Shedding Smokestack Image,\u201d New York Times. Sept. 25, 1983.\nPublication Date: 1983-25-9\nDescription:\nTokens: 911\n\nFilename: 7 - 1984_Mary Fell, The Prophecy (Poem)\nSource: Mary Fell, \u201cThe Prophecy\u201d, from the Persistence of Memory. 1984.\nURL: http:\/\/capa.conncoll.edu\/fell.persistence.html\nPublication Date: 1984\nDescription: Poem collection about Worcester.\nTokens: 328\n\nFilename: 8 - 1985_Roy Rosenzweig, Eight Hours For What We Will (history of recreation for industrial labor)\nSource: Roy Rosenzweig, Eight Hours For What We Will. Cambridge, UK: Cambridge, 1985.\nPublication Date: 1985\nDescription: A history of recreation for Worcester\u2019s industrial labor force.\nTokens: 287\n\nFilename: 9 - 1989-10-12_A New Look for an Old Area (New York Times) (Lafayette Place\/Green Island)\nSource: \u201cWorcester, Mass.: A New Focus for an Old Area,\u201d New York Times.\nPublication Date: 1989-10-12\nDescription: The building of new senior affordable housing in Green Island.\nTokens: 450\n\nFilename: 10 - 1997-11-18_Bureau urges liability relief for brownfields, WTG\nSource: Bronislaus B. Kush, \u201cBureau urges liability relief for brownfields,\u201d Worcester Telegram Gazette.\nPublication Date: 1997-11-18\nDescription:\nTokens: 585\n\nFilename: 11 - 1998-16-7_Green Island Businesses Say City Help Is Killing Them, WTG\nSource: Bronislaus B. Kush, \u201cGreen Island Businesses Say City Help Is Killing Them,\u201d Worcester Telegram Gazette.\nPublication Date: 1998-16-7\nDescription:\nTokens: 597\n\nFilename: 12 - 1999-12-1_Vacant Industrial Sites of No Use to Neighborhood, WTG\nSource: Winston W. Wiley, \u201cVacant Industrial Sites of No Use to Neighborhood,\u201d Worcester Telegram Gazette.\nPublication Date: 1999-12-01\nDescription:\nTokens: 436\n\nFilename: 13 - 2000-29-6_Green Island Revitalization Plan Dropped, WTG\nSource: Lisa Eckelbecker, \u201cGreen Island Revitalization Plan Dropped,\u201d Worcester Telegram Gazette.\nPublication Date: 2000-29-6\nDescription:\nTokens: 446\n\nFilename: 14 - 2007-25-9_Canal District Shapes Up: Old Buildings, New Life on Green Street, WTG\nSource: Shaun Sutner, \u201cCanal District Shapes Up: Old Buildings, New Life on Green Street,\u201d Worcester Telegram Gazette.\nPublication Date: 2007-25-09\nDescription:\nTokens: 645\n\nFilename: 15 - 2011-23-8_Life in Green Island: We have Hope. In City Times (local alternative newspaper)\nSource: Maureen Schwab, \u201cLife in Green Island: We have Hope,\u201d In City Times (local alternative newspaper).\nPublication Date: 2011-23-08\nDescription:\nTokens: 1149\n\nFilename: 16 - 2018-8-17_WooSox Ball Park Has Long History. Boston Globe\nSource: Tim Logan, \u201cNew Home for WooSox Has Long History,\u201d Boston Globe.\nPublication Date: 2018-8-17\nDescription:\nTokens: 632\n\nFilename: 17 - 2018-10-11_Time to Talk About Gentrification in Worcester, Worcester Mag.\nSource: Bill Shaner, \u201cIt\u2019s Time to Talk About Gentrification in Worcester,\u201d Worcester Magazine.\nPublication Date: 2018-10-11\nDescription:\nTokens: 4165\n\nFilename: 18 - 2018-10-23_Worcester: The New  \u201cIt Town.\u201d National Public Radio\nSource: Aaron Schachter, \u201cWorcester: The New\u2018It\u2019 Town.\u201d National Public Radio: WGBH.\nPublication Date :2018-10-23\nDescription:\nTokens: 1184\n\nFilename: 19 - 2018_Worcester, A City Reclaimed.  Vitality Magazine\nSource: Bernard Whitmore, \u201cA Mayor, A Manager, A City Reclaimed.\u201d  Vitality Magazine.\nPublication Date: 2018-11\nDescription:\nTokens: 2077\n\nFilename: 20 - 2019-2-27_Worcester Organizers Hear from Nashville, Buffalo on Tips for WooSox CBA Push, Worcester Mag.\nSource: Bill Shaner, Worcester Organizers Hear from Nashville, Buffalo on Tips for WooSox CBA Push,\u201d Worcester Magazine.\nPublication Date: 2019-02-27\nDescription:\nTokens: 540\n\nFilename: 21_2019-4-10_A Totally Cool Place to Live. MassLive?\nSource: Aviva Luttrell, \u201cA Totally Cool Place to Live,\u201d MassLive.com.\nPublication Date: 2019-04-10\nDescription:\nTokens: 497\n\nFilename: 22 - 2019-6-1_New Shine for Old Building: Former Walker Shoe Factory to be Converted into Studios. WTG\nSource: Scott O\u2019Connell, \u201cNew Shine for Old Building: Former Walker Shoe Factory to be Converted into Studios,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-01\nDescription:\nTokens: 468\n\nFilename: 23 - 2019-6-20_Construction, WooSox, and Regulation are Killing Canal District Dreams. Worcester Business Journal\nSource: Renee Diaz, \u201cConstruction, WooSox, and Regulation are Killing Canal District Dreams,\u201d Worcester Business Journal.\nPublication Date: 2019-06-20\nDescription:\nTokens: 739\n\nFilename: 24 - 2019-6-24_Worcester Gets Brownfield Funds. WTG\nSource: \u201cWorcester Gets Brownfield Funds,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-24\nDescription:\nTokens: 517\n\nFilename: 25 - 2019-6-24_Worcester Pledges $3M to Green Island. WTG\nSource: Kim Ring, \u201cWorcester Pledges $3M to Green Island,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-24\nDescription:\nTokens: 490\n\nFilename: 26 - 2019-7-6_An Away Game for Businesses; Property Owners Near Ballpark Site Make Way for Development. WTG\nSource: \u201cAn Away Game for Businesses; Property Owners Near Ballpark Site Make Way for Development,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-07-06\nDescription:\nTokens: 1322\n\n","96":"Greening a PostIndustrial City:  Applying keyword extractor methods to monitor a fast-changing environmental narrative\nWorcester, Massachusetts (population just under 200,000) is the second largest city after Boston in Massachusetts, USA.  It was important in the American industrial revolution; factories dominated its landscape until the mid-20th century when the city slid physically, and mentally, into post-industrial decline.\nAt first glance, Worcester might present an all-too-familiar global story of gentrification, the loss of city identity and community, but does even a small survey of the historic record of local public discourse confirm this view? Or does a more complicated local story surface?\nTo answer these questions,  literary geographer Sarah Luria teams up with computer scientist Ricardo Campos, developer of the keyword extractor YAKE! to discover if it can create a helpful survey or image of stories told about a neighborhood over time. We believe this interdisciplinary work can play a crucial role by showing how Artificial Intelligence (AI) can track this fast-developing story of urban revitalization and environmental cleanup.\nFrom our survey we curated a small corpus of 26 English-language texts that described the Canal District\/Green Island over time. A majority of the texts are  from 2018-2019, but examples from 1862, 1917, the 1980s and 1990s provide some historic range. The corpus includes descriptions of the Canal District before the industrial revolution altered it dramatically, the building of its canal and railroad, the peak of its industrialization and identity as an Irish working class neighborhood,  its post-industrial decline, and its stages of revitalization. While a majority of texts come from the local major newspaper The Worcester Telegram Gazette, the corpus includes articles from New York Times, Boston Globe, and National Public Radio; also included are  Worcester-born poet Mary Fell\u2019s 1984 poem The Prophecy and historian Roy Rosenzweig\u2019s acclaimed history of Worcester\u2019s working class Eight Hours for What We Will (1985). A description of each one is given below.\nDataset details\nBelow we give some details of each text (sorted by date).\nFilename: 1 - 1862_Lincoln, from History of Worcester (archive.org) landscape description\nSource: William Lincoln, from History of Worcester Lincoln, William. History of Worcester, Massachusetts, from its earliest settlement to September, 1836 : with various notices relating to the history of Worcester County. Worcester: M.D. Phillips & Co., 1862.\nURL: https:\/\/archive.org\/details\/historyofworcest1836linc\/page\/n6\nPublication Date: 1862\nDescription: Worcester\u2019s landscape before the industrial revolution.\nTokens: 256\n\nFilename: 2 - 1879_Abijah Perkins Marvin, History of Worcester (archive.org), railroad descriptionn\nSource: Abijah Perikins Marvin, History of Worcester County, Massachusetts, embracing a comprehensive history of the country from its first settlement to the present time.  Boston; Jewett & Co, 1879, pp. 82-83.\nURL: https:\/\/archive.org\/details\/historyofworcest03marv\/page\/82\nPublication Date: 1879\nDescription: A description of the beginning of the Blackstone canal and the Worcester railroad.\nTokens: 638\n\nFilename: 3 - 1917_Washburn, Industrial History of Worcester (archive.org) re canal\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 23.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/n6\nPublication Date: 1917\nDescription: Description of the creation of the Blackstone canal and its importance\nTokens: 199\n\nFilename: 4 - 1917_Washburn, Industrial History of Worcester (archive.org) re entrepreneurial spirit of city\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 31.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/60\nPublication Date: 1917\nDescription: The introduction of steam-power to Worcester industries.\nTokens: 72\n\nFilename: 5 - 1917_Washburn, Industrial History, re importance of steam power\nSource: Charles G. Washburn, Industrial Worcester. Worcester: The Davis Press, 1917, p. 300.\nURL: https:\/\/archive.org\/details\/industrialworces00wash\/page\/60\nPublication Date: 1917\nDescription: Entrepreneurial spirit of city.\nTokens: 150\n\nFilename: 6 - 1983-25-9_Worcester Shedding Smokestack Image (New York Times)\nSource: \u201cWorcester Shedding Smokestack Image,\u201d New York Times. Sept. 25, 1983.\nPublication Date: 1983-25-9\nDescription:\nTokens: 911\n\nFilename: 7 - 1984_Mary Fell, The Prophecy (Poem)\nSource: Mary Fell, \u201cThe Prophecy\u201d, from the Persistence of Memory. 1984.\nURL: http:\/\/capa.conncoll.edu\/fell.persistence.html\nPublication Date: 1984\nDescription: Poem collection about Worcester.\nTokens: 328\n\nFilename: 8 - 1985_Roy Rosenzweig, Eight Hours For What We Will (history of recreation for industrial labor)\nSource: Roy Rosenzweig, Eight Hours For What We Will. Cambridge, UK: Cambridge, 1985.\nPublication Date: 1985\nDescription: A history of recreation for Worcester\u2019s industrial labor force.\nTokens: 287\n\nFilename: 9 - 1989-10-12_A New Look for an Old Area (New York Times) (Lafayette Place\/Green Island)\nSource: \u201cWorcester, Mass.: A New Focus for an Old Area,\u201d New York Times.\nPublication Date: 1989-10-12\nDescription: The building of new senior affordable housing in Green Island.\nTokens: 450\n\nFilename: 10 - 1997-11-18_Bureau urges liability relief for brownfields, WTG\nSource: Bronislaus B. Kush, \u201cBureau urges liability relief for brownfields,\u201d Worcester Telegram Gazette.\nPublication Date: 1997-11-18\nDescription:\nTokens: 585\n\nFilename: 11 - 1998-16-7_Green Island Businesses Say City Help Is Killing Them, WTG\nSource: Bronislaus B. Kush, \u201cGreen Island Businesses Say City Help Is Killing Them,\u201d Worcester Telegram Gazette.\nPublication Date: 1998-16-7\nDescription:\nTokens: 597\n\nFilename: 12 - 1999-12-1_Vacant Industrial Sites of No Use to Neighborhood, WTG\nSource: Winston W. Wiley, \u201cVacant Industrial Sites of No Use to Neighborhood,\u201d Worcester Telegram Gazette.\nPublication Date: 1999-12-01\nDescription:\nTokens: 436\n\nFilename: 13 - 2000-29-6_Green Island Revitalization Plan Dropped, WTG\nSource: Lisa Eckelbecker, \u201cGreen Island Revitalization Plan Dropped,\u201d Worcester Telegram Gazette.\nPublication Date: 2000-29-6\nDescription:\nTokens: 446\n\nFilename: 14 - 2007-25-9_Canal District Shapes Up: Old Buildings, New Life on Green Street, WTG\nSource: Shaun Sutner, \u201cCanal District Shapes Up: Old Buildings, New Life on Green Street,\u201d Worcester Telegram Gazette.\nPublication Date: 2007-25-09\nDescription:\nTokens: 645\n\nFilename: 15 - 2011-23-8_Life in Green Island: We have Hope. In City Times (local alternative newspaper)\nSource: Maureen Schwab, \u201cLife in Green Island: We have Hope,\u201d In City Times (local alternative newspaper).\nPublication Date: 2011-23-08\nDescription:\nTokens: 1149\n\nFilename: 16 - 2018-8-17_WooSox Ball Park Has Long History. Boston Globe\nSource: Tim Logan, \u201cNew Home for WooSox Has Long History,\u201d Boston Globe.\nPublication Date: 2018-8-17\nDescription:\nTokens: 632\n\nFilename: 17 - 2018-10-11_Time to Talk About Gentrification in Worcester, Worcester Mag.\nSource: Bill Shaner, \u201cIt\u2019s Time to Talk About Gentrification in Worcester,\u201d Worcester Magazine.\nPublication Date: 2018-10-11\nDescription:\nTokens: 4165\n\nFilename: 18 - 2018-10-23_Worcester: The New  \u201cIt Town.\u201d National Public Radio\nSource: Aaron Schachter, \u201cWorcester: The New\u2018It\u2019 Town.\u201d National Public Radio: WGBH.\nPublication Date :2018-10-23\nDescription:\nTokens: 1184\n\nFilename: 19 - 2018_Worcester, A City Reclaimed.  Vitality Magazine\nSource: Bernard Whitmore, \u201cA Mayor, A Manager, A City Reclaimed.\u201d  Vitality Magazine.\nPublication Date: 2018-11\nDescription:\nTokens: 2077\n\nFilename: 20 - 2019-2-27_Worcester Organizers Hear from Nashville, Buffalo on Tips for WooSox CBA Push, Worcester Mag.\nSource: Bill Shaner, Worcester Organizers Hear from Nashville, Buffalo on Tips for WooSox CBA Push,\u201d Worcester Magazine.\nPublication Date: 2019-02-27\nDescription:\nTokens: 540\n\nFilename: 21_2019-4-10_A Totally Cool Place to Live. MassLive?\nSource: Aviva Luttrell, \u201cA Totally Cool Place to Live,\u201d MassLive.com.\nPublication Date: 2019-04-10\nDescription:\nTokens: 497\n\nFilename: 22 - 2019-6-1_New Shine for Old Building: Former Walker Shoe Factory to be Converted into Studios. WTG\nSource: Scott O\u2019Connell, \u201cNew Shine for Old Building: Former Walker Shoe Factory to be Converted into Studios,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-01\nDescription:\nTokens: 468\n\nFilename: 23 - 2019-6-20_Construction, WooSox, and Regulation are Killing Canal District Dreams. Worcester Business Journal\nSource: Renee Diaz, \u201cConstruction, WooSox, and Regulation are Killing Canal District Dreams,\u201d Worcester Business Journal.\nPublication Date: 2019-06-20\nDescription:\nTokens: 739\n\nFilename: 24 - 2019-6-24_Worcester Gets Brownfield Funds. WTG\nSource: \u201cWorcester Gets Brownfield Funds,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-24\nDescription:\nTokens: 517\n\nFilename: 25 - 2019-6-24_Worcester Pledges $3M to Green Island. WTG\nSource: Kim Ring, \u201cWorcester Pledges $3M to Green Island,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-06-24\nDescription:\nTokens: 490\n\nFilename: 26 - 2019-7-6_An Away Game for Businesses; Property Owners Near Ballpark Site Make Way for Development. WTG\nSource: \u201cAn Away Game for Businesses; Property Owners Near Ballpark Site Make Way for Development,\u201d Worcester Telegram Gazette.\nPublication Date: 2019-07-06\nDescription:\nTokens: 1322\n\n","97":"Stochastic\/Environmental Signal Processing: ECEN 5224\n","98":"EnvironmentalGameMS\n","99":"EnvironmentalGameMS\n","100":"Environmental Flows Project - University of California, Davis\nThis repository is a meta-repository keeping track of the various repositories used in the Environmental Flows project from UC Davis Water Management Lab.\n\nProject website\nFunctional Flows Calculator\nTerrain analysis\nGeomorphic classification\nResources\n\nFunctional Flows Calculator\nContributor(s)\n\nBelize Lane\nNoelle Patterson\nLeo Qiu\n\nRepositories\n\nDimensionless Reference Hydrograph (DRH)\nFunctional Flow Calculator\n\nTerrain analysis\nContributor(s)\n\nHerv\u00e9 Guillon\n\nRepositories\n\nModified version of the R raster.terrain function\nCART- and WARD- related scripts\nSlurm-related scripts\n\nGeomorphic classification\nContributor(s)\n\nColin Byrne\n\nRepositories\n\n\n\nProject website\nContributor(s)\n\nLeo Qiu\n\nRepositories\n\nUC Davis Water Management Lab eflow API\nUC Davis Water Management Lab eflow client\n\nResources\n\nUCD eFlow Google Drive Internal\nUCD eFlow Google Drive External\nUCD eFlow Box Drive - Big Data Files\nUCD Water Management Lab meta-repository\n\n","101":"Environmental-Detection-System\nThis is my unversity graduated project which is related with IOT.\nThe goal of this project is for detecting environment.\n1.Setting web server and mysql server in Respberry Pi\n2.Using humidity sensor and Pi camera to detect\/watch environment\n3.Showing the result on website\n","102":"EnvironmentalChamberCode\nWalker Wind's code for humidity sensors, flask website setup, and html templates\n6\/13\/19\nChris and Ethan's Lab\nFiles in the repository:\nhumidsensors.py ::\nWhen run, this program records humidity and temperature sensor data using the Adafruit DHT library, and outputs a two text files and a     jpg file. The two text files, titled humiddataA and humiddataB, have a text chart that displays the exact numbers of each data point.\nThe jpg file is a chart with four subplots displaying the data using the python library matplotlib\nTo run, use the command line input:\n$python humidsensors.py\n\nhumidwebsite.py ::\nThis code is used to set up the flask web server. Using the flask library, it implements several pages connected to the local webserver     that the Raspberry Pi puts out on the network it is connected to. The website has a couple different pages\n$sudo FLASK_APP=humidwebsite.py flask --host=0.0.0.0 --port=80\n\nindexpage.html ::\nThis is html code used to design a template for the index page\nchartdata.html ::\nThis is html code used to display an image\ntextchart.html ::\nThis is html code used to display a text file\n","103":"EnvironmentalChamberCode\nWalker Wind's code for humidity sensors, flask website setup, and html templates\n6\/13\/19\nChris and Ethan's Lab\nFiles in the repository:\nhumidsensors.py ::\nWhen run, this program records humidity and temperature sensor data using the Adafruit DHT library, and outputs a two text files and a     jpg file. The two text files, titled humiddataA and humiddataB, have a text chart that displays the exact numbers of each data point.\nThe jpg file is a chart with four subplots displaying the data using the python library matplotlib\nTo run, use the command line input:\n$python humidsensors.py\n\nhumidwebsite.py ::\nThis code is used to set up the flask web server. Using the flask library, it implements several pages connected to the local webserver     that the Raspberry Pi puts out on the network it is connected to. The website has a couple different pages\n$sudo FLASK_APP=humidwebsite.py flask --host=0.0.0.0 --port=80\n\nindexpage.html ::\nThis is html code used to design a template for the index page\nchartdata.html ::\nThis is html code used to display an image\ntextchart.html ::\nThis is html code used to display a text file\n","104":"EnvironmentalPollution\nINFO 5100 - Project 3\nThis project focuses on some typical and dangerous environment pollutants and its connection to health. We designed a map, a linear regression graph and a bubble chart to presents pollutants\u2019 distribution, its relation to longevity and disease.\n","105":"Corporate environmental performance prediction in China: An empirical study of energy service companies\nAuthors: Saina Zheng Chenhang He, Shu-Chien Hsu, Joseph Sarkis and Jieh-Haur Chen\nThis code was originally used in the paper \"Corporate environmental performance prediction in China: An empirical study of energy service companies\", it contains three fundalmental\nmachine learning regression model Random Forest, SVM and XGBoost. The code is well commented and can be easy to used by anyone with beginner level of python programming skills.\nThe evaluation metrics and some visualization code are also included.\nDependencies\n\npython (tested on 3.5)\nscikit-learn\nXGBoost\npandas\n\nCitation\nIf you find this work useful in your research, please consider cite:\n@article{zheng2020CEPPC,\ntitle={Corporate environmental performance prediction in China: An empirical study of energy service companies },\nauthor={Saina Zheng, Chenhang He, Shu-Chien Hsu, Joseph Sarkis, and Jieh-Haur Chen},\njournal={Journal of Cleaner Production (JCLP)},\nyear={2020}\n}\n\n","106":"software-engineering\nProgetto\n","107":"Palladio-Addons-EnvironmentalDynamics\nEnvironmental dynamics comprises modeling and analysis capabilities to enhance the PCM with probabilistic specifications of the execution environment.\n","108":"OGC API - EDR Sprint 2\n\nThis Github repository is for the second OGC API - EDR code sprint focusing on the OGC API - Environmental Data Retrieval candidate standard.\n#OGCAPI\nAbout the Code Sprint\nThe Open Geospatial Consortium (OGC) invites developers to the OGC API - EDR Sprint 2 virtual event to be held through remote participation\/web-conferencing on November 9-10, 2020, from 9:00am 5:30pm EST. Registration for the OGC API - EDR Sprint 2 virtual event and the associated pre-event Webinar is here.\nThe code sprint will focus on refining the OGC API - Environmental Data Retrieval candidate standard. The candidate standard uses current web technologies and best practices to enable end-users - or anyone with web development experience - to easily identify and retrieve a subset of data from \u2018big data\u2019 stores. The idea is to save those users interested in environmental (or other) data from having to transfer and deal with datasets that inevitably contain data concerning areas or time periods that are irrelevant to their interests.\nRegister at https:\/\/portal.ogc.org\/public_ogc\/register\/202011q4_api_edr.php\n\nSprint Description\nAPI Specs\nSprint Logistics\nSchedule\/Agenda\nImplementations\nDatasets\nWhat is everybody going to be working on?\nLessons and Next Steps\nAdditional Resources\nFrequently Asked Questions (FAQs)\n\nThe sprint will begin at 09:00am EST on the first day, and end at 05:00pm EST on the second day.\n","109":"Environmental Science Associates  \nJekyll theme based on Freelancer bootstrap theme \n","110":"Environmental Science Associates  \nJekyll theme based on Freelancer bootstrap theme \n","111":"\n\nSEAStAR - A framework for the analysis of next-generation metagenomes\nThe Basics\nSEAStAR is a package of tools supporting the construction of complete analysis pipelines for next-generation (Illumina\u00ae, SOLiD\u2122) sequencing data generated from environmental samples.\nIt includes high-performance tools for dealing with:\n\nConverting between file formats (CSFASTA -> FASTQ)\nTrimming raw reads for quality (with tuning support)\nPCR de-duplication of paired reads (without reference sequences)\nSelecting and estimating the relative abundance of sequences from large reference databases (e.g. 16S rDNA)\nSub-sampling paired FASTQ files randomly, or based on reads included in (or excluded from) reference alignments\nConverting assembled color-space (SOLiD) contigs to nucleotide-space\nConnecting assembled contigs together via paired reads (constructing an assembly graph)\nSplitting complicated metagenomic assembly graphs into well-supported scaffolds\nBinning scaffolds by organism using tetra-nucleotide statistics\nIdentifying small circular scaffolds that are likely virus or plasmid genomes\n\nSEAStAR works with, but does not supply:\n\nShort-read sequence aligners (e.g. BWA, Bowtie)\nDe novo contig assemblers (e.g. Velvet)\nTools for visualizing assembly graphs (e.g. GraphViz, ZGRViewer)\n16S Taxonomic classifiers (e.g. RDP Classifier)\n\nYou can find out more about SEAStAR on its Armbrust Lab Homepage.\nThis file contains information on how to build and install the SEAStAR tools. For information on using the tools themselves, please see the included SEAStAR User Guide file.\nLicense\nSEAStAR is released under the GPLv3 license, a copy of which is provided in the included file \"COPYING\". By using this software, you are agreeing to be bound by the terms of this license.\nInstallation\nThe instructions that follow are for building the SEAStAR tools from source code.\nSEAStAR is designed to build and run on any 64-bit Unix-like system, including Linux and Mac OS X version 10.7 or later. Many components of SEAStAR are optimized for multiple CPU cores and require substantial memory. We recommend a machine with a minimum of 4 CPU cores and 32 GB of RAM to run these tools.  Depending on your datasets and what you are trying to do (e.g. de novo assembly) you may require a substantially more powerful machine than this minimum recommendation.\nThe SEAStAR package has dependencies on a small number of software packages that you (or your system administrator) may need to install or update. The process described in the next section will notify you if your system is missing any of these components.\nRequired Tools:\n\ngcc -- version 4.2 or newer, supporting OpenMP (version 4.7 recommended)\ncmake -- version 2.8.5 or newer\nnode.js -- version 0.10 or newer\ngawk -- version 3.1.5 or newer (version 4.0.2 recommended)\n\nAdditional instructions are available below for fulfilling these requirements for Mac OS X, and for programmers wishing to make modifications to the included source code.\nOnce you have the above packages: To build SEAStAR using Unix style command line tools, run the following commands from the directory where all files generated in the build process should be placed (including executables). This is your \"destination tree\".\ncmake [dir]\nmake\n\nWhere [dir] is the path to the root of the SEASTAR source tree (where this README file is found).\nIf the path \".\" is used for [dir] above (run from the \"source tree\"), then the binary and source tree will be the same (an \"in-source build\"). After a successful make, executables will be found in the bin\/ subdirectory.\nThis directory (the bin subdirectory of the destination tree) should be added to your PATH environment variable, so that newly built tools can be found from your data analysis directories:\nexport PATH=$PATH:[dest_dir]\/bin   # Where [dest_dir] is the fully qualified path to your destination tree directory\n\nTo test the newly built components:\nmake test\n\nIf any tests fail, do not use the executables!\nTo clean all files generated in the source directory for an in-source build (this will only work for git checked-out repositories):\ngit clean -fxd\n\nFor an out-of-source build you can simply delete the destination tree directory and start again.\nAdditional installation details for Mac OS X\nFor Mac OS X users: To fulfill the above requirements, you will first need to download and install Apple's \"Command Line Developer Tools\".\nxcode-select --install\n\nAnd then we recommend installing the other required packages using HomeBrew or MacPorts.\nHomeBrew (preferred)\nVisit the link below to download and install HomeBrew.\n\nhttps:\/\/brew.sh\/\n\nThen run the following commands to install the required packages:\nbrew update\nbrew install cmake\nbrew install node   # Node may also optionally be installed using nvm\nbrew install gawk\nbrew install gcc@8\n\nMacPorts\nVisit the link below to download and install MacPorts.\n\nhttp:\/\/www.macports.org\/install.php\n\nThen run the following commands to install the required packages:\nsudo port selfupdate\nsudo port install cmake\nsudo port install node  # Node may also optionally be installed using nvm\nsudo port install gawk\nsudo port install gcc8  # or whatever version you may prefer\n\nAn important note about compilers on Mac OS X :\nXcode's default Clang-based compiler does not support OpenMP (a standard for writing efficiently parallelized C code); this is why we specify above that you must install the gcc compiler. The cmake script provided checks OS X systems to see if the OpenMP support is working correctly with the default (or specified) C compiler. If you receive an error when trying to build that says \"You need to install gcc (version 4.4 or later)\" it is because our build system is attempting to use the Xcode compiler, and not the one you installed using HomeBrew or MacPorts.\nYou will need to define an environment variable to explicitly tell cmake which compiler to use. Note that this must be done each time you start a command line session where you wish to run cmake again (or add it to your shell startup file, e.g. .bash_profile in your home directory).  For example:\nexport CC=\/usr\/local\/bin\/gcc-8  # Homebrew\n\nor\nexport CC=\/opt\/local\/bin\/gcc-8  # MacPorts\n\nChange the numbers above if you are using a different version!\nNote: It may also be possible, with some more work, to use a newer version of clang than provided by Apple to compile with OpenMP support, but we mave not tested this. Both HomeBrew and MacPorts enable installation of LLVM 7 (which includes the clang C compiler). Have fun with that!\nFor Developers\nSome of the included JavaScript (.js) files are automatically generated from CoffeeScript source files (CoffeeScript is a transcompiled dialect of JavaScript with Python-like syntax.) If you wish to modify these components, please edit the .coffee files in the scripts\/ subdirectory of the source tree. The make system will automatically regenerate the .js files in the bin\/ subdirectory of the destination tree. To successfully transcompile these files, you will need the to install the CoffeeScript package for node.js:\nsudo npm install -g coffee-script\n\nIt is sometimes useful to build with GCC debug flags turned on.  To achieve this follow the normal cmake build procedure with one additional user defined cmake cache entry:\ncmake -D DEBUG=ON [dir]\n\n","112":"OPERA\nOPERA is a free and open-source\/open-data suite of QSAR models providing predictions on physicochemical properties, environmental fate and toxcicity endpoints as well as additional information including applicability domain and accuracy assessment. All models were built on curated data and standardized QSAR-ready chemical structures. OPERA is available in command line and user-friendly graphical interface for Windows and Linux operating systems. It can be installed as a standalone desktop application or embedded in a different tool\/workflow.\nReferences:\n[1] Mansouri K. et al. J Cheminform (2018) https:\/\/doi.org\/10.1186\/s13321-018-0263-1.\n[2] Mansouri, K. et al. SAR and QSAR in Env. Res. (2016). https:\/\/doi.org\/10.1080\/1062936X.2016.1253611\n[3] Williams A. J. et al. J Cheminform (2017) https:\/\/doi.org\/10.1186\/s13321-017-0247-6\n[4] The CompTox Chemistry Dashboard https:\/\/comptox.epa.gov\/dashboard\n[5] JRC QSAR Model Database https:\/\/qsardb.jrc.ec.europa.eu\/qmrf\/endpoint\n[6] Mansouri, K. et al. EHP (2016) https:\/\/doi.org\/10.1289\/ehp.1510267\n[7] Mansouri, K. et al. J Cheminform (2019) https:\/\/doi.org\/10.1186\/s13321-019-0384-1\n[8] Mansouri, K. et al. EHP (2020) https:\/\/doi.org\/10.1289\/EHP5580\nModels:\n      + Molecular descriptors:  \n- PaDEL (2.21) (https:\/\/doi.org\/10.1002\/jcc.21707 )\n- CDK (2.0) (https:\/\/doi.org\/10.1186\/s13321-017-0220-4)\n\n\n      + New models (since v2.0):\n\n\n- FuB: Plasma fraction unbound (human)\n\n- Clint: hepatic intrinsic clearance (human)\n\n- pKa: acid dissociation constant\n\n- LogD: Octanol-water distribution constant. LogD is equivalent to logP for non-ionisable compounds.\n\n- CERAPP: Collaborative Estrogen Receptor Activity Prediction Project. Binding, Agonist and Antagonist Estrogen Receptor activity (https:\/\/ehp.niehs.nih.gov\/15-10267\/)\n\n- CoMPARA: Collaborative Modeling Project for Androgen Receptor Activity. Binding, Agonist and Antagonist Androgen Receptor activity (https:\/\/doi.org\/10.13140\/RG.2.2.19612.80009, https:\/\/doi.org\/10.13140\/RG.2.2.21850.03520)\n\n- CATMoS: Collaborative Acute Toxicity Modeling Suite. Very-Toxic, Non-Toxic, EPA categories, GHS categories, LD50 (Log mg\/kg) (https:\/\/doi.org\/10.1016\/j.comtox.2018.08.002)\n\n- Structural Properties: MolWeight, nbAtoms, nbHeavyAtoms, nbC, nbO, nbN, nbAromAtom, nbRing, nbHeteroRing, Sp3Sp2HybRatio, nbRotBd, nbHBdAcc, ndHBdDon, nbLipinskiFailures, TopoPolSurfAir, MolarRefract, CombDipolPolarizability.\n\n\n      + Previous models (since v1.5):\n\n\n- OH (LogOH) in cm3\/molecule-sec: The OH rate constant for the atmospheric, gas-phase reaction between photochemically produced hydroxyl radicals and organic chemicals.\n\n- BCF (Log): Fish bioconcentration factor\n\n- Biodeg (LogHalfLife) in days: biodegradation half-life for compounds containing only carbon and hydrogen (i.e. hydrocarbons). \n\n- Ready_biodeg (classification: 0\/1): Ready biodegradability of organic chemicals. \n\n- BP in deg C: Boiling Point at 760 mm Hg\n\n- HL (LogHL) in atm-m3\/mole: The Henry\u2019s Law constant (air\/water partition coefficient) at 25C\n\n- Km (Log KmHL) half-lives in days: The whole body primary biotransformation rate constant for organic chemicals in fish. \n\n- KOA (Log): The octanol\/air partition coefficient.\n\n- LogP (Log): Octanol-water partition coefficient, log KOW, of chemicals.\n\n- MP in deg C: Melting Point\n\n- Koc (Log) in L\/Kg: the soil adsorption coefficient of organic compounds. \u00a0The ratio of the amount of chemical adsorbed per unit weight of organic carbon in the soil or sediment to the concentration of the chemical in solution at equilibrium.\n\n- VP (Log) in mmHg: Vapor Pressure experimental values between 15 and 30 deg C (majority at 25-20C)\n\n- WS (Log) in Molar moles\/L: Water solubility at 25C. \n\n- RT in minutes: HPLC retention time.\n\n","113":"Mining Sensor Data to Evaluate Indoor Environmental Quality of Public Educational Buildings\nIn this project, we will collect, extract, transform, load, and analyse sensor data transmitted\nfrom large amount of sensors installed in school buildings across three European countries.\nThe sensor data report multiple types of information including temperature, humidity, outdoor weather,\nelectronic consumption, human activities, and etc.\nUsing python library such as Pandas, Matplotlib, Numpy and Comfort Tools from Berkeley ,\nwe aim to predict the overall comfort and other KPI for indoor environmental quality in the school buildings\nBased on our findings, school management can optimise the environment quality.\nBasic information\nWhy do we care about collecting data in schools?\nFrom six until we are ready to step out to get to work, most of our time spent at school,\nsometimes even after school we still stay there for sports activities, or stay in the library for studying.\nWhat should be an ideally school?\nEnough illumination, comfortable temperature not too hot not too cold,\nenough fresh air to cool you head down before you crashed down by study and\/or stress?\nThe governments spend a lot of money on education,\nbut is that enough money or is there some way to put the money on the better place instead of just paying the electricity bills?\nIf we really want to do something for making a better place to study,\nwhere should we start to do or look at?\nWe need some first-hand data to tell us which part is the most expensive one so we can start to work on it.\nIn this situation, sensor data is good cutting point for having an observation and do the analysis.\nPoints of sensors geography distribution\n\nLocations in Google Map\n\n\n\n\n\nCountry\nParameter\nNumber\nComment\n\n\n\n\nGreece\nSensing endpoints\n872\nEach sensor equals 1 sensing endpoint\n\n\n\nSensing rate\n1 minute\nCan be modified\n\n\n\nEducators\n294\nGreek public schools in GAIA\n\n\n\nStudents\n2267\nGreek public schools in GAIA\n\n\nItaly (Roma)\nSensing endpoints\n118\nWill soon be augmented\n\n\n\nSensing rate\n1 minute\nCan be modified\n\n\n\nEducators\n120\nUniversity faculty and Post Doc\n\n\n\nStudents\n1706\nUniversity students\n\n\nItaly(Prato)\nSensing endpoints\n117\n\n\n\nSweden(Soderhamn)\nSensing endpoints\n3\ncreate on 2017-07-12T12 more to add, no data yet\n\n\n\nTotal : 16 sites and 1922 sensors are on the record till 2017\/09\/16.\nPart of them are newly installed in this year and some have history data from 2015.\nThe data is collected under different weather condition, from different cultures , with different user behaviour pattens\nIt is a good start we could see the difference and find the similarity.\nSensor data \/ unit\n\n\nPower consumption\n\n\nCalculated Power Consumption : mWh\n\n\nPower Consumption : mWh\n\n\nElectrical Current : mA\/A\n\n\nActive Power : mW\n\n\nApparent Energy : Vah\n\n\nApparent Power : VA\n\n\nVoltage : V\n\n\nPower Factor : Raw Value\n\n\nReactive Energy : VARh\n\n\nReactive Power : VAR\n\n\n\n\nEnvironmental parameters\n\n\nNoise : Raw Value\n\n\nMotion : Raw Value\n\n\nMovement : Raw Value\n\n\nLuminosity : Raw Value\n\n\nLight : lux\n\n\nAtmospheric Pressure : kPa\n\n\nExternal Relative Humidity : %\n\n\nRelative Humidity :  %\n\n\nRain Height : mm\n\n\nWind Direction : degrees\n\n\nWind Speed : m\/sec\n\n\nTemperature : Centigrade\n\n\nExternal Temperature : C\n\n\nRadiation \uff1auSv\/h ( be careful, too high, you will die )\n\n\nExternal Air Contaminants : Raw Value\n\n\nExternal Ammonia Concentration : Raw Value\n\n\nExternal Carbon Dioxide Concentration : Raw Value\n\n\nExternal Carbon Monoxide Concentration : Raw Value\n\n\nExternal Oxygen Concentration : Raw Value\n\n\nCarbon Monoxide Concentration : Raw Value\n\n\nMethane Concentration : Raw Value\n\n\n\n\n\nPoints of sensors connection\n\nWiFi\n2G\/3G mobile network connection\nEthernet\nLow-rate wireless personal area networks(IEEE 802.15.4)\n\nData Variability and Potential Patterns\nTake a close look on the raw data and seek for the changing patterns :\n\nTemperature\nLight\nMotion\nPower consumption\n\nDemo on all the schools in Greece, for one year, time interval: day\n\nPower Consumption, 10 schools in Greece\n\nThere are 3 other schools without power consumption sensors or no data.\n- Temperature, 12 schools in Greece\n\n\nDemo on site 8\u03bf \u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u03a0\u03b1\u03c4\u03c1\u03ce\u03bd,Greece ,for 3 weeks, time interval: hour\n\n\nTemperature for 4 weeks in the main building with building floor plan\nSo here are the pattens\n\nthe room(4ce) at ground floor , heading to the north has lowest temperature all the time.\nthe two on the first floor, two classrooms to west (class 1 and 2) are next to each other and have similar pattern of temperature changing\nand the \"warmest\" rooms in the whole school building.\nthe rooms at the north are cooler\n\n\n\n\nDemo on site \u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u03a0\u03b5\u03bd\u03c4\u03b1\u03b2\u03c1\u03cd\u03c3\u03bf\u03c5 \u039a\u03b1\u03c3\u03c4\u03bf\u03c1\u03b9\u03ac\u03c2,Greece, for 4 weeks, time interval: hour\n\n\nTemperature at the main building with building floor plan\nPatterns :\nTemperature in Computer Lab is more stable than the rest of others , but still fit our expectation.\n\n\n\nHumidity at the main building with building floor plan\nPatterns : we can see the basement has the highest humidity and the music class is stable and remain in a good dry condition for preserving the music instruments\n\n\n\nLuminosity at the main building and the sub-site building\nPattern:\nMost of the rooms use natural light but there is always light only turn off during the weekend\nThe rooms facing south exposed in the longer daylight have maximum luminosity compared with the lab in the basement.\n\n\n\n\nAvailability\nAlgorithm for Availability is different from prediction in clean outlier and inactive data\nIntuition:\n\nSome sensor data has reasonable zero value as true value,like motion while no one walking around\nSome sensor data should never be zero , like Temperature and humidity always above zero, or some others type might below zero.\n\nIn general , the summary for one point of sensors even there is(\/are) some sensor data has \"legal\" zero\nthe summary based on the same timestamps should always be above zero as long as it is active.\nProcess :\n\nPut all [value != 0] =  1\nSum for all sensor data in one points of sensors\nNormalized all [value > 0] = 1\n\nOutput :  1 = active , 0 = inactive for each points of sensors\nVisualize in Heatmap for points of sensors availabilities\n\nThis table includes large range of data which are missing due to sensors no longer working or the whole sites are power off  during [2015-Nov-1,2017-Oct-30]\n\n\n\nID\nName\nInactive\nstart time\noutlier\nTotal number of measurements\n\n\n\n\n144024\n\u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03cc \u03a3\u03c7\u03bf\u03bb\u03b5\u03af\u03bf \u039b\u03c5\u03b3\u03b9\u03ac\u03c2\n30.48 %\nbefore 2015-10-30\n16.49%\n73,000\n\n\n144242\n1\u03bf \u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u039d. \u03a6\u03b9\u03bb\u03b1\u03b4\u03ad\u03bb\u03c6\u03b5\u03b9\u03b1\u03c2\n2.94 %\nbefore 2015-10-30\n10.90%\n94,900\n\n\n144243\n\u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03cc \u03a3\u03c7\u03bf\u03bb\u03b5\u03af\u03bf \u039c\u03b5\u03b3\u03af\u03c3\u03c4\u03b7\u03c2\n24.49 %\nbefore 2015-10-30\n15.80%\n64,970\n\n\n155076\nGramsci-Keynes School\n4.56 %\n2016-08-04\n39.77%\n39.77%\n\n\n155077\nSapienza\n58.22 %\n2016-10-29\n51.01%\n177,390\n\n\n155849\n6\u03bf \u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03cc \u03a3\u03c7\u03bf\u03bb\u03b5\u03af\u03bf \u039a\u03b1\u03b9\u03c3\u03b1\u03c1\u03b9\u03b1\u03bd\u03ae\u03c2\n22.98 %\nbefore 2015-10-30\n16.76%\n52,560\n\n\n155851\n5\u03bf \u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03cc \u03a3\u03c7\u03bf\u03bb\u03b5\u03af\u03bf \u039d\u03ad\u03b1\u03c2 \u03a3\u03bc\u03cd\u03c1\u03bd\u03b7\u03c2\n29.23 %\n2016-08-02\n39.25%\n75,190\n\n\n155865\n46\u03bf \u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03cc \u03a3\u03c7\u03bf\u03bb\u03b5\u03af\u03bf \u03a0\u03b1\u03c4\u03c1\u03ce\u03bd\n38.72 %\n2016-09-22\n38.28%\n53,290\n\n\n155877\n2\u03bf \u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03cc \u03a3\u03c7\u03bf\u03bb\u03b5\u03af\u03bf \u03a0\u03b1\u03c1\u03b1\u03bb\u03af\u03b1\u03c2 \u03a0\u03b1\u03c4\u03c1\u03ce\u03bd\n35.18 %\n2017-02-01\n45.80%\n46,720\n\n\n157185\n\u0395\u03bb\u03bb\u03b7\u03bd\u03bf\u03b3\u03b5\u03c1\u03bc\u03b1\u03bd\u03b9\u03ba\u03ae \u0391\u03b3\u03c9\u03b3\u03ae\n3.31 %\n2017-02-01\n40.50%\n123,370\n\n\n159705\nSoderhamn\n0.00 %\n2017-09-21\n48.24%\n70,810\n\n\n19640\n\u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u03a0\u03b5\u03bd\u03c4\u03b1\u03b2\u03c1\u03cd\u03c3\u03bf\u03c5 \u039a\u03b1\u03c3\u03c4\u03bf\u03c1\u03b9\u03ac\u03c2\n1.72 %\nbefore 2015-10-30\n20.34%\n112,420\n\n\n27827\n8\u03bf \u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u03a0\u03b1\u03c4\u03c1\u03ce\u03bd\n3.71 %\nbefore 2015-10-30\n10.71%\n71,540\n\n\n28843\n2\u03bf \u0395\u03a0\u0391\u039b \u039b\u03ac\u03c1\u03b9\u03c3\u03b1\u03c2\n43.08 %\nbefore 2015-10-30\n22.17%\n117,530\n\n\n28850\n55o \u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03cc \u03a3\u03c7\u03bf\u03bb\u03b5\u03af\u03bf \u0391\u03b8\u03b7\u03bd\u03ce\u03bd\n21.23 %\nbefore 2015-10-30\n22.36%\n91,250\n\n\n\nVisualize in Heatmap all sensor data availabilities\n\nIn this table, statistic for sensors belong to three different vendors and different connections\n\n\n\nName\nInactive\noutlier\nTotal number of measurements\n\n\n\n\nLibelium for outdoor weather\n15.16%\n10.61%\n31,390\n\n\nSynfield for outdoor weather\n14.40 %\n9.28%\n10,950\n\n\nElectrical Power Consumption\n18.68 %\n33.40 %\n73010\n\n\n\nVisualize in Heatmap category by different connections for sensors data\n\nReliability\n\n\nClean the times period which all the sensors are inactive.\n\n\nClean the sensors which are always inactive\n\nIf the site is not power-on yet, it will not be counted as inactive\nOnly after sensor(s)(maybe just a few of them)actived, start to count inactive missing data\n\n\n\nRemove the outliers with Turkey's fences and replace with min\/max value\nWhat is outliers \n    In statistics, an outlier is an observation point that is distant from other observations.\n    An outlier may be due to variability in the measurement or it may indicate experimental error; \n    the latter are sometimes excluded from the data set.[3] Outliers can occur by chance in any distribution, \n    but they often indicate either measurement error or that the population has a heavy-tailed distribution. \n    In the former case one wishes to discard them or use statistics that are robust to outliers, \n    while in the latter case they indicate that the distribution has high skewness \n    and that one should be very cautious in using tools or intuitions that assume a normal distribution. \n    A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, \n    or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model.\n\n    Output Two ways to indicate a data point is an outlier\n     - Real-valued outlier score, higher values of the score make the point more like an outlier\n     - Binary label binary value yes or no for an data point to be outlier\n\n\n\nidentify outliers by using Turkey's fences, aka inter quartile range\nQ1 = First Quartile\nQ3 = Third Quartile\nInter-quartile Range (IQR) = Q3 - Q1\nLower Outlier Boundary = Q1 - 3 * IQR\nUpper Outlier Boundary = Q3 + 3 * IQR\n\n\nidentify outliers by using a sliding windows W holds last W-1 values\nMoving windows through data from the beginning\n\nIf the inter quartile range becomes biggest ever seen,here comes a outliers : replace it with min or max\nIf the new value is NaN, it is also an outlier : replace it with average\nmin\/max\/average = min\/max\/average (previous W-1 values)\n\n\n\nmoving window average to smooth out short-term fluctuations and highlight longer-term trends or cycles\nThe SMA is the most straightforward calculation, the average over a chosen time period. \nThe main advantage of the SMA is that it offers a smoothed line, less prone to whipsawing up and down in response to slight, \ntemporary price swings back and forth. Therefore, it provides a more stable level indicating support or resistance. \nThe SMA's weakness is that it is slower to respond to rapid changes that often occur at market reversal points. \nThe SMA is often favored by analysts operating on longer time frames, such as daily or weekly charts.\n\n\n\nrefill the NaN with average of the whole series values\n\n\n\nLinear fit\nIn statistics, linear regression is a linear approach for modeling the relationship \nbetween a scalar dependent variable y and one or more explanatory variables \n(or independent variables) denoted X.```\n\n\n\nVisualize one day temperature data after processes mentioned above\n\n\n\nAccuracy\nCan we retrieve outdoor weather through API ?\nOpenweathermap for real-time data\nBut this response is only for the real time request.\nWorldweatheronline for history data\nBoth of APIs response :\n\n\n\nTemperature\nWind\nHumidity\nPressure\nCloud...\n\n\n\nWhat about the accuracy between data retrieved from API and sensors?\n\n\nNotice API from worldweatheronline does not provide longer than 32days data\nConclusion : Yes we can retrieve both realtime and history,but the accuracy is not pretty enough\nInterpretation\nOrientation Prediction and Deviation\n\n\nAssuming the indoor temperature should rise as the day time passing by.\nWe do not put human activity or others into the consideration, for now\n\n\nIdentify patten by peak time:\n\n\nIntuitively while observing the temperature peak for different rooms:\n\nEast: the peak temperature mostly arrives at the early day\nWest: the peak  at the late of the day\nSouth: the peak should be at the mid-noon or later\nRest: room facing north\/music room\/computer lab\/basement room will have relatively low variation and average of the temperature\n\n\n\nIf we put cloud cover persentage with orientation of the room.\nObserve the time difference for indoor vs outdoor reach daily peak temperature\n\n\n\n\nPredicting orientation by using peak temperature :\n\n\nUsing RESTful API to retrieve the time : [sunrise , noon ,and sunset],unit: hour.\n\n\nOnly check the temperature during the daytime between [sunrise,sunset]\n\n\npick the hottest time and put that time:hour into [list_peak_at_hour]\n\n\nOrientation = sum( [list_peak_at_hour]  \/24hour *360degree)\/length([list_peak_at_hour] ) (unit:degree)\n\n\nSimply match Orientation into :\n\n0-90 degree: North-East\n90-180 degree: South-East\n180-270 degree: South-West\n270-360 degree: North-West\n\n(.\/image\/comp1.jpg)\n\n\nExmaple:8\u03bf \u0393\u03c5\u03bc\u03bd\u03b1\u0301\u03c3\u03b9\u03bf \u03a0\u03b1\u03c4\u03c1\u03c9\u0301\u03bd, \nClass 1 id fb8, the classroom reach the highest temperature at hour: \npeak_at_hour_list = [ 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n17, 17, 17, 16, 16, 16, 16, 18, 17, 16, 16, 17, 16, 17, 16, 16, 17, 16, 18, 17, 17, 17, 16, 16, 17, 17, 16, 17,17, \n16, 16, 17, 17, 17, 16, 17, 17, 18, 17, 17, 16, 16, 17, 17, 17, 17,17, 17, 16, 17, 17, 16, 17, 15, 16, 17, 16, 16, \n16, 16, 16, 16, 17, 17, 16, 17, 16, 17, 17, 15, 16, 17, 17, 17, 16, 16, 16] \nin total: \n[(16 o'clock, 33times), (18 o'clock, 5times),(15 o'clock, 2times), (17 o'clock, 60times)]\nOrientation = sum(16\/24*360*26+...+15\/24*360*2+17\/24*360*60)\/Length(peak_at_hour_list) = 250.2 degree\nSo we got the orientation is 250.2 degrees which looks like south-west. \n\n\nBut ... if there are also a lot of peaks happened in the morning  \nClasslB2 id :0x317.\nThis room gets enough exposed in the sunshine\/high temperature during the morning and also likely it facing to the south-east\nin total: (14 o'clock, 5times), (16 o'clock, 3times), (10 o'clock, 3times), (12 o'clock, 3times), \n(11 o'clock, 3times), (15 o'clock, 3times),(13 o'clock, 2times), (18 o'clock, 2times), (17 o'clock, 2times), \n(7 o'clock, 1times), (19 o'clock, 1times)\nOrientation = 205.7 degree,have peak temperature in the mornings at [7, 10, 11], takes 25.0% in total\nwe got something more like heading to the east , or south east\n\n\nWe take a close look at the distribution of site rooms in  different orientation\nThe south-east are in lower temperature compared with south and south-west room\n\nCategory by site :\n\n\n\nSITE ID\nOrientation Prediction Correct\n\n\n\n\n144024\n60%\n\n\n144243\n50%\n\n\n155851\n20%\n\n\n155865\n50%\n\n\n19640\n0%\n\n\n27827\n25%\n\n\n144242\n0%\n\n\n155877\n33%\n\n\n159705\n28%\n\n\n155849\n0%\n\n\n157185\n44%\n\n\nREST (no data)\n\n\n\n\nCategory by the room orientation :\n\n\n\nOrientation\nOrientation Prediction Correct\ncomment\n\n\n\n\nN - E\n0%\n0\/12\n\n\nE - S\n35%\n6\/17\n\n\nS - W\n56%\n9\/16\n\n\nW\n0%\n0\/1\n\n\nW - N\n0%\n0\/11\n\n\n\n\n\nThe cloudy coverage impact on the indoor temperature:\n\n\n\n\n\n\n\n\nIdentify deviation by slope = delta(temperature)\/delta(time):\n\n\nSlope for one room: first picture is the data after ETL, second is the slop for the data\nIf we are focusing on detecting sudden change of the indoor temperature, this slope plot could provide fast and efficient\nway, such as the fluctuate exists on Sep-23 on the top plot, a peak matching to this change is observed at the bottom plot as well.\nWe can use the similar algorithm: searching the \"outliers\" for slope to detect the fluctuate.\n\n\n\nAnd put all the classrooms from one site together(ETL data on top and slope on bottom) we can detect which room behavior abnormal :\nthe room in red during the day time and room in orange in the night time.\n\n\n\n\n\n\nComfort\nFrom Wikipedia\n\nThermal comfort\n\nThermal comfort is the condition of mind that expresses satisfaction with the thermal environment \nand is assessed by subjective evaluation (ANSI\/ASHRAE Standard 55).\n\n\nANSI\/ASHRAE Standard 55\n\n(Thermal Environmental Conditions for Human Occupancy) is a standard \nthat provides minimum requirements for acceptable thermal indoor environments. \nThe purpose of the standard is to specify the combinations of indoor thermal environmental factors \nand personal factors that will produce thermal environmental conditions \nacceptable to a majority of the occupants within the space\n\nThe standard addresses the four primary environmental factors \n(temperature, thermal radiation, humidity, and air speed) \nand two personal factors (activity and clothing) that affect thermal comfort. \nIt is applicable for healthy adults at atmospheric pressures in altitudes up to (or equivalent to) 3,000 m (9,800 ft), \nand for indoor spaces designed for occupancy of at least 15 minutes.\n\n\nComfort zone\n\nRefers to the combinations of air temperature, mean radiant temperature (tr), \nand humidity that are predicted to be an acceptable thermal environment at particular values of \nair speed, metabolic rate, and clothing insulation (Icl)\n\nIntuitively, we want the temperature indoor in the certain range like [18,24]\nduring Monday to Friday, from 8:00 to 18:00\nObviously, the truth is not always what we wish for\n\nTool: CBE Thermal Comfort Tool for ASHRAE-55 \nHow to use:\nBy choosing the Adaptive method at the very top of the user interface, \nthe chart changes and the input variables include air temperature, mean radiant temperature and prevailing mean outdoor temperature. \nThis is because the personal factors and humidity are not significant in this method since adaptation is considered, and the only variable is the outdoor temperature.\nSee above for explanation of the first two variables, air and mean radiant temperature.\n\nPrevailing mean outdoor temperature\nHere you can type the outdoor temperature averaged as explained on the standard. \nSee the Wikipedia link for a brief explanation.\nChanging this variable makes the dot representing the current condition move horizontally. \nThe meaning of this chart is that certain conditions of indoor-outdoor temperature fall inside the comfort zone, \nwhich in this case is static.\n\n\n\n\nSample period 2017.Sep.05-2017.Nov.04 , weekday: Monday-Friday, Time: 8:00-16:00, week of year: 36-44\n\n\nIn daytime, this room is comforable for (n*8hours) , 0 < n < 1\n\n\nAll day comfortable = 1 all day not comfortable = 0\n\n\nThe following two pictures are comfortness ratio per day is based on hourly temperature from site ID 27827 8\u03bf \u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u03a0\u03b1\u03c4\u03c1\u03ce\u03bd\n\nuse \"Site-Temperature\" sensor as outdoor temperature source\n\nUse Worldweatheronline API as outdoor temperature source\n\nThe difference might because of the not accuracy from \"Site-Temperature\"\n\n\n\nThe following two pictures are comfortness ratio per day is based on hourly temperature from site ID 144242,1\u03bf \u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u039d. \u03a6\u03b9\u03bb\u03b1\u03b4\u03ad\u03bb\u03c6\u03b5\u03b9\u03b1\u03c2\n\nuse libelium sensor as outdoor temperature source\n\nUse Worldweatheronline API as outdoor temperature.\n\nThe difference might because of the \"complete nonsense data\" from this libelium sensor\n\n\n\nThe following two pictures are comfortness ratio per day is based on hourly temperature from site ID 19640,\u0393\u03c5\u03bc\u03bd\u03ac\u03c3\u03b9\u03bf \u03a0\u03b5\u03bd\u03c4\u03b1\u03b2\u03c1\u03cd\u03c3\u03bf\u03c5 \u039a\u03b1\u03c3\u03c4\u03bf\u03c1\u03b9\u03ac\u03c2\n- use libelium sensor as outdoor temperature source\n\n\nUse Worldweatheronline API as outdoor temperature.\n\n\n\n\n\n\nRetrieve the data\n\n\n Retrieve the data by using APIs on  https:\/\/api.sparkworks.net\/swagger-ui.html\nDemo for \"POST \/v1\/resource\/query\/timerange\" :\n\n\n    Command line:\n        curl -X POST --header 'Content-Type: application\/json' --header 'Accept: application\/json' --header 'Authorization: Bearer cd885cf5-7fca-4be8-b32e-97225da6763f' -d '{\n          \"queries\": [\n            {\n              \"from\": 1498867200000,\n              \"granularity\": \"day\",\n              \"resourceID\": 156972,\n              \"resultLimit\": 0,\n              \"to\": 1500076799000\n            }\n          ]\n        }' 'https:\/\/api.sparkworks.net\/v1\/resource\/query\/timerange'\n        \n        \n    Reponse body:\n        {\n          \"results\": {\n            \"{\\\"resourceID\\\":156972,\\\"resourceURI\\\":\\\"gaia-ea\/room-1\/temp\\\",\\\"from\\\":1498867200000,\\\"to\\\":1500076799000,\\\"granularity\\\":\\\"day\\\",\\\"resultLimit\\\":0}\": {\n              \"average\": 31.995042261495424,\n              \"summary\": 479.92563392243136,\n              \"data\": [\n                {\n                  \"timestamp\": 1498856400000,\n                  \"reading\": 34.19535065107274\n                },\n                {\n                  \"timestamp\": 1498942800000,\n                  \"reading\": 35.618584889499054\n                },\n                ....\n                {\n                  \"timestamp\": 1499979600000,\n                  \"reading\": 25.807027188020786\n                },\n                {\n                  \"timestamp\": 1500066000000,\n                  \"reading\": 28.399119190883642\n                }\n              ]\n            }\n          }\n        }\n\nKnown Issues:\n\n\n when we request data within the time range, for different granularity ,\nthe response time stamps are different\n\n5min : it's the code running time, not the fixed data timestamp as below\n1hour : beginning of every hour\n1day : 21:00 for each day\n1month :  at 21:00 last day of the month\n\nSolution : use \".toInstant().toEpochMilli()\/ 300000*300000\" to change the time into 5mins interval in one hour.\nSo we will have 5', 10',15',...55' for the record timestamp\n\n\n Cassandra do not have connector as data source for flink\nSolution\nreference : CassandraConnectorITCase \nClusterBuilder cb = new ClusterBuilder() {\n        @Override\n        public Cluster buildCluster(Cluster.Builder builder) {\n            return builder.addContactPoint(\"127.0.0.1\").build();\n        }\n    };\nString query = \"SELECT ResourceID,Reading FROM gaia.reading_data WHERE ResourceID=155873\";\nInputFormat<Tuple2<Integer, Float>, InputSplit> source = new CassandraInputFormat<>(query, cb);\nsource.configure(null);\nsource.open(null);\nList<Tuple2<Integer, Float>> result = new ArrayList<>();\nwhile (!source.reachedEnd()) result.add(source.nextRecord(new Tuple2<Integer, Float>()));\nsource.close();\n\n\n\n Can't read data through API:\nResource  historical data resource ID: 90946 from : 2017-08-24T09:25:49.323Z to 2017-08-31T09:25:49.080Z with steps per hour\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nHowever the real data is not all-zeros in https:\/\/console.sparkworks.net\/resource\/view\/90946\nWe will use the data extracted from API for further process , the console data are only for reference.\n\n\nData is missed from time to time:\nResource  historical data resource ID: 155918 from : 2017-08-24T09:25:49.323Z to 2017-08-31T09:25:49.080Z with steps per hour [32.34, 32.34, 32.34, 32.473637, 33.4425, 34.259167, 34.75733, 34.365334, 33.32, 32.764668, 32.570587, 32.36722, 32.3155, 32.3068, 32.241306, 31.868149, 31.868149, 31.85, 31.838118, 31.808867, 31.826338, 31.822662, 31.832302, 31.852188, 31.842134, 31.841246, 31.85319, 32.140522, 33.013374, 33.919827, 34.664608, 34.72307, 32.34, 32.34, 32.34, 31.85, 31.85, 32.34, 31.85, 31.85, 31.85, 31.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.367912, 31.374935, 31.410751, 31.416487, 31.51673, 31.94285, 32.77366, 33.64148, 34.232605, 33.768044, 32.68922, 32.30267, 32.140594, 31.958946, 31.87191, 31.852951, 31.830269, 31.64, 31.46889, 31.456898, 31.3657, 31.385254, 31.381018, 31.41322, 31.534771, 31.668463, 31.79291, 31.836, 31.869188, 32.38108, 33.044456, 33.666306, 33.827496, 32.96945, 32.38393, 32.23478, 32.038074, 31.907423, 31.856163, 31.844433, 31.838556, 31.69923, 31.6932, 31.511343, 31.412098, 31.391008, 31.380096, 31.443954, 31.619139, 31.776262, 31.830036, 31.832235, 31.799591, 31.84828, 32.09569, 32.928936, 33.33732, 33.126163, 32.188957, 31.83223, 31.623442, 31.422161, 31.361742, 31.356749, 31.3123, 30.906296, 30.886333, 30.878448, 30.859062, 30.843264, 30.859325, 30.861336, 30.849495, 30.85837, 30.85697]\n\n\nReference\n\nThe building data genome project\nBig data computing\nNaked Statistics: Stripping the Dread from the Data by Charles Wheelan\nReal Time sensor status on GAIA\n\nSupervisors:\nProfessor Ioannis Chatzigiannakis\nProfessor Aris Anagnostopoulos\nAcknowledgement :\nSince the first day I stepped into Sapienza , you are the ones lead me to \u201cfind true love with data mining\u201d.\nTill the last six months, you not only gave me \u201cfish\u201d but also taught me \u201chow to fish\u201d with all your supports and encourage.\nThat is beyond anything I could expect but I am so blessed to have. Without you, I won\u2019t be standing here today.\n","114":"Assistant System by Environmental Analysis For Visually Impaired\n\u4e2d\u6587\u7248 Readme \u8acb\u9078\u6211\nSummary\nWith the evolution of society, Socially vulnerable groups getting attention gradually, so use technology to solve or help socially vulnerable groups make life more friendly and also have become a major issue.\nAnd although there are many assistive devices can assist visually impaired people to live, but there are still many problems can not be solved, such as the use of guide cane can not sense obstacles upper body when walking or unable to understand the environment and other problems, resulting in danger.\n\n\n\nTherefore, the contribution of this paper is to propose an innovative Assistive technology devices conception, through a combination of hardware and software wearable device, design a vest for the visually impaired person can wear and combine with the system which can do image recognition technology to identify surrounding environments.\nThis paper focuses on visually impaired people when walking issue. Design an environment recognition system and to detect intersection zebra, avoid straying into dangerous lane systems and billboards when they\u2019re walking. For billboards system, through the color histogram filter object by color feature, and do object recognition by SURF feature; crossing detection system, looking straight through Hough Line, then repairing and filtering line, and analysis zebra crossing texture to judge finally.\n\n\n\n\n\n\nThe system can be modular for the expansion needs of the visually impaired and function. Developing friendly and useful assistive technology devices for visually impaired people.\nDevelopment Environment\n\nProgramming Language \uff1aC#\nIDE : Visual Studio 2012\nLibrary : EmguCV 2.4.0 for x86 (libemgucv-windows-x86-2.4.0.1717.exe)\n\nSlide Link\nYou could see the slide from the Slide Link\nPaper Link\nYou could read the introduction of paper from Paper Link\n","115":"MonalisR \n\n\n\nHandling open Databases in South Tyrol\nThis R-Package is designed to interact with open environmental databases in the Autonomous Province of South Tyrol in Italy. Our mission is to use exposed APIs in order to etrieve a multitude of environmental variables. This approach guarantees a steady and reproducible access to a wide variety of data potentially interesting for diverse scientific research.For each database core functionalities were implemented aiming at simplifying:\n\ndata access and exploration\ndownload and storage of the desired data set(s)\nspatial plotting of the data \n\nFor each database the respective functions are divided according to these three tasks. Following that, the naming conventions utilize the prefixes get, plot and download.\nDatabases\nThis package offers the possibility to access databases exposing JSON Files with Sensor Observation Service (SOS) convention defined by the Open Geospatial Consortium (OGC) or by CKAN APIs. We implemented the access to two different Databases as exposed by EURAC Research and the OpenData Portal South Tytol.\nBoth Databases offer rich environmental and meteorological Databases across the province of South Tyrol.\nMONALISA\nThe MONALISA project (MONitoring key environmental parameters in the ALpine environment Involving Science, technology and Application) aims at the development of multi-scale monitoring approaches for key environmental parameters and production processes using innovative monitoring technologies and non-destructive methods in the application field of agriculture.\nWithin this project the MONALISA Database has been created to store and distribute the wide variety of environmental parameters collected.\nOpen Data Portal South Tyrol - Meteo Data\nA second platform to retrieve open data for scientific applications is now openly accessible via the Open Data Portal of the Autonomous Province of South Tyrol. On this platform multiple meteorological variables have been opened to the public. A central API handles the request for each single database stored. For now the package offers the possibility to access one of these databases is addressable with this package containing meteorological data of several fixed Stations continuously operated by the Meteorological Service of South Tyrol.\nDownload the Package\nThe stable versions of the packages will be shared on the Github and can be downloaded without the need to provide the credentials: \nlibrary(devtools)\n\ndevtools::install_github(\"https:\/\/github.com\/mattia6690\/MonalisR\")\n\nContributors & Contact\nMattia Rossi \nDaniel Frisinghelli \n\n","116":"EnvironmentalThreatsApliedDataScience\n","117":"env\nEnvironmental configuration files to be shared across any machine I happen to\nbe using.\nIn addition to these files, it have also installed git completion manually\n(not sure if this is still needed):\nhttps:\/\/raw.githubusercontent.com\/git\/git\/master\/contrib\/completion\/git-completion.bash\nvim\nFor vim, install these plugins by cloning into\n${HOME}\/.vim\/pack\/bundle\/start\/. The following are a list of vim libraries\nI've used at one point or another.\n\nvim-slime Inject from one tmux\npane to another.\nale Asynchronous Lint Engine\ntabular Auto-align text,\ntypically by a record separator. For example, :Tabularize \/| aligns on\npipe characters.\nvim-repeat Allows using . with\nplugins.\nvim-sandwich\nnerdcommenter Code\ncommenting functions.\nvim-grepper Grepper for\nasynchronous greps\nrainbow For rainbow\nparentheses.\n\nClojure\n\nvim-clojure-static\nvim-iced\nvim-sexp\n\nErlang\n\nerlang-motions\nvim-erlang-compiler\nvim-erlang-tags\nvim-erlang-omnicomplete\n\nElixir\n\nalchemist\nphoenix\n\nvim Colors\nI've been favoring light themes for a bit now, but I keep a sparse\n..vim\/colors\/ directory. Currently using\nPaperColor,\nbut have also been happy with\nsolarized8_flat.\nCTags\nI sometimes set up projects to use ctags.\nuniversal-ctags\nOCaml\nI'm not actively developing in OCaml at the moment, but I still like to have\nit set up in my environment, particularly so I can take advantage of\npatdiff. My\n.bash_profile already checks for opam installation (the\nOCaml package manager), .ocamlinit is already set up, and\n.gitconfig is set up to allow git patdiff <file>.\nbrew install opam\nopam init\neval `opam config env`\nopam update\nopam install patdiff\n","118":"\n\nEnvironmentally-Driven Edge Detection Program\nDisentangling environmental effects in microbial association networks\nEven though ecological interactions among microbes are fundamental for ecosystem functioning,\nmost of them remain unknown. High-throughput omics can help unveiling microbial interactions\nby inferring species correlations over space or time, which can be represented as networks.\nAssociations in these networks can indicate ecological interactions between species or\nalternatively, similar or different environmental preferences. Therefore, it is important to\ndisentangle these associations and determine whether two species are correlated because they\ninteract ecologically or because they are correlated to an abiotic or biotic environmental\nfactor. We developed an approach to determine whether or not two species are associated in a\nnetwork due to environmental preference. We use four methods (Sign Pattern, Overlap,\nInteraction Information, and Data Processing Inequality) that in combination can detect what\nassociations in a network are environmentally-driven. The approach is implemented in the\npublicly available software tool EnDED.\nGENERAL USAGE NOTES\nEnDED is a program that aims to detect environmentally-driven (\"indirect\") edges in an association network.\nThe makefile contains the code to generate all required commands to compile the program.\nGo within the terminal into the folder EnDED and type\nmake\n\nThe command to use the program is:\nEnDED\/build\/EnDED --input_network_file [PATH\/NETWORKFILE] --methods [METHODS] [optional OPTIONS]\n\nDEPENDENCIES\nThe program requires the\n\ngcc = GNU Compiler Collection (GCC)\nboost = boost C++ Libraries\n\nGet help with\n.\/build\/EnDED -h\n\nor\n.\/build\/EnDED --help\n\nOptions\n\n-h,--help Returns this help message\n-v,--version Returns the version of the program.\n-d,--defaults Returns the default values of the program.\n\nRequired options\n\n-f,--input_network_file [network_filename] Path and name of\nthe input network file. The first line(row) represent the\ncolumn names.\n-m,--methods [method IDs] Lists the method\/s you would like\nto use separated by a comma. SP=SignPattern, OL=Overlap,\nII=InteractionInformation, DPI=DataProcessingInequality\n(minMi), CO=Co-Occurrence, e.g.: SP,II. Instead of mentioning\nall methods, you can shortly write -m all\n\nMethods Settings:\nSign Pattern:\n\n--SP_colnum_interaction_score [num] Required for method\nSignPattern(SP). \"num\" is the column number of the interaction\nscore.\n\nOverlap:\n\n\n--OL_colnum_interactionlength_startX_startY [num1,num2,num3]\nRequired for method Overlap(OL).\n\"num1\" = column containing the interaction length,\n\"num2\" and \"num3\" = columns indicating start point of X and Y.\n\n\n--OL_percentage_threshold [num] Default: 60.0, edge is\nconsidered not indirect, if overlap is below this number.\n\n\nData Processing Inequality:\n\n--DPI_minMI_threshold [double] Default: 0, threshold for how much\nsmaller min MI has to be compared to other two MIs.\n\nInteraction Information and Data Processing Inequality:\n\n\n--II_DPI_max_nan_threshold [num] Default: 20, threshold(in\npercentage) for maximum portion of 'nan' in vectors used for II\nand DPI.\nInteraction Information - Significance determination:\n\n\n--II_significance_level [double] Default: 0.05, significance\nlevel for interaction information is a number between 0 and 1.\n\n\n--II_permutation_iteration [int].Default: 100, number of\niterations to determine the significance of the interaction\ninformation.\n\n\n--do_pre_jointP_comp Default: ENV vectors are randomized anew\nfor each Signifiance determination.\nOptional: ENV vectors are permutated before and stored.\nNote: might cause memory troubles, so it is not set by default.\n\n\nStrategy for method combination:\n\n\n--method_count_threshold [double] Default: 100, threshold(in\npercentage) for minimum methods that suggest edge as indirect.\n\n\n--triplet_count_threshold [double] Default: 1, threshold for\nminimum triplets that suggest edge as indirect.\n\n\nAdditional input files:\n\n\n--II_DPI_abundance_file [filename] Required for method\nInteractionInformation(II) and DataProcessingInequality(DPI).\nName(if not in same folder, also with path) of the abundance file\n(first line are the column names: 1st column = ID\/ENV-name, other\ncolumns are abundances).\n\n\n--II_DPI_ENVparameter_file [filename] Optional for method\nInteractionInformation(II) and DataProcessingInequality(DPI).\nName(if not in same folder, also with path) of the ENV parameter file,\nif ENV not within the abundance file, then first line are the column\nnames: 1st column = ENV-name, other columns are abundances)(Column names\nhave to match the ones from the abundance file).\n\n\n--input_ID_ENV_nw_file [network_filename] Optional: Name of the input\nnetwork file that contains ID-ENV edges (first line are the column names,\nthat have to be the same as the ones for the network file).\n\n\nInputfile separators:\n\n\n--separator_network_file \"[sep]\" Default: tab, other possibilities, but\nnot limited to: \";\" or \" \"\n\n\n--separator_abundance_file \"[sep]\" Default: tab, other possibilities, but\nnot limited to: \";\" or \" \"\n\n\nOutput:\n\n\n-o,--output_network_file [filename] Default: \"extended_nw.txt\"; name of\nthe output network file.\n\n\n--output_ID_ENV_edge_dont_print Default: ID-ENV edges are printed in\nextended nw file. With this option, they will not be printed.\n\n\n-t,--output_triplet_info [filename] Create output with Triplets information.\n\n\n--output_discretized_vectors [filename] Create output with discretized vectors.\n\n\nOther:\n\n\n--input_node_col [colnum_x,colnum_y]Default: \"1,2\". Number of the 2 columns\ncontaining the nodes X and Y.\n\n\n--input_ENV_identicator [indicator string]Default: \"ENV\"; indicator for\nenvironmental parameter name, needs to be included in node name.\n\n\nDefault settings\n\nNo method is selected by default.\nMethod Overlap: The percentage threshold to decide if an edge\nis regared as indirect by the method is 60.\nMethod Interaction Information: The significance level of the\nInteraction Information Score is 0.05.\nMethod Interaction Information: The number of permutation to\ndetermine the significance of the Interaction Information\nScore is 100.\nMethod Data Processing Inequality compares Mutual\nInformations. Ranks Mutual Information and by default regards\nID-ID as indirect if it is the smallest. That means, by\ndefault is the threshold for how much smaller the MI has to\nbe: 0.\nENV-vector will be permuted anew during examining the networks\ntriplets.\nMethod Interaction Information and Data Processing Inequality\nuse data that may contain 'nan'. By default is the threshold(in\npercentage) for maximum portion of 'nan' in vectors: 20.\nCombination strategy of considering an edge indirect. By\ndefault all methods have to agree that an edge within one\ntriplet is indirect before it is considered as indirect. And\nat least one triplet has to suggest the edge as being indirect\nbefore the edge is considered indirect.\nInputfile separators are by default tabs\nOutput: by default the only output is the network file which\nis extended by the methods information and named as\nextended_nw.txt\nID-ENV edges are printed in extended nw file.\nNumber of the 2 columns containing the nodes X and Y are by\ndefault 1,2.\nENV is the indicator for environmental parameter name that\nneeds to be included in node name.\n\nExample: test data\nInput data\nThe folder test_data contains an example datasets that can be used\nto try out the program. The following files are included:\nInput folder containing\n- network.txt containing the network file (ID-ID edges,\nID-ENV edges, and ENV-ENV edges)\n- ID_ENV_edges.txt containing ID-ENV edges\n- ID_abundance.txt contains the abundances\/counts for\nthe IDs (rows) per sample (columns)\n- ENV_parameters.txt contains the environmental\nparameters for the ENVs (rows) per sample (columns)\nOutput data\nlogfile\nThe log-file contains information about the program and the\nindirect edge detection:\n\ncommand used to run the program\nprogram version\nSettings\nInputfiles\nInformation tracked during the indirect edge detection\n\nnumber of ID-ENV edges found in input file and how many\nof them are considered for the indirect edge detection\n(duplicated are not considered, only first appearance\nwill be used).\nnumber of ID-ENV edges that are within at least one\ntriplet.\nnumber of ENV-ENV edges.\nnumber of ID-ID edges.\nnumber of ID-ID edges that are not in a triplet.\nnumber of IDs are connected to at least one ENV\nparameter.\nnumber of ID-ID edges that are in at least one triplet.\nnumber of edges that are considered indirect, and not indirect by each selected method and (if more\nthan one method got selected) by the method combination.\n\n\nStart and end time as well as the duration of the computation.\n\nextended_network\nThe output of the EnDED program is Output\/extended_network.txt.\nIt's columns are all columns from the input network file and\nadditionally the following columns:\n\nSignPattern is 0 for indirect and 1 for not indirect.\nOverlap is the maximal detected Overlap in percentage of the\nX-Y edge with the edges X-ENV and Y-ENV.\nMutualInformation is the mutual information between X and Y.\nInteractionInformation is the minimal significant\ninteraction information of X, Y, and ENV.\nII_p_value is the significance of the interaction\ninformation of X, Y, and each of the ENV.\nDataProcessingInequality_Rank is the minimal rank of the\nmutual information of X, Y in comparison with the mutual\ninformation of X-ENV and Y-ENV for each of the ENV: 1 being\nthe smallest and 3 being the highest.\nDataProcessingInequality_indirect is 0 for indirect and\n1 for not indirect. The data processing information regards\nthe X-Y association as indirect if the mutual information of\nX and Y is smaller than the other two (by default). Or it\nhas to be smaller by a certain value that can be specified\nwith: --DPI_minMI_threshold. That means the mutual\ninformation has to be smaller than MI-DPI_minMI_threshold in\norder for DPI to determine the XY-association as indirect.\nCOMBI_SP_OL_II_DPI is 0 for indirect and 1. It is based\non a combination of the 4 methods in detecting indirect\nedges.\npercentage_co_occurrence is the percentage of how often X and Y\nco-occurred in all samples in which at least one of them occurred.\n\ntriplet_info.txt\nAn extra output that contains information about the triplets: Output\/triplet_info.txt.\nColumns:\n\nX nodes X as indicated in the network file\nY nodes Y as indicated in the network file\nnum_triplets number of triplets that contain the edge\nbetween X and Y\nENV environmental factor\/s that is\/are in a closed triplet\nwith X and Y\nSignPattern is a ';'-separated list that contains for each\ntriplet the Sign Pattern indirect detection determination:\n0 for indirect and 1 for not indirect.\nOverlap is a ';'-separated list that contains for each\ntriplet the Overlap in percentage of the X-Y edge with the\nedges X-ENV and Y-ENV.\nMutualInformation is the mutual information between X and Y.\nConditionalMutualInformation is a ';'-separated list that\ncontains for each triplet the conditional mutual information\nof X, Y, and each of the ENV.\nInteractionInformation is a ';'-separated list that contains\nfor each triplet the interaction information of X, Y, and\neach of the ENV.\nII_p_value is a ';'-separated list that contains for each\ntriplet the significance of the interaction information of X,\nY, and each of the ENV.\nDataProcessingInequality_MI_rank is a ';'-separated list\nthat contains for each triplet the rank of the mutual\ninformation of X, Y in comparison with the mutual\ninformation of X-ENV and Y-ENV for each of the ENV: 1 being\nthe smallest and 3 being the highest.\nDataProcessingInequality_indirect is a ';'-separated list\nthat contains for each triplet if the data processing\ninformation regards the X-Y association as indirect (0) or\nnot indirect (1). It is regarded as indirect if the mutual\ninformation of X and Y is smaller than the other two (by\ndefault). Or it has to be smaller by a certain value that can\nbe specified with: --DPI_minMI_threshold. That means the\nmutual information has to be smaller than MI reduced by\nDPI_minMI_threshold (=MI-DPI_minMI_threshold) in order for\nDPI to determine the XY-association as indirect.\nDPI_num_MI_rank1 counts the number of times the\nXY-association had rank 1.\nDPI_num_MI_rank2 counts the number of times the\nXY-association had rank 2.\nDPI_num_MI_rank3 counts the number of times the\nXY-association had rank 3.\nCOMBI_SP_OL_II_DPI is a ';'-separated list that contains for\neach triplet the combination of the 4 methods in detecting\nindirect edges: 0 for indirect and 1 for not indirect.\nportion_NA_X_Y gives the portion of pairwise NAs in the\nvectors for X and Y. If it is above the threshold, MI will be\nNA and II and DPI will be NA as well.\n`portion_NA_X_Y_ENV is a ';'-separated list that gives the\nportion of three-wise NAs in the vectors for X and Y and ENV\nfor each ENV. If it is above the threshold, MI will be NA and\nII and DPI will be NA as well.\npresence_X is the number of samples in which X occurred.\npresence_Y is the number of samples in which Y occurred.\npresence_X_AND_Y is the number of samples in which both, i.e.\nX and Y, occurred.\npresence_X_OR_Y is the number of samples in which at least one\nof X and Y occurred.\npercentage_co_occurrence is the percentage of how often X and Y\nco-occurred in all samples in which at least one of them occurred.\nNote: the column names X and Y are the column names for node X\nand node Y in the network file, and ENV is the\nENV-indicator that can be adjusted with option\n--input_ENV_identicator [indicator string].\n\ndisc_vectors.txt\nAn extra output that contains discretized vectors:\nOutput\/disc_vectors.txt.\nColumns = columns from input abundance file\nContent = discretized values of the original abundance\/count\nID data and environmental parameters\nCommand\nAfter loading all required modules, and compiling the program.\nThe program can be run.\nThe command to produce the output files in the test data set is presented here:\n.\/..\/build\/EnDED --input_network_file Input\/network.txt --methods SP,OL,II,DPI,CO --SP_colnum_interaction_score 3 --OL_colnum_interactionlength_startX_startY 8,6,7 --II_DPI_abundance_file Input\/ID_abundance.txt --II_DPI_ENVparameter_file Input\/ENV_parameters.txt --output_network_file Output\/extended_network.txt --output_triplet_info Output\/triplet_info.txt --output_discretized_vectors Output\/disc_vectors.txt --do_pre_jointP_comp --II_permutation_iteration 100\n\nExample: real data (used in EnDED paper)\nThe folder BBMO_data contains the abundance table, ASV taxonomy assignment, network including EnDED results, and file containing all triplets with the environmental factor and how methods performed on each one of them.\nSimulated data (used in EnDED paper)\nThe file FromDataSimulationToEvaluatingEnDED.RMD contains R code to generate simulated abundance tables, commands to run eLSA network construction and EnDED, as well as the command to run the C++ program network_evaluation.cpp and R code used for evaluation.\nBackground\nMicrobial communities are not a mere collection of independent\nindividuals; they are interconnected being involved in various\necological interactions such as symbiosis, parasitism or\npredation. These interactions are important to maintain\necosystem function. For understanding microbial ecosystems, it\nis essential to understand microbiome interactions. They are\nbarely understood due to previous limitations in tools and\nmethods. However, during the last decade, there have been\nadvances in omics tools, analytical methods as well as\ncomputing performance.\nThe omics-technologies allow the screening of a large number of\nmicrobes; we obtain a list of microbes that are present in an\necosystem and are also able to quantify them. This data can be\nused to infer associations. Analyzing and converting microbiome\ndata into meaningful biological insights is challenging.\nFurthermore, microbes and microbial interactions can be\naffected by the environment. Thus, associations between\nmicrobes and the environment should be considered when studying\nan ecosystem.\nMicrobial interactions can be represented and modeled as\nnetworks: The nodes of the network represent the microbes as\nwell as the environmental parameters. An edge between two nodes\nrepresents an association between two microbes, or between a\nmicrobe and environmental parameter. State of the art network\nconstruction tools are still far from perfect. The obtained\nMicrobial Association Networks are likely to contain false\nassociations due to indirect associations (indirect edges). The\nso called Effect of indirect dependencies is when two species\nare indirectly correlated because both are correlated with a\nthird species or environmental factor, i.e. two microbes\nrespond similarly to a common environmental condition. Thus, in\nassociation networks we obtain two types of edges: direct and\nindirect associations.\nOnly direct associations predict microbial interactions.\nTherefore, indirect associations need to be removed from the\nnetwork before analyzing and interpreting the network as a\nrepresentation of the microbial ecosystem. In order to remove\nindirect edges from microbial networks that are environmentally\ndriven we implemented (C++ program EnDED - indirect edge\ndetection), which contains four methods that aim to detect\nindirect edges: Sign Pattern, Overlap, Interaction Information,\nand Data Processing Inequality.\nThe indirect edge detection methods\nThere are four methods that aim to detect indirect edges: Sign\nPattern, Overlap, Interaction Information, and Data Processing\nInequality. All four methods use closed triplets to detect\nenvironmentally driven edges within the network, which is a\nconstellation of three nodes that are all connected to each\nother. Let T={v_1,v_2,v_3} be the closed triplet. Let v_1 and\nv_2 be species and v_3 be the environment. Such triplets have\nbeen used by (Lima-Mendez, Faust, Henry et al. 2015,\nDeterminants of community structure in the global plankton  interactome) and were called environmental triplets. Note that\nan edge between two species might be in none, one or several\ntriplets with different considered environmental parameters,\ne.g. temperature, salinity, oxygen level.\n\nSign Pattern uses the sign of the association, i.e. whether\nthe association is positive or negative.\nOverlap uses the start and length of the association.\nInteraction Information and Data Processing Inequality use the\nabundance\/count and environmental data given for the two\nspecies and the environmental parameter.\n\nWe build up on three methods, namely Sign Pattern, Interaction\nInformation, and Data Processing Inequality, as well as\ndeveloped the method Overlap to use the additional information\nthat can be obtained from temporal data. We combine these methods.\nOnly if all methods suggest that the edge is indirect, the edge is\nconsidered to be indirect. As for now, the methods (and combination\nfor them) are implemented in the C++ program, EnDED (short for\nEnvironmentally-Driven Edge Detection).\nAdditionally, the co-occurrence of the X and Y is determined that\nmay be used filter and analyse the network further.\nPlease cite as\nIna Maria Deutschmann, Gipsi Lima-Mendez, Anders K. Krabber\u00f8d, Jeroen Raes, Karoline Faust and Ramiro Logares (2019). EnDED - Environmentally-Driven Edge Detection Program\nhttps:\/\/doi.org\/10.5281\/zenodo.3271730\nVersion\nEnDED_v1.0.1, 15. June  2019\nCONTACT\nIna Maria Deutschmann\nina.m.deutschmann[at]gmail.com\n","119":"EPA SMS Dispatcher\n\nThis is a simple system for Environmental Protection Agency to dispatch its Substitute Military Service personnel.\nAlthough this system is designed for EPA, it can be easily modified to fit other departments as well.\nDemo\nDemo page:\n\nhttp:\/\/chunnorris.cc\/demo\/epa-sms\/\n\nDemo files:\n\nSample Input (txt)\nSample Output Personnel (csv)\nSample Output Personnel (jpg)\nSample Output Region (csv)\nSample Output Region (jpg)\n\n\uff08\u96f6\uff09\u6b65\u9a5f\u7e3d\u89bd\n\n\u4e8b\u524d\u6e96\u5099\uff0c\u4fee\u6539\u5f79\u7537\u4eba\u6578\u3001\u5404\u7e23\u5e02\u540d\u984d\u7b49\u53c3\u6578\u3002\n\u6574\u7406\u5f79\u7537\u540d\u55ae\uff0c\u4e26\u5f59\u6574\u6210\u6b64\u7cfb\u7d71\u53ef\u63a5\u53d7\u7684\u683c\u5f0f\u3002\n\u958b\u59cb\u9810\u6392\u3002\n\u5132\u5b58\u4e26\u5217\u5370\u9810\u6392\u4e4b\u7d50\u679c\u3002\n\n\uff08\u4e00\uff09\u4fee\u6539\u53c3\u6578\n\n\u4f9d\u7167\u9019\u4e00\u68af\u7684\u5f79\u7537\u7e3d\u4eba\u6578\u4fee\u6539 region.json \u7684 what_T \u6b04\u4f4d\n\u4f9d\u7167\u9019\u4e00\u68af\u7684\u5f79\u7537\u7e3d\u4eba\u6578\u4fee\u6539 region.json \u7684 total_students \u6b04\u4f4d\n\u4f9d\u7167\u9019\u4e00\u68af\u5404\u7e23\u5e02\u7684\u540d\u984d\u4fee\u6539 region.json \u7684 available \u6b04\u4f4d\n\n\n\uff08\u4e8c\uff09\u5916\u90e8\u532f\u5165\u8cc7\u6599\u4e4b\u683c\u5f0f\n\u9664\u4e86\u624b\u52d5\u8f38\u5165\u5916\uff0c\u4e5f\u53ef\u4ee5\u5f9e\u6a94\u6848\u76f4\u63a5\u532f\u5165\u8cc7\u6599\u3002\n\u5916\u90e8\u532f\u5165\u7684\u8cc7\u6599\u5fc5\u9808\u7b26\u5408\u4e0b\u5217\u683c\u5f0f\uff1a\n\n\u5fc5\u9808\u662f\u4ee5 ANSI Big5 \u7de8\u78bc\u7684 csv \u6a94\u6848\uff0c\n\u6216\u662f\u6709 BOM \u7684 UTF-8 \u7de8\u78bc\u7684 csv \u6a94\u6848\u3002\ncsv \u8868\u683c\u5167\u7e3d\u5171\u4e09\u6b04\uff0c\u7b2c\u4e00\u6b04\u70ba\u865f\u78bc\u3001\u7b2c\u4e8c\u6b04\u70ba\u5206\u6578\u3001\u7b2c\u4e09\u6b04\u70ba\u6236\u7c4d\u5730\u3002\u5982\u4e0b\u5716\uff1a\n\n\n\uff08\u4e09\uff09\u5916\u90e8\u532f\u5165\u8cc7\u6599\u7bc4\u4f8b\u6d41\u7a0b\n(1) \u62ff\u5230\u6559\u52d9\u7d44\u7d66\u7684 \u6236\u7c4d\u5730 \u8207 \u6210\u7e3e \u8cc7\u6599\n\n\n(2) \u958b\u65b0\u6a94\u6848 -> \u65b0\u589e -> \u7a7a\u767d\u6d3b\u9801\u7c3f\uff0c\u8cbc\u4e0a\u8cc7\u6599\uff0c\u7b2c\u4e00\u6b04\u70ba\u865f\u78bc\u3001\u7b2c\u4e8c\u6b04\u70ba\u5206\u6578\u3001\u7b2c\u4e09\u6b04\u70ba\u6236\u7c4d\u5730\u3002\n\n(3) \u8cbc\u4e0a\u6642\uff0c\u8a18\u5f97\u9078\u64c7\"\u8cbc\u4e0a\u7d14\u503c\"\uff0c\u82e5\u5206\u6578\u8d85\u904e\u5c0f\u6578\u9ede\u5169\u4f4d\u4e5f\u6c92\u95dc\u4fc2\uff0c\u4e0d\u7528\u523b\u610f\u7de8\u8f2f\u3002\n\n(4) \u53e6\u5b58\u65b0\u6a94\u6642\uff0c\u9078\u64c7 CSV(\u9017\u865f\u5206\u683c)\uff0c\u6a94\u540d\u5efa\u8b70\u547d\u540d\u6210 xxxT_input_file.csv\n\n(5) \u5b58\u6a94\uff0c\u9047\u5230\u8b66\u544a\u6642\u8acb\u6309\u78ba\u5b9a\n\n\n(6) \u95dc\u9589\u6a94\u6848\uff0c\u6b64\u6642\u9078\u64c7\u4e0d\u8981\u5132\u5b58(\u525b\u525b\u5df2\u7d93\u5132\u5b58\u904e\u4e86)\n\n\u7279\u6b8a\u60c5\u5f62\n\u72c0\u6cc1\uff1a\u5047\u8a2d\u67d0\u68af\u5f79\u7537\u5171 50 \u4eba\uff0c\u5176\u4e2d 7 \u865f\u540c\u5b78\u5728\u958b\u59cb\u53d7\u8a13\u5f8c\u624d\u9a57\u9000\u800c\u9000\u8a13\uff0c\u5269\u4e0b 49 \u4eba\u3002\n\u89e3\u6cd5\uff1a\u4e0d\u7528\u4fee\u6539\u7a0b\u5f0f\u7e3d\u4eba\u6578\uff0c\u4e00\u6a23\u5c07 7 \u865f\u540c\u5b78\u7684\u8cc7\u6599\u653e\u9032 csv \u4e2d\uff0c\u4e26\u5728\u5176\u6210\u7e3e\u6b04\u586b\u5165 NA \u3002\u5982\u6b64\u4e00\u4f86\uff0c\u672c\u7cfb\u7d71\u5c31\u4e0d\u6703\u5c07 7 \u865f\u540c\u5b78\u7b97\u5165\u5e73\u5747\u5206\u6578\u4ee5\u53ca\u4e4b\u5f8c\u7684\u9810\u6392\u6f14\u7b97\u6cd5\u4e2d\u3002\n\n\uff08\u4e09\uff09\u958b\u59cb\u9810\u6392\n\n\u958b\u555f Firefox \u700f\u89bd\u5668\uff0c\u82e5\u5c1a\u672a\u5b89\u88dd\u53ef\u4ee5\u7531\u6b64\u4e0b\u8f09\n\u5f9e github \u53f3\u65b9\u7684 download zip \u6309\u9215\u4e0b\u8f09\u672c\u7a0b\u5f0f\uff0c\u89e3\u58d3\u7e2e\u5f8c\u7528 firefox \u958b\u555f index.html \n\u9ede\u9078\u5de6\u4e0a\u89d2\u7684\u700f\u89bd\u6309\u9215\uff0c\u8b80\u53d6\u4e4b\u524d\u7684 csv \u6216\u662f txt \u6a94\u6848\u7576\u4f5c\u7bc4\u4f8b\uff0c\u4e26\u6309\u4e0b\u4e0b\u65b9\u7684\u300c\u958b\u59cb\u9810\u6392\u300d\u6309\u9215\n\n\n\u5c07\u756b\u9762\u5f80\u4e0b\u6372\uff0c\u4ee5\u6b64\u7bc4\u4f8b\u6a94\u6848\u5411\u5f79\u7537\u8b1b\u89e3\u672c\u7cfb\u7d71\uff1a\n\n\n\u78ba\u8a8d\u5e73\u5747\u5206\u6578\u662f\u5426\u6b63\u78ba\n\u8b1b\u89e3\u540d\u984d\u8207\u52a0\u6e1b\u4eba\u6578\u6b04\u4f4d\n\u8aaa\u660e\u4e0a\u65b9\u984f\u8272\u5340\u584a\uff08\u5206\u767c\u898f\u5247\u8aaa\u660e\uff09\n\u4e0b\u65b9\u5c1a\u672a\u5206\u914d\u5230\u7684\u5b78\u865f\u3001\u9ede\u9078\u53ef\u81ea\u52d5\u641c\u5c0b\u6700\u4f73\u843d\u9ede\n\n\n\n\u63a5\u8457\u8b1b\u89e3\u9810\u6392\u7684\u898f\u5247\uff1a\n\n\n\u88ab\u9ede\u5230\u865f\u78bc\u7684\u5f79\u7537\u8acb\u5927\u8072\u8aaa\u51fa\u81ea\u5df1\u7684\u5fd7\u9858\n\u7576\u5207\u63db\u5230\u81ea\u5df1\u6240\u5c6c\u7684\u9801\u9762\u6642\uff0c\u8acb\u6aa2\u67e5\u81ea\u5df1\u7684\u6236\u7c4d\u5730\u8207\u6210\u7e3e\u662f\u5426\u6b63\u78ba\n\u6bcf\u56de\u5408\u7d50\u675f\u5f8c\uff0c\u6703\u6709\u4e09\u5230\u4e94\u5206\u9418\u8a0e\u8ad6\u6642\u9593\n\u544a\u77e5\u9810\u6392\u622a\u6b62\u6642\u9593\uff08\u554f\u7ba1\u7406\u5e79\u90e8\uff09\n\u8a62\u554f\u662f\u5426\u6709\u4e0d\u61c2\u5206\u767c\u898f\u5247\u7684\uff0c\u6216\u6709\u5176\u4ed6\u7591\u554f\u7684\u8acb\u767c\u554f\n\n\n\u63a5\u4e0b\u4f86\u958b\u59cb\u6b63\u5f0f\u9810\u6392\uff1a\n\n\u91cd\u65b0\u6574\u7406\u9801\u9762\uff0c\u4e26\u8b80\u53d6\u9019\u68af\u7684 csv \u6216\u662f txt \u6a94 (e.g. 144T_input_file.csv)\n\u7b2c\u4e00\u8f2a\uff1a\u7167\u865f\u78bc\u53eb\uff0c\u9078\u64c7\u5fd7\u9858\u3002\u5bb6\u56e0\u512a\u5148\u8005\u82e5\u6236\u7c4d\u5730\u6c92\u958b\u7f3a\u984d\uff0c\u53ef\u512a\u5148\u9078\u64c7\u6236\u7c4d\u5730\u5468\u570d\u7e23\u5e02\n\u5b58\u64cb\u3001\u770b\u9810\u6392\u7d50\u679c\u3001\u8a0e\u8ad6\n\u7b2c\u4e8c\u8f2a\uff1a\u4e00\u5230\u5341\u865f\u60f3\u4fee\u6539\u5fd7\u9858\u7684\u8209\u624b\uff0c\u4f9d\u6b64\u985e\u63a8\n\u5b58\u64cb\u3001\u770b\u9810\u6392\u7d50\u679c\u3001\u8a0e\u8ad6\n\u7b2c\u4e09\u8f2a\u4e4b\u5f8c\uff0c\u60f3\u4fee\u6539\u7684\u76f4\u63a5\u8209\u624b\n\u7576\u5c1a\u672a\u5206\u914d\u5230\u7684\u4eba\u6578\u8d8a\u4f86\u8d8a\u5c11\u6642\uff0c\u76f4\u63a5\u9ede\u9078\u5176\u865f\u78bc\uff0c\u7cfb\u7d71\u6703\u63d0\u793a\u9019\u500b\u865f\u78bc\u9084\u5269\u54ea\u4e9b\u5730\u65b9\u53ef\u4ee5\u9078\u64c7\n\n\u8acb\u6bcf\u6b21\u6309\u4e0b\u9810\u6392\u6309\u9215\u5f8c \u90fd\u5148\u5b58\u64cb\uff08txt\u6a94\uff09\uff0c\u4ee5\u907f\u514d\u7a81\u767c\u72c0\u6cc1\u9020\u6210\u8cc7\u6599\u6d41\u5931\u3001\u5f71\u97ff\u9810\u6392\u6642\u9593\u3002\n\u9810\u6392\u7d50\u675f\u5f8c\uff0c\u53ef\u8b93\u5f79\u7537\u5011\u62cd\u7167\u7559\u5ff5\uff0c\u4e5f\u53ef\u622a\u5716\u5099\u4efd\u3002\n\u9810\u6392\u7d50\u675f\u5f8c\uff0c\u6309\u4e0b\u53f3\u4e0b\u89d2\u8f38\u51fa\u8868\u683c\u7d66\u6559\u52d9\u7d44\u3002\n\u5176\u4ed6\u8a2d\u5b9a\njs\/global.js \u5167\u7684 printRound_N \u53ef\u4ee5\u8a2d\u5b9a\u9810\u6392\u7d50\u679c\u7684\u300c\u7e23\u5e02\u6b04\u4f4d\u300d\u986f\u793a\u5e7e\u7b46\u8cc7\u6599\u5f8c\u624d\u63db\u884c\nprintRound_N = 3; \/\/ \u6bcf\u4e09\u7b46\u8cc7\u6599\u5c31\u63db\u884c\uff0c\u5982\u4e0b\u5716\uff1a\n\n\nprintRound_N = 6; \/\/ \u6bcf\u516d\u7b46\u8cc7\u6599\u624d\u63db\u884c\uff0c\u5982\u4e0b\u5716\uff1a\n\n\n\njs\/global.js \u5167\u7684 fontColors{} \u53ef\u4ee5\u6539\u8b8a\u4e0d\u540c\u968e\u6bb5\u9304\u53d6\u7684\u984f\u8272\n\u4f7f\u7528\u4e0d\u540c\u984f\u8272\u4f86\u4ee3\u8868 \u4e0d\u540c\u968e\u6bb5\u9304\u53d6 \u7684\u5f79\u7537\nfontColors = {\n  type1 : \"black\", \/\/ \u7b2c\u4e00\u968e\u6bb5\u9304\u53d6\uff08\u5206\u6578\u5927\u65bc\u5747\u6a19\uff0c\u6236\u7c4d\u5730\uff09\n  type2 : \"#229922\", \/\/ \u7b2c\u4e8c\u968e\u6bb5\u9304\u53d6\uff08\u5206\u6578\u5927\u65bc\u5747\u6a19\uff0c\u975e\u6236\u7c4d\u5730\uff09\n  type3 : \"#0000dd\", \/\/ \u7b2c\u4e09\u968e\u6bb5\u9304\u53d6\uff08\u5206\u6578\u4f4e\u65bc\u5747\u6a19\uff0c\u6236\u7c4d\u5730\uff09\n  type4 : \"#4488ff\", \/\/ \u7b2c\u56db\u968e\u6bb5\u9304\u53d6\uff08\u5206\u6578\u4f4e\u65bc\u5747\u6a19\uff0c\u975e\u6236\u7c4d\u5730\uff09\n  \/\/ typeDefault : \"black\", \/\/ \u9810\u8a2d\u984f\u8272\n  typeHome : \"orange\", \/\/ \u5bb6\u56e0\u984f\u8272\n  typeKicked : \"red\", \/\/ \u9078\u67d0\u500b\u5730\u5340\u6642\uff0c\u88ab\u64e0\u6389\u7684\u4eba\u7684\u984f\u8272\n  leftOver : \"red\", \/\/ \u672c\u56de\u5408\u7d50\u675f\u5f8c\uff0c\u5c1a\u672a\u5206\u914d\u5230\u670d\u52e4\u55ae\u4f4d\u7684\u984f\u8272\n  shortage : \"blue\", \/\/ \u5730\u5340\u4eba\u6578\u77ed\u7f3a\u6642\u7684\u984f\u8272\n  overheat : \"red\" \/\/ \u5730\u5340\u4eba\u6578\u904e\u591a\u6642\u7684\u984f\u8272\n};\n\nTodo Lists\n\n\u53f0\u5317\u5e02 \u8ddf \u81fa\u5317\u5e02 \u5230\u5e95\u8981\u7528\u54ea\u500b\u5b57\uff1f\n123456789123\n\u88dd firefox, notepad++, D\u789f\u4e0d\u6703\u81ea\u52d5\u56de\u5fa9\nChrome modification.\n\nhttps:\/\/stackoverflow.com\/questions\/2541949\/problems-with-jquery-getjson-using-local-files-in-chrome\nMAC: open \/Applications\/Google\\ Chrome.app --args --allow-file-access-from-files\nhttp:\/\/eureka.ykyuen.info\/2013\/09\/24\/chrome-bypass-access-control-allow-origin-on-local-file-system\/\n\n\n\nLicense\nThis project is licensed under the terms of the MIT license.\nPlease note that this project is built with materials from the following parties:\n\nBootstrap\nflatly\njQuery\n\nPlease also refer to their Licenses for further information.\n","120":"sensor-puck\nThis is the repository for all collateral related to the Silicon Labs Sensor Puck.\nNote. The Sensor Puck is no longer actively supported and this code is provided as is.\nThe android directory has the Sensor Puck android app source code.\nThe iOS directory has the iPhone app source code.\n","121":"MoodCube\n3D lattice of RGB LEDs driven by ANN with environmental sensors as inputs\nExisting Things\n\nProgrammable cube ($390) with Mic and Acc:  http:\/\/cubetube.org\/\nhttp:\/\/www.instructables.com\/id\/8X8X8-RGB-LED-Cube\/\nNeoPixel (https:\/\/learn.adafruit.com\/adafruit-neopixel-uberguide\/overview) single wire, RGB\nAudio Spectrum Analyzer with Pi\n\nSignal Flow\n\nSomething acquires a sample each from many sensors\nThese samples are passed to a Neural Network which does some nonlinear processing on the vector of input time series.\nThe outputs of the NN are passed to a an output processor, which takes the outputs and writes them to the LEDs.\n\n\nLED Drive\n\nThe NeoPixel or DotStar style of Arduino \/ Raspberry Pi compatible LEDs are a single strip of addressable RGB LEDs.\n\nthere are 3rd party products also like HKBAYI\n\n\nThe FadeCandy board takes a USB input and can drive 8 strips having 64 LEDs each. That's a total of 8x64 = 512 LEDs.\n\ncould do a cube with 4 sides + 1 top. 10x10 LEDs per side = 500 total.\n\n\nThe LED strips can be mounted on some clear plastic rods so as to make the shape into something like a cube.\n\nuse a 3D printer to make some wild shapes to mount it on: trees, spheres, Japanese lantern, Klein bottle\nmaybe hang them from a frame like Hanging Gardens or the living trees in Avatar\n\n\nneeds ~60 mA per LED for full power. Should use a 5V, 10A AC\/DC adapter and a power bus to spread power to each strip.\n\n\n\n\nGitHub Markdown: https:\/\/guides.github.com\/features\/mastering-markdown\/\n\nLearning\n\nstochastic gradient method with least mean squares\n\nsynapse sensors (sources)\n\naudio                     data=((CHUNK,), int16), fs=1\/CHUNK, (-16k, 16k)\naudio_blrms:chunk,bands   data=((bands,), int16), fs=1\/chunk, (-16k, 16k)\nproximity:fs              data=((bands,), int16), fs=fs,      (0, 400)\ndate:fs                   data=(dt.weekday(), dt.hour, dt.minute, dt.second), fs=fs\n\nsynapse auto-start with systemd\nBoth the fcserver and synapse processes will auto-start using the \"pi\"\nuser system --user session:\n\n\/home\/pi\/.config\/systemd\/user\/fcserver.service\n\/home\/pi\/.config\/systemd\/user\/moodcube.service\n\nThe services are called \"fcserver\" and \"moodcube\".  They launch the\nfollowing scripts:\n\nfcserver: \/home\/pi\/GIT\/MoodCube\/FadeCandy\/fcserver_launch.sh\nmoodcube: \/home\/pi\/GIT\/MoodCube\/launch\n\nYou can control the processes using the \"systemctl --user\" command.  *\nshow service status:\n$ systemctl --user status moodcube\n\n\nreload configuration (only needed if you change the config file in\n~\/.config\/systemd\/user\":\n$ systemctl --user daemon-reload\n\n\nrestart service:\n$ systemctl --user restart moodcube\n\n\nstop service:\n$ systemctl --user stop moodcube\n\n\nfollow logs:\n$ journalctl -f\n$ journalctl -f -o cat   (more terse)\n\n\n","122":"s_environmentalProtectionBigData\n\u73af\u4fdd\u5927\u6570\u636e\u5e73\u53f0\n","123":"Streaming Data from ESP8266 using MicroPython and MQTT\nResources\n\nMicroPython\nMicroPython Online Simulator\nMicroPython libraries\nMicroPython PiP Package Index\nESP8266 Documentation\nESP8266 Firmware\nESP8266 Firmware Tutorial\nAdafruit MicroPython Resources\n\nOther\n\nMicroPython BME280 Sensor Driver\nMicroPython SHT30 Sensor Driver\nEspressif esptool\nMosquitto Broker\nPuTTY\nSerial Support on the Windows Subsystem for Linux\n\nOverview of ESP8266 flashing process\n\nInstall esptool\nErase Flash\nDeploy MicroPython Firmware\n\nSee Flashing MicroPython Flasing How-to Tutorial\nMicroPython and Publish over MQTT\nDeploy the solution to the ESP8266 MicroPython flash as follows.\nSee Adafruit MicroPython Tool (ampy) for information on copying files to the ESP32.\npip install adafruit-ampy\n\nampy --port \/dev\/ttyUSB0 put boot.py\nampy --port \/dev\/ttyUSB0 put main.py\nampy --port \/dev\/ttyUSB0 put config.py\n\nampy --port \/dev\/ttyUSB0 put bme280.py\nampy --port \/dev\/ttyUSB0 put sht30.py\n\nampy --port \/dev\/ttyUSB0 put config_default.json\n\nampy --port \/dev\/ttyUSB0 put sensor_bme280.py\nampy --port \/dev\/ttyUSB0 put sensor_sht30.py\nampy --port \/dev\/ttyUSB0 put sensor_fake.py\n# boot.py\n\nimport network\nsta_if = network.WLAN(network.STA_IF)\nsta_if.active(True)\nsta_if.connect(\"Wifi SSID\", \"Wifi password\")\n# main.py\n\n# http:\/\/docs.micropython.org\/en\/latest\/esp8266\/index.html\n# http:\/\/garybake.com\/wemos-oled-shield.html\n# http:\/\/docs.micropython.org\/en\/latest\/esp8266\/esp8266\/quickref.html#adc-analog-to-digital-conversion\n\nfrom umqtt.robust import MQTTClient\nfrom machine import I2C, Pin\nimport utime as time\nimport gc\nimport ssd1306 as oled\nfrom machine import ADC\nimport esp\nimport config\n\n\n# Wifi connect established in the boot.py file. Uncomment if needed\n# import network\n# sta_if = network.WLAN(network.STA_IF)\n# sta_if.active(True)\n\n\n#upip packages - see README.md\n# upip.install('micropython-umqtt.simple')\n# upip.install('micropython-umqtt.robust')\n\nBuiltinLedPin = 2\n\ncfg = config.Config('config_default.json')\n\nsta_if.connect(cfg.wifiSsid, cfg.wifiPwd)\nclient = MQTTClient(str(esp.flash_id()), cfg.mqttBroker)\nmySensor = cfg.sensor.Sensor()\n\nbuiltinLed = Pin(BuiltinLedPin, Pin.OUT)\nadc = ADC(0)            # create ADC object on ADC pin\ni2c = I2C(scl=Pin(5), sda=Pin(4)) \n\nesp.sleep_type(esp.SLEEP_LIGHT)\n\n\ndef initDisplay(i2c):\n    i2cDevices = I2C.scan(i2c)\n    if 0x3c in i2cDevices:\n        display = oled.SSD1306_I2C(64, 48, i2c)\n        return True\n    else:        \n        print('No OLED Display found')\n        return False\n\ndef initialise():\n    blinkcnt = 0\n    checkwifi()\n    while blinkcnt < 50:\n        builtinLed.value(blinkcnt % 2)\n        blinkcnt = blinkcnt + 1\n        time.sleep_ms(100)\n\ndef checkwifi():\n    blinkcnt = 0\n    while not sta_if.isconnected():\n        time.sleep_ms(500)\n        builtinLed.value(blinkcnt % 2)\n        blinkcnt = blinkcnt + 1\n\ndef setContrast():\n    lightlevel = adc.read()\n    if lightlevel < 200:\n        return 0\n    if lightlevel < 600:\n        return 100\n    return 255\n\ndef publish():\n    count = 1\n    while True:\n        builtinLed.value(0)\n        checkwifi()\n        \n        temperature, pressure, humidity = mySensor.measure()\n\n        freeMemory = gc.mem_free()\n\n        if oledDisplay:\n            display.fill(0)\n            display.contrast(setContrast())\n            display.text(v[0], 0, 0)\n            display.text(v[1], 0, 10)\n            display.text(v[2], 0, 20)\n            display.text(str(freeMemory), 0, 30)\n            display.text(str(count), 0, 40)\n            display.show()    \n\n        msg = b'{\"DeviceId\":\"%s\",MsgId\":%u,\"Mem\":%u,\"Celsius\":%s,\"Pressure\":%s,\"Humidity\":%s}' % (cfg.deviceId, count, freeMemory, temperature, pressure, humidity)\n        client.publish(b\"home\/weather\/%s\" % cfg.deviceId, msg)\n\n        builtinLed.value(1)\n        count = count + 1\n        \n        time.sleep(cfg.sampleRate)\n\n\noledDisplay = initDisplay(i2c)\n\ninitialise()\n\nclient.reconnect()\n\npublish()\nConnecting to ESP32 with Putty\nInstall Putty for your platform. Connect at 115200 baud rate\nSee Adafruit Serial REPL Tutorial.\n","124":"Python Audio Feature Extraction\nThis repository holds a library of implementations of a few separate utilities to be used for the extraction and processing of features from audio files. The underlying extraction library is librosa, which offers the ability to extract a variety of spectral features as well as a few other miscellaneous features.\nProject Goals\n\nTo learn more about feature extraction from audio files\nTo standardize parameters for use during extraction on large amounts of audio samples\nTo allow for a relatively easy interface to select and extract subsets of parameters from subsets of samples\nTo provide Pandas wrappings of extraction results to hold important metadata and information about the extraction process\n\nCurrent Implementations\n\nAudioFeatureExtractor: this class defines an object that can be used to standardize a set of parameters to be used during feature extraction. It provides wrapper methods to librosa functions and can handle preprocessing steps such as preemphasis filtering and hard low and high cutoffs to facilitate data cleaning.\nBatchExtractor: this class defines an object that holds information about a batch of audio samples for which feature extraction should be performed. It implements methods that handle batch extraction using a set of standardized settings and easy selection of desired features.\nFeatureVisualizer: this class defines an object that can handle the visualization of features through Matplotlib.\n\n\nUsage Documentation\nAudioFeatureExtractor\nEach class can be imported from the afe module. In other words, to import the AudioFeatureExtractor object, simply put from afe import AudioFeatureExtractor along with the rest of your needed imports. Then, you can instantiate an AudioFeatureExtractor object and put it to work. This object takes as parameters at instantiation:\n\nThe desired sample rate in Hz to use for loading and analysis of audio (default 22050)\nThe desired number of samples to use as the window length for framed computations and feature extractions. This number should be set to an integer power of 2 to optimize the Fourier engine (default 1024)\nThe desired ratio of a window length to hop during framed computations -- i.e. an overlapping factor, setting this to 4 implies that the frame jumps 1\/4 of a window length during framed computations (default 4).\n\nAn AudioFeatureExtractor:\n\nhas the capability of retrieving audio from a string file path at the instance's sample rate and loading it as a Numpy array\ncan detect onsets, perform preemphasis filtering, and bandpass filtering for noise removal in the low and high regions\ncan extract a feature using the instance's standardized framing attributes, either from a Numpy array of audio samples or from a preprocessed STFT\/CQT (such as with bandpass noise removal)\n\nCurrently the following feature extraction methods are implemented (all feature extraction methods begin with the prefix extract_:\n\nextract_stft: extracts a short time Fourier transform\nextract_cqt: extracts a constant-Q transform\nextract_chroma_stft: extracts a chromagram\nextract_chroma_cqt: extracts a chromagram of a CQT\nextract_chroma_cens: extracts an energy normalized variant chromagram\nextract_melspectrogram: extracts a Mel-windowed spectrogram\nextract_mfcc: extracts the Mel-frequency cepstral coefficients\nextract_rms: extracts the framed root-mean-square\nextract_spectral_centroid: extracts the spectral centroid\nextract_spectral_bandwidth: extracts the spectral bandwidth\nextract_spectral_contrast: extracts the spectral contrast\nextract_spectral_flatness: extracts the spectral flatness\nextract_spectral_rolloff: extracts the spectral rolloff\nextract_zero_crossing_rate: extracts the framed zero crossing rate\nextract_tonnetz: extracts the tonnetz (tonal centroid)\nextract_poly_features: extracts polynomial combinations of features from a given feature matrix or audio\n\nUltimately I would like to add these functionalities to the AudioFeatureExtractor:\n\nTempo related feature extraction methods\nFeature manipulation tools offered by librosa\nFeature inversion tools to translate back from the feature space to the auditory space, to hopefully facilitate some interesting generative projects later on\nPerhaps incorporating some other utilities could be helpful for later projects or methods of feature engineering.\n\nBatchExtractor\nThe BatchExtractor is somewhat of an extension of the AudioFeatureExtractor to handle standardized extraction of a batch of samples. In order to use the BatchExtractor object, we must import it in the same way: from afe import BatchExtractor. Then we can instantiate a BatchExtractor object. This object accepts at instantiation the following parameters:\n\nThe same three parameters as accepted during instantiation of an AudioFeatureExtractor object\nA string path of a folder containing audio samples\nEither a Pandas dataframe or a string path to a CSV file that can be read as a dataframe which has metadata information about samples in the specified folder of audio. Details about the formatting of this index dataframe are discussed more in-depth below\nThe number of Mel-frequency cepstral coefficients to compute (default 12)\nA boolean flag indicating whether or not preemphasis filtering should be applied to audio before computation of features (default False)\nA float between 0 and 1 indicating the desired preemphasis filter coefficient if this option is desired and appropriately set using the Boolean flag (default 0.97)\nA boolean flag indicating whether or not hard-bandpass filtering of noise in the higher and lower frequencies should be applied (default False)\nIf hard-bandpass noise filtering should be applied and the flag has been appropriately set, then integer values can be set for the upper and lower limits of this noise filtering (default None)\nA boolean flag indicating whether or not the start of the audio samples should be trimmed to the first computed onset (default False)\n\nThe BatchExtractor, as specified, requires that an index dataframe be specified, either as a string pointing to a CSV file or by passing in the dataframe itself. In particular, this dataframe needs a column named file_name which indicates for each audio sample to be analyzed the file path as a string and relative to the BatchExtractor's stored audio folder path. Other columns present in the index dataframe could depend on the context of the project.\nCurrently, the BatchExtractor can be used to perform the following tasks:\n\nThere are methods available to set any of the preprocessing options:\n\nset_bp_filter: sets the bandpass noise filtering flag and parameters\nset_preemphasis: sets the preemphasis flag and parameters\nset_trim: sets the flag for trimming to first onset\n\n\nThere are methods to extract and merge features from the batch of samples stored in the instance's index of the folder of audio. Each of these methods accepts a string indicating a results_folder in which to either save the extracted feature matrices as CSV files or look for the saved extraction results as CSV files (in the case of merging). Additional options for each method are further detailed below.\n\nbatch_extract_feature: accepts a string abbreviation of a single extraction method to apply to the entire batch of audio in the index. Saves the results to the given results_folder.\nbatch_extract_features: accepts a list of string abbreviations of extraction methods to apply to the entire batch of audio in the index. Saves the results to the given results_folder.\nmerge_features: accepts a list of string abbreviations of features to merge into a single dataframe. In other words, for each sample in the audio index, each of the feature matrices specified in the list of abbreviations will be loaded from the results_folder and merged, then saved as a new dataframe.\nbatch_extract_and_merge: Performs a batch extraction then a merging of all features given in the list of string abbreviations for all audio samples in the index.\nmerge_and_flatten_features: The only of these methods to have a non-null return, this method loads for each sample in the audio index the feature matrices for the given list of extraction abbreviations, flattens them into a single row (creating many many columns), then concatenates all these rows into an appropriately padded dataframe containing feature information for all samples in the index.\n\n\n\nI would eventually like to add the following to the BatchExtractor implementation\nFeatureVisualizer\nThe FeatureVisualizer class defines an object which could be helpful for the purposes of debugging or identifying import sonic features. In general, computations such as the STFT\/spectrogram are helpful because they can give us a very good visual description of what is happening in audio, even though their quantities are also helpful by making more specific quantifications for the purposes of analysis. The FeatureVisualizer class is my attempt at creating an interface for the visualization of features extracted using the BatchExtractor (or similarly AudioFeatureExtractor) object. At instantiation, this object accepts the following parameters:\n\nA string path indicating a folder containing extracted features with the following naming convention: '{sample_name}_{feature_abbreviation}_features.csv'\nA default figure size to use when plotting (default (18, 8)).\n\nThis is currently the class for which the least implementation has been written. Currently the object is capable of visualizing by simply calling the name of the sample to be loaded from the folder of extracted features.\n\nSTFT\/spectrograms\nMel-windowed spectrograms\nChromagrams\n\nThere are many more feature visualization methods that I would like to implement (CQTs, spectral bandwidth and related features, tempograms eventually).\n","125":"d3-EnvironmentalHealthScores\nSelect Environmental Health Scores displayed as a d3 map and graph\n","126":"SnapGreen | Cal Poly Software Engineering I & II 2020\n\n\n\n\nProviding users with a measurable awareness of their environmental impact. Users can track daily habits and scan products to increase and improve their environmental score.\nUnderstanding Our Backend\nCoding Style\nKotlin Style Convention\nJavascript Style Convention: Prettier\nStatic Code Analysis\nView SonarCloud Project Dashboard\nContinuous Integration Software\nView Travis CI Dashboard\nDesign Diagrams and Prototypes\n\nView Diagrams Here\n\nUI Prototypes\nView Figma Mockup\nComponent Diagram\n\nUse Case Diagram\nThe app involves two actors, one being a player(user) and the other being a clock. The player can perform various activities. They can login to the app and if they don't have an account they can create a new account. A player can also access their settings, add friends, enter usage stats which will also lead to the system to calculate the stats. They can create a new game, which in turn will start the game clock countdown and this is managed by the clock. Finally the player can scan the product barcode and in return view the environmental impact of it.\n\nActivity Diagram\nThis diagram displays the process of creating, playing, and ending a game. Reading from top to bottom you can see the different decisions at each step and what happens after the user makes a decision on whether or not to perform a certain action. The diagram is pretty self-explanatory and easy to follow.\n\nThis diagram shows the basic workflow when adding usage data into SnapGreen. Several different statistics are updated including any games in progress.\n\nClass Diagram\nThis diagram is a rough draft that shows the interaction between the different main classes of the game. It also shows the different methods that perform the various actions within the app. It also highlights the dependencies between one class and another.\n\nSequence Diagram\nThis diagram shows the interaction between the app, server, and database when a user tries to login. The app sends the login attempt information to the server and the server queries the database and recieves a response. The server then sends a response to the app based on whether the login attempt was valid, whether the user doesn't exist, or whether the password doesn't match.\n\n\nTesting\nOur team is utilizing Espresso and JUnit for Kotlin and Jest for JS testing.\nView Code Coverage\nAcceptance Tests:\nView Acceptance Test Specification\nView Acceptance Test Code\nUnit \/ Integration Tests:\nView Android Unit \/ Integration Test Code\nView Backend Unit Test Code\nSetting up the Developer Environment\nOur app utilizes Android Studio for front-end development and Node.js for the backend. View both\n\nAndroid Studio Setup\n\nInstallation:\n\nInstall latest version of [Android Studio](https:\/\/developer.android.com\/studio).\nNavigate to Tools > SDK Manager\nDownload and Install Android 9.0 (Pie)\nNavigate to SDK Tools\nDownload and Install Google Play Services\nClone the repository\nGo to File > Open and select the \"Client\" folder from the repository.\nWait for import and gradle sync to complete.\nIf prompted, download and install the latest versions of both gradle and kotlin (may not be neccessary)\nDownload the GoogleServices.json from the Firebase console.\nPlace the JSON in the \"app\" directory\nConnect an android phone with developer mode activated and USB debugging turned on\nOR Navigate to Tools > AVD Manager\nSelect Create Virtual Device\nSelect Pixel 3 > Pie > Finish\nClick on the play button on the top of Android Studio to build and run the app!\n\n\n\nNode Server Setup\n\nInstallation:\nWindows\nWe recommend enabling WSL (Windows Subsystem for Linux) first--while it is\npossible to install these programs on Windows without doing so, the server will\nultimately be hosted in a Linux environment and therefore will be expressed with\nLinux commands. When choosing a \"flavor\" of Linux to install, choose \"Ubuntu\n18.04 LTS\" from the Microsoft store--certain commands vary depending on which\nvariety of Linux you choose, and for this we are going with Ubuntu (for now).\nHow to install\/enable WSL on Windows 10\nOnce WSL has been installed\/enabled, you can start it by going to any folder in\nexplorer, clicking in the box showing your location (e.g. \"This PC > Local Disk\n(C:) > Users...,\" just above the folder contents), then typing \"wsl\" and hitting\nenter. That will put you into the Linux command line.\nLinux\nInstalling node.js (from the command line)\n\nsudo apt update\n\n\nsudo apt install nodejs\n\n\nnodejs -v\n\nInstalling npm (node package manager)\n\nsudo apt install npm\n\n\nnpm -v\n\nInstalling express.js\n\nnpm install express\n\nMac\nFirst, you need to install XCode (from the Apple App Store), and Homebrew\n(Apple's package manager for Mac). All following commands should be entered\ninto the terminal:\nInstalling Homebrew\n\nruby -e \"$(curl -fsSl https:\/\/raw.githubusercontent.com\/Homebrew\/install\/master\/install)\"\n\nInstalling node and npm\n\nbrew install node\n\n\nnode -v\n\n\nnpm -v\n\nupdating node and npm\n\nbrew upgrade\n\n\nbrew upgrade node\n\n\nnode -v\n\n\nnpm -v\n\nUninstall:\nWindows\nIf you've installed via WSL, follow the Linux instructions below from the linux\ncommand line. Otherwise, uninstall programs as you normally would.\nLinux\n\nsudo apt remove nodejs\n\n\nsudo apt purge nodejs\n\n\nsudo apt autoremove\n\nMac\n\nbrew uninstall node\n\nSome Important Dev Dependencies\nhusky: Allows for pre-commits hooks (Used to run prettier styling for every JS commit)\njest: Testing framework for JS\nnodemon: Utilized to have the server refresh automatically with every change\nprettier: Automatic code formatting for every JS commit\nRunning the Server\nRunning the server is as simple as two commands\n\nnpm install\n\n\nnpm run serve\n\nYou may get a warning from your firewall--go ahead and let it slide.\nYour terminal should announce that the server is running. Open up a browser\nwindow, and go to \"localhost:8080\". You should see a blank page with a button\nat the bottom; you should also see a message in the terminal that states \"user\nconnected.\" Try clicking on the button--you will see repeated messages.\n\n","127":"TracerLPM\nAn Excel workbook program for evaluating groundwater age distributions from environmental tracer data\nThis folder contains the installation packages (.msi) for 32- and 64-bit versions of TracerLPM. The installation package must macth the version of Microsoft Office installed on a user's machine. By default, the 32-bit version of Office is normally installed even though most operating systems are 64-bit. Only use the 64-bit installation package of TracerLPM if the version of Office is 64-bit.\nSubfolders contain all the code and workbooks used to develop TracerLPM. The Workbook folder contains the latest version of the Excel workbook and is written in visual basic for applications (VBA). The folder LPM_FunctionsXLL contains Visual Studio (2015) solution code written in C++ that computes tracer concentrations. The folder OptimizationXLL contains Visual Studio (2015) solution code written in C++ that computes best-fit lumped parameter models to environmental tracer data. The Visual Studio codes generate XLLs (dynamic linked library recognized by Excel) that are installed in the Microsoft Add-in folder on a user's machine. The XLL have worksheet functions that can be called from any worksheet cell in TracerLPM. TracerLPM will pass data to the XLL fitting routines and generate output from the results.\n","128":"GDAL-Point-Extraction-for-Environmental-Raster-Data\nThe following code is for pulling the pixel data by lat\/long coordinates from environmental raster datasets provided by the USGS: 1) annual maximum green vegetation fraction derived NDVI (normalized difference vegetation index) satellite data; and 2) land cover type (Water, Mixed Forest, Grasslands, Urban & Built Up, etc.) Understanding the geographic distribution of vegetation and environmental resources is crucial for planning and sustainable design strategies.\nThe code pulls data from the following .tif maps: 1 km MODIS-based Maximum Green Vegetation Fraction (http:\/\/landcover.usgs.gov\/green_veg.php) and 0.5 km MODIS-based Global Land Cover Climatology (http:\/\/landcover.usgs.gov\/global_climatology.php).\nNOTE: Make sure to set the GDAL_DATA environment data. Since I installed GDAL with Anacanda, before executing the python file from terminal I ran:\nexport GDAL_DATA=\/austinarrington\/anaconda2\/share\/gdal\n","129":"\nLake levels from the Yahara Watershed\nhttp:\/\/www.yahara.info\nMadison Wisconsin and the surrounding Dane County saw near record level rainfalls in late August. Widespread flooding caused over two hundred million dollars in damage (Associated Press).\nIn the months leading up to the flood, the lakes surrounding Madison were higher than the maximum level set by the Wisconsin Department of Natural Resources in 1979.\nHow often has that been true? Are the lakes currently above that maximum level? Why were they kept so high? All of these questions and more we hope to address.\nEnvironment setup\nInstall postgresql.\nMake sure you have an environment variable named USER with your username as its value. On UNIX variants this likely already exists\nCreate a new user with your username who has permissions to create a database, e.g. with sudo -u postgres createuser -s $USER.\nIt is recommended to set up some sort of virtual environment. After that, install the requirements with\npip install -r requirements.txt\nCode\nThe code is in madison_lake_levels, and requires python >= 3.6. You can run tests with python -m pytest run from the top level of this project.\nRunning locally\nRun with\n# Get env set up\nexport FLASK_APP=app.py\nexport DATABASE_URL=postgres:\/\/$USER:@:\/madisonlakes # heroku's env var format\ncreatedb madisonlakes -U $USER\n# Run the app\nflask run\n# To run in debug mode (don't do in prod!):\nexport FLASK_ENV=development\nflask run\n\nDeploy\nThe webapp is deployed to Heroku. It can be found at http:\/\/www.yahara.info. The heroku-format Procfile and runtime.txt are used to control deployment.\nA free-tier of a database on Heroku is used to persist the data.\nA cron job runs every 30 minutes that updates the database. The job is created using Heroku Scheduler, and hits a simple API route on the web-app that causes an update. Running the job every 30 minutes has a nice side effect of preventing the website from going into hibernation mode (which Heroku does on the free tier).\n","130":"Deprecated\nThis version of epp is deprected, please use the version found at\nhttps:\/\/github.com\/blendle\/epp.\n","131":"SETENV\nThis project helps manage project-specific environmental variables: There when\nyou want 'em, gone when you don't.\nUsage\n\n\nClone this repository and copy the setenv.sh file to the root of your project.\n\n\nIf installing for the first time - PERFORM INITIAL SETUP OF THE VIRTUAL ENVIRONMENT (by issuing virtualenv venv)\n\n\nCreate a file named .env and list your project's environmental variables,\ne.g., DATABASE=\/path\/to\/database.db\n\n\nIssue: source setenv.sh to set up environment variables AND start the virutalenv. If the venv\nand environment variables are correctly setup a [VENV+] indicator will appear on the left\nhand side of the prompt in bold purple. (Issuing pip -V should also show the directory of the\nvirtual environment's python interpreter.)\n\n\nIssue: usetenv to exit the virtual environment AND unset all environment variables.\n\n\nNOTES:\n\nThis will completely override the builtin deactivate command.\nThe default venv scripts are not modified; so the canned venv setup script will still work out of\nthe box if you chose to use it. However, if you launch setenv (using source setenv.sh), any\ninteraction with the canned venv scripts will be disabled.\n\n","132":"epidemiar\nThe Epidemic Prognosis Incorporating Disease and Environmental Monitoring for Integrated Assessment (EPIDEMIA) Forecasting System is a set of tools coded in free, open-access software, that integrate surveillance and environmental data to model and create short-term forecasts for environmentally-mediated diseases.\nThis R package contains the functions for modeling, forecasting, validation, and early detection & early warning alerts.\nFor producing formatted reports, see also the demo project based on malaria in Ethiopia (with demo data): https:\/\/github.com\/EcoGRAPH\/epidemiar-demo\nEPIDEMIA project: http:\/\/ecograph.net\/epidemia\/\n","133":"ResonantEco\nDevelop environment setup\nResonantEco has server, client two components. They are located under server and client directory respectively.\nPrerequisite\n\nPyhton 3.5+\nMongodb running at the default port\nNode 8\nClone this repo git clone https:\/\/github.com\/OpenDataAnalytics\/resonanteco.git\n\nServer\n\npip install -e .\/resonanteco\/server\/\ngirder build\nThe girder interface will be available at http:\/\/localhost:8080\/girder\n\nClient\nThe client is a Vue CLI based application. All Vue-CLI options are available.\n\ncd .\/resonanteco\/client\nnpm install\nnpm run serve\nNavigate to localhost:8081\nRegister user then login the system\n\nlinting\n\nnpm run lint\n\nData ingestion\n\nIn the same python envrionment as the server\nNavigate to server\/data\nexport GIRDER_MONGO_URI=mongodb:\/\/localhost:27017\/girder-resonanteco if you db is not the default girder\nexecute python ingest.py .\/data\n\n","134":"trafficsignrecognize\n\u0110\u1ed3 \u00e1n nh\u1eadn di\u1ec7n bi\u1ec3n b\u00e1o c\u1ea5m \u0111\u01a1n trong \u1ea3nh m\u00f4i tr\u01b0\u1eddng c\u00f3 s\u1eed d\u1ee5ng deep learning.\nY\u00eau c\u1ea7u:\nPython:\n\npython 3.6.9\n\n\nPackages:\n*Khuy\u1ebfn kh\u00edch s\u1eed d\u1ee5ng Anaconda 3 t\u1ea1o m\u1ed9t environment m\u1edbi t\u00ean \"opencv\" \u0111\u1ec3 c\u00e0i t\u1ea5t c\u1ea3 packages nh\u01b0 h\u00ecnh\n\n\n\nnumpy 1.17.2\nmatplotlib 3.1.1\nopencv 3.4.2\ndjango 2.2.5\nscikit-image 0.15.0\ntensorflow 2.0.0\ntensorflow-mkl 1.15.0\nkeras 2.2.4\npillow 6.2.1\n\nRun project:\nActivate bi\u1ebfn m\u00f4i tr\u01b0\u1eddng Anaconda 3\nsource ospath\/anaconda3\/anaconda3\/bin\/activate\nActivate m\u00f4i tr\u01b0\u1eddng ch\u1ee9a c\u00e1c packages c\u1ea7n thi\u1ebft\nconda activate opencv\nDi chuy\u1ec3n \u0111\u1ebfn th\u01b0 m\u1ee5c ch\u1ee9a project\ncd parentProjectPath\/traffic_sign_recognize-master \nCh\u1ea1y server\npython manage.py runserver\nSau khi ch\u1ea1y server th\u00e0nh c\u00f4ng truy c\u1eadp \u0111\u1ecba ch\u1ec9 localhost:8000 \u0111\u1ec3 thao t\u00e1c.\n\nH\u1ed7 tr\u1ee3 c\u00e1c bi\u1ec3n b\u00e1o: (Theo b\u1ed9 bi\u1ec3n b\u00e1o chu\u1ea9n Vi\u1ec7t Nam)\n\n101: \u0110\u01b0\u1eddng c\u1ea5m\n102: C\u1ea5m \u0111i ng\u01b0\u1ee3c chi\u1ec1u\n122: D\u1eebng l\u1ea1i\n127: T\u1ed1c \u0111\u1ed9 t\u1ed1i \u0111a cho ph\u00e9p\n\nTham kh\u1ea3o source code train file model.h5 t\u1ea1i github.com\/quangkhoiuit98\/trainmodeltrafficsignrecognize\n\n\nCh\u1ee9c n\u0103ng ch\u00ednh\nTrang ch\u1ee7\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Nh\u1eadn di\u1ec7n bi\u1ec3n b\u00e1o t\u1eeb \u1ea3nh m\u00f4i tr\u01b0\u1eddng\n\nTrang tra c\u1ee9u bi\u1ec3n b\u00e1o\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Tra c\u1ee9u bi\u1ec3n b\u00e1o t\u1eeb d\u1eef li\u1ec7u c\u1ee7a \u1ee9ng d\u1ee5ng\n\n","135":"A collection of environment configurations that I use for bash.\n","136":"Modeling for Environmental Management Workshop\n13-14 August 2018, Universiti Sains Malaysia\n","137":"Extracting-land-use-proportion-around-spatial-points-from-environmental-shapefiles\nThis script allows to extract land-use proportions around points in a given radius from land-use shapefiles, and then quickly\nget theses proportions for any lower sizes of buffers. Outputs are in form of shapefiles and crossed tables to use information easily.\nMainly adapted for land-use shapefiles containing all types of land-use such as cesbio landcover shapefiles which can be downloaded here:\nhttp:\/\/osr-cesbio.ups-tlse.fr\/echangeswww\/TheiaOSO\/vecteurs_2017\/liste_vecteurs.html\nExtracting-land-use-proportion-around-spatial-points-from-one-environmental-raster\nThis script allows to extract land-use proportions around points in several radius sizes from a raster layer.\nLand-use layer used available here: http:\/\/osr-cesbio.ups-tlse.fr\/echangeswww\/TheiaOSO\/OCS_2018_CESBIO.tif\nEncapsulated in a function.\nScript used\n\nExtractingLandUseBuffers.R # adapted for shapefile environmental layer (time-consuming)\nExtractingLandUseBuffers_fromRaster.R # adapted for raster environmental layer (fast)\n\nData used\n\ndepartement_01.shp # first French department used as example which must be downloaded here: http:\/\/osr-cesbio.ups-tlse.fr\/echangeswww\/TheiaOSO\/vecteurs_2017\/departement_01.zip\ndepartement_39.shp # second French department used as example which must be downloaded here: http:\/\/osr-cesbio.ups-tlse.fr\/echangeswww\/TheiaOSO\/vecteurs_2017\/departement_39.zip\npoints.shp # random points used as example\nDEPARTEMENT.shp # French departement layer allowing to only select in the script department layers (when a lot of layers are in the folder) containing points\n\n","138":"mPies: metaProteomics in environmental sciences \nmPies is a workflow to create annotated databases for metaproteomic analysis.\nThis workflow uses three different databases for a metagenome (i) OTU-table, (ii) assembled-derived, (iii) and\nunassembled-derived to build a consensus of these databases and increase the mapping sensitivity.\nIf you use mPies for your research, please cite our publication:\nWerner, J., G\u00e9ron, A., Kerssemakers, J. et al. mPies: a novel metaproteomics tool for the creation of relevant protein databases and automatized protein annotation. Biol Direct 14, 21 (2019) doi: 10.1186\/s13062-019-0253-x\nInstallation\nThe easiest way is to use bioconda and create a new environment.\nconda env create -n mpies --file conda_env.yml\nconda activate mpies\nSingleM has been packaged by AppImage (due to the Python 2 dependency).  Download\nAppImage and build the image with\ncd appimages\n.\/appimage_singlem.sh\nappimagetool-x86_64.AppImage singlem-x86_64.AppImage\/ singlem.AppImage\nUsage\nmPies consists of two parts: database creation and annotation. Both parts are written in Snakemake.\n# database creation\nsnakemake --snakefile database_creation.smk --configfile database_creation.json --cores 28\n\n# annotation\nsnakemake --snakefile annotation.smk --configfile annotation.json --cores 28\nDetailed explanation of the mpies workflow\nDatabase creation\nPreprocessing\nThe preprocessing trims the raw reads and combines the single reads into one file.\nAmplicon-derived proteome file\nIn order to create the amplicon-derived proteome file, there are two possibilities. If amplicon data is available,\nthen a text file with the taxon names (one per line) is used for downloading the proteomes from UniProt. If no\namplicon data is available, you can set the option config[\"otu_table\"][\"run_singlem\"] to true and a taxon file is\ncreated with SingleM (this tool detects OTU abundances based on metagenome shotgun sequencing data).\nFunctional-derived subset\nIt is also possible to create a subset derived from UniProt based not only on taxonomy but to also restrict the\ngene and functional names instead of downloading the entire proteomes for the taxa of interest. To do so, a TOML file\nshould be created (see example below)\nTaxonomy = [\n    \"Bacteria\"\n]\nGene_names = [\n     \"dnaK\",\n     \"soxA\"\n]\nProtein_names = [\n    \"Heat shock protein 70\", # something commented\n]\nand the path needs to be set in the snakemake configuration (config[\"functional_subset\"][\"toml_file\"]).\nAssembled-derived proteome file\nIf only raw data is available, it is possible to run an assembly with MEGAHIT or metaSPAdes (set\nconfig[\"assembled\"][\"run_assembly\"] to true and config[\"assembled\"][\"assembler\"] to megahit or metaspades).\nPlease keep in mind that assemblies can take a lot of time depending on the size of the dataset. If you already have an\nassembly, set config[\"assembled\"][\"run_assembly\"] to false and create a symlink of your assembly into\n{sample}\/assembly\/contigs.fa. If you have no gene calling yet, remember to set\nconfig[\"assembled\"][\"run_genecalling\"] to true.\nIf you have both assembly and gene calling already performed, set config[\"assembled\"][\"run_assembly\"] and\nconfig[\"assembled\"][\"run_genecalling\"] to false and create a symlink of the assembled proteome into\n{sample}\/proteome\/assembled.faa.\nUnassembled-derived proteome file\nTo create the unassembled-derived proteome file, FragGeneScan is used (and prior to that a fastq-to-fasta\nconversion).\nPostprocessing\nDuring the postprocessing, the all three proteomes are combined into one file. Short sequences (< 30 amino acids)\nare deleted and all duplicates are removed. Afterwards, the fasta headers are hashed to shorten the headers (and save\nsome disk space).\nAnnotation\nPreprocessing\nFor now, the identified proteins are inferred from ProteinPilot. The resulting Excel file is used to create a protein\nfasta file that only contains the identified proteins. Taxonomic and functional analysis are conducted for the\nidentified proteins.\nTaxonomical annotation\nThe taxonomic analysis is performed with blast2lca from the MEGAN package. Per default, the taxonomic analysis is set\nto false in the snake config file.\nSome prerequisites are necessary to run the taxonomic analysis for the created proteome fasta file.\n\n\nDownload MEGAN. Don't forget to also\nto download and unzip the file prot_acc2tax-June2018X1.abin.zip from the same page.\n\n\nDownload the nr.gz fasta file from NCBI (size: 40 GB).\n\n\nwget ftp:\/\/ftp.ncbi.nlm.nih.gov\/blast\/db\/FASTA\/nr.gz\nwget ftp:\/\/ftp.ncbi.nlm.nih.gov\/blast\/db\/FASTA\/nr.gz.md5\nmd5sum -c nr.gz.md5\nIf the checksum does not match, the download was probably not complete. wget -c continues a partial download.\n\nCreate a diamond database of the file nr.gz.\n\ndiamond makedb --threads <number_of_threads> --in nr.gz --db nr.dmnd\n\nNow you can set config[\"taxonomy\"][\"run_taxonomy\"] to true and run snakemake. Remember to set the paths for the\ndiamond database, the binary of blast2lca and the path to the file prot_acc2tax-Jun2018X1.abin. Please note that\ndiamond blastp takes a very long time to execute.\n\nFunctional annotation\nDifferent databases can be used to add functional annotation. Per default, the funtional annotation is set to false.\nCOG\nIn order to use the COG database, some prerequisites have to be fulfilled before.\n\nDownload the necessary files from the FTP server.\n\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/prot2003-2014.fa.gz\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/cog2003-2014.csv\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/cognames2003-2014.tab\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/fun2003-2014.tab\n\nCreate a diamond database of the file prot2003-2014.fa.gz.\n\ndiamond makedb --threads <number_of_threads> --in prot2003-2014.fa.gz --db cog.dmnd\n\nNow you can set config[\"functions\"][\"run_cog\"][\"run_functions_cog\"] to true and run snakemake. Remember to set\nthe paths for the diamond database and the files cog_table, cog_names, and cog_functions.\n\nUniProt\/GO\nIn order to use the GO ontologies included in the UniProt database (SwissProt or TrEMBL), some prerequisites have to\nbe fulfilled before.\n\nDownload the necessary files from the FTP server.\n\n# SwissProt\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_sprot.fasta.gz\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_sprot.dat.gz\n\n# TrEMBL\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_trembl.fasta.gz\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_trembl.dat.gz\nPlease note that TrEMBL is quite large (29 GB for uniprot_trembl.fasta.gz and 78 GB for uniprot_trembl.dat.gz).\n\nCreate a diamond database of the fasta file (here the SwissProt database will be used)\n\ndiamond makedb --threads <number_of_threads> --in uniprot_sprot.fasta.gz --db sprot.dmnd\n\nUse the dat file downloaded from UniProt to create a table with protein accessions and GO annotations\n\n.\/main.py prepare_uniprot_files -u ...\/uniprot_sprot.dat.gz -t ...\/sprot.table.gz\nPlease note that input and output files must be\/are compressed with gzip.\n\nNow you can set config[\"functions\"][\"run_uniprot\"][\"run_functions_uniprot\"] to true and run snakemake.\n\nTest data\nThe test data set is a subset from the Ocean Sampling Day (first 18,000 lines for each read file), Accession number\nERR770958 obtained from https:\/\/www.ebi.ac.uk\/ena\/data\/view\/ERR770958). The data is deposited in the test_data\ndirectory of this repository.\n","139":"mPies: metaProteomics in environmental sciences \nmPies is a workflow to create annotated databases for metaproteomic analysis.\nThis workflow uses three different databases for a metagenome (i) OTU-table, (ii) assembled-derived, (iii) and\nunassembled-derived to build a consensus of these databases and increase the mapping sensitivity.\nIf you use mPies for your research, please cite our publication:\nWerner, J., G\u00e9ron, A., Kerssemakers, J. et al. mPies: a novel metaproteomics tool for the creation of relevant protein databases and automatized protein annotation. Biol Direct 14, 21 (2019) doi: 10.1186\/s13062-019-0253-x\nInstallation\nThe easiest way is to use bioconda and create a new environment.\nconda env create -n mpies --file conda_env.yml\nconda activate mpies\nSingleM has been packaged by AppImage (due to the Python 2 dependency).  Download\nAppImage and build the image with\ncd appimages\n.\/appimage_singlem.sh\nappimagetool-x86_64.AppImage singlem-x86_64.AppImage\/ singlem.AppImage\nUsage\nmPies consists of two parts: database creation and annotation. Both parts are written in Snakemake.\n# database creation\nsnakemake --snakefile database_creation.smk --configfile database_creation.json --cores 28\n\n# annotation\nsnakemake --snakefile annotation.smk --configfile annotation.json --cores 28\nDetailed explanation of the mpies workflow\nDatabase creation\nPreprocessing\nThe preprocessing trims the raw reads and combines the single reads into one file.\nAmplicon-derived proteome file\nIn order to create the amplicon-derived proteome file, there are two possibilities. If amplicon data is available,\nthen a text file with the taxon names (one per line) is used for downloading the proteomes from UniProt. If no\namplicon data is available, you can set the option config[\"otu_table\"][\"run_singlem\"] to true and a taxon file is\ncreated with SingleM (this tool detects OTU abundances based on metagenome shotgun sequencing data).\nFunctional-derived subset\nIt is also possible to create a subset derived from UniProt based not only on taxonomy but to also restrict the\ngene and functional names instead of downloading the entire proteomes for the taxa of interest. To do so, a TOML file\nshould be created (see example below)\nTaxonomy = [\n    \"Bacteria\"\n]\nGene_names = [\n     \"dnaK\",\n     \"soxA\"\n]\nProtein_names = [\n    \"Heat shock protein 70\", # something commented\n]\nand the path needs to be set in the snakemake configuration (config[\"functional_subset\"][\"toml_file\"]).\nAssembled-derived proteome file\nIf only raw data is available, it is possible to run an assembly with MEGAHIT or metaSPAdes (set\nconfig[\"assembled\"][\"run_assembly\"] to true and config[\"assembled\"][\"assembler\"] to megahit or metaspades).\nPlease keep in mind that assemblies can take a lot of time depending on the size of the dataset. If you already have an\nassembly, set config[\"assembled\"][\"run_assembly\"] to false and create a symlink of your assembly into\n{sample}\/assembly\/contigs.fa. If you have no gene calling yet, remember to set\nconfig[\"assembled\"][\"run_genecalling\"] to true.\nIf you have both assembly and gene calling already performed, set config[\"assembled\"][\"run_assembly\"] and\nconfig[\"assembled\"][\"run_genecalling\"] to false and create a symlink of the assembled proteome into\n{sample}\/proteome\/assembled.faa.\nUnassembled-derived proteome file\nTo create the unassembled-derived proteome file, FragGeneScan is used (and prior to that a fastq-to-fasta\nconversion).\nPostprocessing\nDuring the postprocessing, the all three proteomes are combined into one file. Short sequences (< 30 amino acids)\nare deleted and all duplicates are removed. Afterwards, the fasta headers are hashed to shorten the headers (and save\nsome disk space).\nAnnotation\nPreprocessing\nFor now, the identified proteins are inferred from ProteinPilot. The resulting Excel file is used to create a protein\nfasta file that only contains the identified proteins. Taxonomic and functional analysis are conducted for the\nidentified proteins.\nTaxonomical annotation\nThe taxonomic analysis is performed with blast2lca from the MEGAN package. Per default, the taxonomic analysis is set\nto false in the snake config file.\nSome prerequisites are necessary to run the taxonomic analysis for the created proteome fasta file.\n\n\nDownload MEGAN. Don't forget to also\nto download and unzip the file prot_acc2tax-June2018X1.abin.zip from the same page.\n\n\nDownload the nr.gz fasta file from NCBI (size: 40 GB).\n\n\nwget ftp:\/\/ftp.ncbi.nlm.nih.gov\/blast\/db\/FASTA\/nr.gz\nwget ftp:\/\/ftp.ncbi.nlm.nih.gov\/blast\/db\/FASTA\/nr.gz.md5\nmd5sum -c nr.gz.md5\nIf the checksum does not match, the download was probably not complete. wget -c continues a partial download.\n\nCreate a diamond database of the file nr.gz.\n\ndiamond makedb --threads <number_of_threads> --in nr.gz --db nr.dmnd\n\nNow you can set config[\"taxonomy\"][\"run_taxonomy\"] to true and run snakemake. Remember to set the paths for the\ndiamond database, the binary of blast2lca and the path to the file prot_acc2tax-Jun2018X1.abin. Please note that\ndiamond blastp takes a very long time to execute.\n\nFunctional annotation\nDifferent databases can be used to add functional annotation. Per default, the funtional annotation is set to false.\nCOG\nIn order to use the COG database, some prerequisites have to be fulfilled before.\n\nDownload the necessary files from the FTP server.\n\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/prot2003-2014.fa.gz\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/cog2003-2014.csv\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/cognames2003-2014.tab\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/fun2003-2014.tab\n\nCreate a diamond database of the file prot2003-2014.fa.gz.\n\ndiamond makedb --threads <number_of_threads> --in prot2003-2014.fa.gz --db cog.dmnd\n\nNow you can set config[\"functions\"][\"run_cog\"][\"run_functions_cog\"] to true and run snakemake. Remember to set\nthe paths for the diamond database and the files cog_table, cog_names, and cog_functions.\n\nUniProt\/GO\nIn order to use the GO ontologies included in the UniProt database (SwissProt or TrEMBL), some prerequisites have to\nbe fulfilled before.\n\nDownload the necessary files from the FTP server.\n\n# SwissProt\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_sprot.fasta.gz\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_sprot.dat.gz\n\n# TrEMBL\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_trembl.fasta.gz\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_trembl.dat.gz\nPlease note that TrEMBL is quite large (29 GB for uniprot_trembl.fasta.gz and 78 GB for uniprot_trembl.dat.gz).\n\nCreate a diamond database of the fasta file (here the SwissProt database will be used)\n\ndiamond makedb --threads <number_of_threads> --in uniprot_sprot.fasta.gz --db sprot.dmnd\n\nUse the dat file downloaded from UniProt to create a table with protein accessions and GO annotations\n\n.\/main.py prepare_uniprot_files -u ...\/uniprot_sprot.dat.gz -t ...\/sprot.table.gz\nPlease note that input and output files must be\/are compressed with gzip.\n\nNow you can set config[\"functions\"][\"run_uniprot\"][\"run_functions_uniprot\"] to true and run snakemake.\n\nTest data\nThe test data set is a subset from the Ocean Sampling Day (first 18,000 lines for each read file), Accession number\nERR770958 obtained from https:\/\/www.ebi.ac.uk\/ena\/data\/view\/ERR770958). The data is deposited in the test_data\ndirectory of this repository.\n","140":"mPies: metaProteomics in environmental sciences \nmPies is a workflow to create annotated databases for metaproteomic analysis.\nThis workflow uses three different databases for a metagenome (i) OTU-table, (ii) assembled-derived, (iii) and\nunassembled-derived to build a consensus of these databases and increase the mapping sensitivity.\nIf you use mPies for your research, please cite our publication:\nWerner, J., G\u00e9ron, A., Kerssemakers, J. et al. mPies: a novel metaproteomics tool for the creation of relevant protein databases and automatized protein annotation. Biol Direct 14, 21 (2019) doi: 10.1186\/s13062-019-0253-x\nInstallation\nThe easiest way is to use bioconda and create a new environment.\nconda env create -n mpies --file conda_env.yml\nconda activate mpies\nSingleM has been packaged by AppImage (due to the Python 2 dependency).  Download\nAppImage and build the image with\ncd appimages\n.\/appimage_singlem.sh\nappimagetool-x86_64.AppImage singlem-x86_64.AppImage\/ singlem.AppImage\nUsage\nmPies consists of two parts: database creation and annotation. Both parts are written in Snakemake.\n# database creation\nsnakemake --snakefile database_creation.smk --configfile database_creation.json --cores 28\n\n# annotation\nsnakemake --snakefile annotation.smk --configfile annotation.json --cores 28\nDetailed explanation of the mpies workflow\nDatabase creation\nPreprocessing\nThe preprocessing trims the raw reads and combines the single reads into one file.\nAmplicon-derived proteome file\nIn order to create the amplicon-derived proteome file, there are two possibilities. If amplicon data is available,\nthen a text file with the taxon names (one per line) is used for downloading the proteomes from UniProt. If no\namplicon data is available, you can set the option config[\"otu_table\"][\"run_singlem\"] to true and a taxon file is\ncreated with SingleM (this tool detects OTU abundances based on metagenome shotgun sequencing data).\nFunctional-derived subset\nIt is also possible to create a subset derived from UniProt based not only on taxonomy but to also restrict the\ngene and functional names instead of downloading the entire proteomes for the taxa of interest. To do so, a TOML file\nshould be created (see example below)\nTaxonomy = [\n    \"Bacteria\"\n]\nGene_names = [\n     \"dnaK\",\n     \"soxA\"\n]\nProtein_names = [\n    \"Heat shock protein 70\", # something commented\n]\nand the path needs to be set in the snakemake configuration (config[\"functional_subset\"][\"toml_file\"]).\nAssembled-derived proteome file\nIf only raw data is available, it is possible to run an assembly with MEGAHIT or metaSPAdes (set\nconfig[\"assembled\"][\"run_assembly\"] to true and config[\"assembled\"][\"assembler\"] to megahit or metaspades).\nPlease keep in mind that assemblies can take a lot of time depending on the size of the dataset. If you already have an\nassembly, set config[\"assembled\"][\"run_assembly\"] to false and create a symlink of your assembly into\n{sample}\/assembly\/contigs.fa. If you have no gene calling yet, remember to set\nconfig[\"assembled\"][\"run_genecalling\"] to true.\nIf you have both assembly and gene calling already performed, set config[\"assembled\"][\"run_assembly\"] and\nconfig[\"assembled\"][\"run_genecalling\"] to false and create a symlink of the assembled proteome into\n{sample}\/proteome\/assembled.faa.\nUnassembled-derived proteome file\nTo create the unassembled-derived proteome file, FragGeneScan is used (and prior to that a fastq-to-fasta\nconversion).\nPostprocessing\nDuring the postprocessing, the all three proteomes are combined into one file. Short sequences (< 30 amino acids)\nare deleted and all duplicates are removed. Afterwards, the fasta headers are hashed to shorten the headers (and save\nsome disk space).\nAnnotation\nPreprocessing\nFor now, the identified proteins are inferred from ProteinPilot. The resulting Excel file is used to create a protein\nfasta file that only contains the identified proteins. Taxonomic and functional analysis are conducted for the\nidentified proteins.\nTaxonomical annotation\nThe taxonomic analysis is performed with blast2lca from the MEGAN package. Per default, the taxonomic analysis is set\nto false in the snake config file.\nSome prerequisites are necessary to run the taxonomic analysis for the created proteome fasta file.\n\n\nDownload MEGAN. Don't forget to also\nto download and unzip the file prot_acc2tax-June2018X1.abin.zip from the same page.\n\n\nDownload the nr.gz fasta file from NCBI (size: 40 GB).\n\n\nwget ftp:\/\/ftp.ncbi.nlm.nih.gov\/blast\/db\/FASTA\/nr.gz\nwget ftp:\/\/ftp.ncbi.nlm.nih.gov\/blast\/db\/FASTA\/nr.gz.md5\nmd5sum -c nr.gz.md5\nIf the checksum does not match, the download was probably not complete. wget -c continues a partial download.\n\nCreate a diamond database of the file nr.gz.\n\ndiamond makedb --threads <number_of_threads> --in nr.gz --db nr.dmnd\n\nNow you can set config[\"taxonomy\"][\"run_taxonomy\"] to true and run snakemake. Remember to set the paths for the\ndiamond database, the binary of blast2lca and the path to the file prot_acc2tax-Jun2018X1.abin. Please note that\ndiamond blastp takes a very long time to execute.\n\nFunctional annotation\nDifferent databases can be used to add functional annotation. Per default, the funtional annotation is set to false.\nCOG\nIn order to use the COG database, some prerequisites have to be fulfilled before.\n\nDownload the necessary files from the FTP server.\n\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/prot2003-2014.fa.gz\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/cog2003-2014.csv\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/cognames2003-2014.tab\nwget ftp:\/\/ftp.ncbi.nih.gov\/pub\/COG\/COG2014\/data\/fun2003-2014.tab\n\nCreate a diamond database of the file prot2003-2014.fa.gz.\n\ndiamond makedb --threads <number_of_threads> --in prot2003-2014.fa.gz --db cog.dmnd\n\nNow you can set config[\"functions\"][\"run_cog\"][\"run_functions_cog\"] to true and run snakemake. Remember to set\nthe paths for the diamond database and the files cog_table, cog_names, and cog_functions.\n\nUniProt\/GO\nIn order to use the GO ontologies included in the UniProt database (SwissProt or TrEMBL), some prerequisites have to\nbe fulfilled before.\n\nDownload the necessary files from the FTP server.\n\n# SwissProt\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_sprot.fasta.gz\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_sprot.dat.gz\n\n# TrEMBL\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_trembl.fasta.gz\nwget ftp:\/\/ftp.uniprot.org\/pub\/databases\/uniprot\/current_release\/knowledgebase\/complete\/uniprot_trembl.dat.gz\nPlease note that TrEMBL is quite large (29 GB for uniprot_trembl.fasta.gz and 78 GB for uniprot_trembl.dat.gz).\n\nCreate a diamond database of the fasta file (here the SwissProt database will be used)\n\ndiamond makedb --threads <number_of_threads> --in uniprot_sprot.fasta.gz --db sprot.dmnd\n\nUse the dat file downloaded from UniProt to create a table with protein accessions and GO annotations\n\n.\/main.py prepare_uniprot_files -u ...\/uniprot_sprot.dat.gz -t ...\/sprot.table.gz\nPlease note that input and output files must be\/are compressed with gzip.\n\nNow you can set config[\"functions\"][\"run_uniprot\"][\"run_functions_uniprot\"] to true and run snakemake.\n\nTest data\nThe test data set is a subset from the Ocean Sampling Day (first 18,000 lines for each read file), Accession number\nERR770958 obtained from https:\/\/www.ebi.ac.uk\/ena\/data\/view\/ERR770958). The data is deposited in the test_data\ndirectory of this repository.\n","141":"Environmental-Inequality\nThis contains the data and code I utilized to build an economic model detailing air quality levels depending on one's socioeconomic characteristics.\nI did not include any code, as raw data exceed github's repository limit. Please contact me if you wish to recieve the code and I can import it to you.\nIncluded are my presentation, poster, thesis, and STATA.do file for this project. They encompass all the significant parts of my project, specifically the results and anlysis.\n","142":"environmental-demo\n","143":"environmental_scrollytelling\nOur repo for the environmental scrollytelling project\nGeneral goal:\nWe want to contextualise the issue of how Germany is progressing on decarbonisation, and potentially place this within a European.\nWhat do we need:\n\nSpatial data on coal mining\/consumption sites in Germany\nSubsidy data for fossil fuels\/renewables\nCoal consumption for Germany\nGermany legislation on climate policy\nInvestment of Germany companies in coal related projects (maybe focus on the five major power companies)\n\n","144":"Read environmental information and plot with bokeh\nThere are two main files:\nread_environmentals.py\nThis file reads environmental information from the raspberry pi sense-hat as\nwell as the MPL3115A2 sensors (for calibration), saves them, and then pushes\nthem to an AWS EC2 instance which in turn plots them.\nThere is another directory to be created in the home directory '~\/data' where\nthe files will be saved locally.\nenvironmentals.py\nThis file lives on the EC2 instance and plots the data from the raspberry pi on\na bokeh server.  There is another directory '~\/data' that needs to be created\nin the home directory for storing the data.\n","145":"Environmental-Project-\nWater Level Effect on San Diego Terrain\n","146":"This project was bootstrapped with Create React Native App.\nBelow you'll find information about performing common tasks. The most recent version of this guide is available here.\nTable of Contents\n\nUpdating to New Releases\nAvailable Scripts\n\nnpm start\nnpm test\nnpm run ios\nnpm run android\nnpm run eject\n\n\nWriting and Running Tests\nEnvironment Variables\n\nConfiguring Packager IP Address\n\n\nCustomizing App Display Name and Icon\nSharing and Deployment\n\nPublishing to Expo's React Native Community\nBuilding an Expo \"standalone\" app\nEjecting from Create React Native App\n\nBuild Dependencies (Xcode & Android Studio)\nShould I Use ExpoKit?\n\n\n\n\nTroubleshooting\n\nNetworking\niOS Simulator won't open\nQR Code does not scan\n\n\n\nUpdating to New Releases\nYou should only need to update the global installation of create-react-native-app very rarely, ideally never.\nUpdating the react-native-scripts dependency of your app should be as simple as bumping the version number in package.json and reinstalling your project's dependencies.\nUpgrading to a new version of React Native requires updating the react-native, react, and expo package versions, and setting the correct sdkVersion in app.json. See the versioning guide for up-to-date information about package version compatibility.\nAvailable Scripts\nIf Yarn was installed when the project was initialized, then dependencies will have been installed via Yarn, and you should probably use it to run these commands as well. Unlike dependency installation, command running syntax is identical for Yarn and NPM at the time of this writing.\nnpm start\nRuns your app in development mode.\nOpen it in the Expo app on your phone to view it. It will reload if you save edits to your files, and you will see build errors and logs in the terminal.\nSometimes you may need to reset or clear the React Native packager's cache. To do so, you can pass the --reset-cache flag to the start script:\nnpm start --reset-cache\n# or\nyarn start --reset-cache\n\nnpm test\nRuns the jest test runner on your tests.\nnpm run ios\nLike npm start, but also attempts to open your app in the iOS Simulator if you're on a Mac and have it installed.\nnpm run android\nLike npm start, but also attempts to open your app on a connected Android device or emulator. Requires an installation of Android build tools (see React Native docs for detailed setup). We also recommend installing Genymotion as your Android emulator. Once you've finished setting up the native build environment, there are two options for making the right copy of adb available to Create React Native App:\nUsing Android Studio's adb\n\nMake sure that you can run adb from your terminal.\nOpen Genymotion and navigate to Settings -> ADB. Select \u201cUse custom Android SDK tools\u201d and update with your Android SDK directory.\n\nUsing Genymotion's adb\n\nFind Genymotion\u2019s copy of adb. On macOS for example, this is normally \/Applications\/Genymotion.app\/Contents\/MacOS\/tools\/.\nAdd the Genymotion tools directory to your path (instructions for Mac, Linux, and Windows).\nMake sure that you can run adb from your terminal.\n\nnpm run eject\nThis will start the process of \"ejecting\" from Create React Native App's build scripts. You'll be asked a couple of questions about how you'd like to build your project.\nWarning: Running eject is a permanent action (aside from whatever version control system you use). An ejected app will require you to have an Xcode and\/or Android Studio environment set up.\nCustomizing App Display Name and Icon\nYou can edit app.json to include configuration keys under the expo key.\nTo change your app's display name, set the expo.name key in app.json to an appropriate string.\nTo set an app icon, set the expo.icon key in app.json to be either a local path or a URL. It's recommended that you use a 512x512 png file with transparency.\nWriting and Running Tests\nThis project is set up to use jest for tests. You can configure whatever testing strategy you like, but jest works out of the box. Create test files in directories called __tests__ or with the .test extension to have the files loaded by jest. See the the template project for an example test. The jest documentation is also a wonderful resource, as is the React Native testing tutorial.\nEnvironment Variables\nYou can configure some of Create React Native App's behavior using environment variables.\nConfiguring Packager IP Address\nWhen starting your project, you'll see something like this for your project URL:\nexp:\/\/192.168.0.2:19000\n\nThe \"manifest\" at that URL tells the Expo app how to retrieve and load your app's JavaScript bundle, so even if you load it in the app via a URL like exp:\/\/localhost:19000, the Expo client app will still try to retrieve your app at the IP address that the start script provides.\nIn some cases, this is less than ideal. This might be the case if you need to run your project inside of a virtual machine and you have to access the packager via a different IP address than the one which prints by default. In order to override the IP address or hostname that is detected by Create React Native App, you can specify your own hostname via the REACT_NATIVE_PACKAGER_HOSTNAME environment variable:\nMac and Linux:\nREACT_NATIVE_PACKAGER_HOSTNAME='my-custom-ip-address-or-hostname' npm start\n\nWindows:\nset REACT_NATIVE_PACKAGER_HOSTNAME='my-custom-ip-address-or-hostname'\nnpm start\n\nThe above example would cause the development server to listen on exp:\/\/my-custom-ip-address-or-hostname:19000.\nSharing and Deployment\nCreate React Native App does a lot of work to make app setup and development simple and straightforward, but it's very difficult to do the same for deploying to Apple's App Store or Google's Play Store without relying on a hosted service.\nPublishing to Expo's React Native Community\nExpo provides free hosting for the JS-only apps created by CRNA, allowing you to share your app through the Expo client app. This requires registration for an Expo account.\nInstall the exp command-line tool, and run the publish command:\n$ npm i -g exp\n$ exp publish\n\nBuilding an Expo \"standalone\" app\nYou can also use a service like Expo's standalone builds if you want to get an IPA\/APK for distribution without having to build the native code yourself.\nEjecting from Create React Native App\nIf you want to build and deploy your app yourself, you'll need to eject from CRNA and use Xcode and Android Studio.\nThis is usually as simple as running npm run eject in your project, which will walk you through the process. Make sure to install react-native-cli and follow the native code getting started guide for React Native.\nShould I Use ExpoKit?\nIf you have made use of Expo APIs while working on your project, then those API calls will stop working if you eject to a regular React Native project. If you want to continue using those APIs, you can eject to \"React Native + ExpoKit\" which will still allow you to build your own native code and continue using the Expo APIs. See the ejecting guide for more details about this option.\nTroubleshooting\nNetworking\nIf you're unable to load your app on your phone due to a network timeout or a refused connection, a good first step is to verify that your phone and computer are on the same network and that they can reach each other. Create React Native App needs access to ports 19000 and 19001 so ensure that your network and firewall settings allow access from your device to your computer on both of these ports.\nTry opening a web browser on your phone and opening the URL that the packager script prints, replacing exp:\/\/ with http:\/\/. So, for example, if underneath the QR code in your terminal you see:\nexp:\/\/192.168.0.1:19000\n\nTry opening Safari or Chrome on your phone and loading\nhttp:\/\/192.168.0.1:19000\n\nand\nhttp:\/\/192.168.0.1:19001\n\nIf this works, but you're still unable to load your app by scanning the QR code, please open an issue on the Create React Native App repository with details about these steps and any other error messages you may have received.\nIf you're not able to load the http URL in your phone's web browser, try using the tethering\/mobile hotspot feature on your phone (beware of data usage, though), connecting your computer to that WiFi network, and restarting the packager. If you are using a VPN you may need to disable it.\niOS Simulator won't open\nIf you're on a Mac, there are a few errors that users sometimes see when attempting to npm run ios:\n\n\"non-zero exit code: 107\"\n\"You may need to install Xcode\" but it is already installed\nand others\n\nThere are a few steps you may want to take to troubleshoot these kinds of errors:\n\nMake sure Xcode is installed and open it to accept the license agreement if it prompts you. You can install it from the Mac App Store.\nOpen Xcode's Preferences, the Locations tab, and make sure that the Command Line Tools menu option is set to something. Sometimes when the CLI tools are first installed by Homebrew this option is left blank, which can prevent Apple utilities from finding the simulator. Make sure to re-run npm\/yarn run ios after doing so.\nIf that doesn't work, open the Simulator, and under the app menu select Reset Contents and Settings.... After that has finished, quit the Simulator, and re-run npm\/yarn run ios.\n\nQR Code does not scan\nIf you're not able to scan the QR code, make sure your phone's camera is focusing correctly, and also make sure that the contrast on the two colors in your terminal is high enough. For example, WebStorm's default themes may not have enough contrast for terminal QR codes to be scannable with the system barcode scanners that the Expo app uses.\nIf this causes problems for you, you may want to try changing your terminal's color theme to have more contrast, or running Create React Native App from a different terminal. You can also manually enter the URL printed by the packager script in the Expo app's search bar to load it manually.\n","147":"environmental_notices\nOpen data application for Estonian Fund for Nature https:\/\/elfond.ee\/.\nScrapes official notifications from Estonian government site, parses and filters the results sends them regularily to the mailing list and provides geodata preview.\nImplemented in NodeJS.\nInstallation\nnpm install\n\nThen create a config\/config.json file:\n{\n  \"baseUrl\": \"site_base_url\",\n  \"mailFrom\": \"mail_to_send_from\",\n  \"mailTo\": [\n    \"mail_to_send_to_1\",\n    \"mail_to_send_to_2\"\n  ],\n  \"mailUsername\": \"gmail_account_to_sent_from\",\n  \"mailPassword\": \"gmail_account_password\"\n}\n\nRunning\nTo invoke data scraping:\nnode scraper.js\n\nTo invoke mailer:\nnode mailer.js\n\n","148":"Environmental-Sustainability\nA document that was created during ThinkChicago's Idea Week that focuses on a environmental sustainability in Chicago.\nSelected as one of 20 total teams to pitch our solution to judges.\nLinks to surveys at the end of the document.\nQuality of the app and food survey: https:\/\/www.surveymonkey.com\/r\/9JYVNZK\nDemographic survey: https:\/\/www.surveymonkey.com\/r\/9QB3NYY\nFarmer survey: https:\/\/www.surveymonkey.com\/r\/9SQ2WZP\nI plan on creating a web app for this once I get more experience in web backend.\n","149":"Environmental-Issues\n","150":"environmental-organisation\n","151":"environmental-app\n","152":"foodprint-calculator\nSetup\nnpm install\nnpm run dev\n\nDeploy\npush to master!\n\n","153":"foodprint-calculator\nSetup\nnpm install\nnpm run dev\n\nDeploy\npush to master!\n\n","154":"foodprint-calculator\nSetup\nnpm install\nnpm run dev\n\nDeploy\npush to master!\n\n","155":"\nUrban Insights\n\n\n\nAEC Hackathon 2019 Silicon Valley project.\nAn application for visualizing proposed buildings, their code-constraints, and environmental analyses in situ using AR & VR.\nCurrent Deployed Build\nCurrent build can be accessed from the \"Deployment\" Badge above.\nDeveloper Set Up\nClone the repository and run the commands below to start the development server. Navigate to localhost:8080 on your local machine to visualize.\nnpm install\nnpm run serve\nIn order to deploy this project as intended, use Now's documentation to deploy a static, serverless build of the application. Once an account has been created with Now, you should be able to run the commands below in the root of the repository to deploy.\nnpm run build\nnow\nConcept\nPrototype\n\n\n\n\nMobile\nWeb\n\n\n\n\n\n\n\n\n\nStack\n\n","156":"\nUrban Insights\n\n\n\nAEC Hackathon 2019 Silicon Valley project.\nAn application for visualizing proposed buildings, their code-constraints, and environmental analyses in situ using AR & VR.\nCurrent Deployed Build\nCurrent build can be accessed from the \"Deployment\" Badge above.\nDeveloper Set Up\nClone the repository and run the commands below to start the development server. Navigate to localhost:8080 on your local machine to visualize.\nnpm install\nnpm run serve\nIn order to deploy this project as intended, use Now's documentation to deploy a static, serverless build of the application. Once an account has been created with Now, you should be able to run the commands below in the root of the repository to deploy.\nnpm run build\nnow\nConcept\nPrototype\n\n\n\n\nMobile\nWeb\n\n\n\n\n\n\n\n\n\nStack\n\n","157":"hamlet\n","158":"hamlet\n","159":"hamlet\n","160":"#required-env-var-plugin \n\nRequire an environmental variable in your application or throw routhlessly.\n\nA zero-dependency webpack plugin.\n##Motivation\nIf you have ever found yourself setting up default env vars in your project just to make sure your app does get some data when a developer forgets to provide it, you run a risk of serving a hard debugging time to other contributors or even worse, ending up with wrong configuration in deployment process.\nNo more accidental values for environmental variables in your app. Get a hard reminder on your face that there's something to be set.\n##Usage\nThe only requirement is a webpack package installed, but since you're checking out a webpack plugin, you're probably already there.\nJust register the plugin and provide the required env var names as parameters:\nconst RequiredEnvVarPlugin = require('required-env-var-plugin')\n\/\/...\nmodule.exports = {\n  \/\/...\n  plugins: [\n    new RequiredEnvVarPlugin('API_URL', 'USER', 'PASS')\n  ]\n  \/\/...\n}\nYou can provide the variables both as a list: REVP('API_URL', 'USER', 'PASS') or as an array: REVP(['API_URL', 'USER', 'PASS']).\n#How does it work?\nUnder the hood it just uses webpack's DefinePlugin and passes it the object of shape:\n{\n  'process.env': {\n    API_URL: xxx,\n    USER: xxx,\n    PASS: xxx,\n  }\n}\nwhere xxx are respective environmental variables derived from process.env.xxx. If it doesn't find one, throws.\n##FAQ\n###Why throw? Can't I just warn the user?\nNo. That's the whole purpose of the plugin. If developer didn't infere from the code, didn't find out from the docs or didn't deduce from the application working that the env var hadn't been set, then he surely won't notice it in a bunch of logs spitted onto the console during startup. The message has to be clear: You forgot to do it, I won't launch!\nLicense\nMIT (https:\/\/opensource.org\/licenses\/mit-license.php)\n","161":"CRUES\nCo-operative robotics using environmental sensors\nDirectory structure\n\ndocs: Documentation, including PDF of final report\n\ndocs\/interim: Interim report LaTeX root\ndocs\/final: Final report LaTeX root\ndocs\/img: Image root for reports\n\n\nwiring: Wiring diagrams, PCB files\nrviz: Rviz configuration files for visualising odemetry and mapping data\ncrues_pi: ROS nodes and Python code\n\ncrues_pi\/ros_pkgs: ROS packages to be added to catkin workspace\ncrues_pi\/crues: Scripts used for testing and debugging\ncrues_pi\/config: Configuration files for individual robots\n(Deploy script copies correct file to each RPi)\ncrues_pi\/crues_deploy: Script for deploying code to RPis\ncrues_pi\/crues_run: Script for running Roomba code on RPis\n\n\n\nDeploying and running code\n\nTurn on robots\nConnect to CruesNet ad-hoc WiFi network\nAdd crues_pi\/crues_deploy and crues_pi\/crues_run to PATH\nRun crues_deploy [ROBOTS] to deploy code, with [ROBOTS] replaced by any combination of blinky, inky and clyde\nRun crues_run [ROBOTS] to run the roomba.launch ROS launch file\n\nMore details can be found in the project Wiki.\n","162":"Grow\n\nFeatures:\n\nMonitor the enviroment\nOptionally control temperature with Wemo switches (edit the script to control other devices)\nOptionally report soil moisture on a per-plant basis (up to 8)\nE-mail alerts when enviroment exceeds alarm values\n\nRequires:\n\nEcowitt GW1000 WiFi gateway\nLinux, apache, MySQL, python3\n\nOptional:\n\nWemo switches for automation\nIP Webcam\nEcowitt soil moisture sensors\n\nSee my full grow list.\nAs an Amazon Associate I earn from qualifying purchases.\nInstallation\n\nPrepare linux host with working MySQL and python3.\nPlace files under the web root and make sure .py files execute as CGI\nCreate a database called grow and create tables with mysql> create database grow and mysql grow < schema.mysql.\ncp myconfig.sample myconfig.py and edit cofiguration.\nConfigure Ecowitt GW1000 to post data to ecowitt.py (Use \"WS View\" app, go to \"Weather Services -> Customized\", enter server address, \/ecowitt.py as path, upload interval 60.\nCopy grow.service to \/etc\/systemd\/system and edit to make the grow-control.py run at boot, and restart on crash.  systemctl daemon-reload once in place, then systemctl enable grow then service grow start then service grow status.\n\nWemo automation:\n\nInstall ouimeaux\nTest command line: wemo list\nMake sure device names match those in grow-control.py.\n\nWebcam setup:\n\nEdit get-image.bash to fetch an image from your webcam so the python script can add to it.\nCreate a cronjob to run the above script every minute: * * * * * \/path\/to\/get-image.bash\nRun the script by hand to test it creates output as expected\nFetch out.jpg via your webserver\nOptionally speciy location (x,y coordinates with 0,0 at top left) of pots\/soil sensors in config file\n\n","163":"Environmental Variables\nWhy use environmental variables?\nLeaving sensitive information in the code you push to Github can leave you vulnerable to attacks by malicious, evil, ne'er-do-wells, which can land you in financial or legal trouble. By using environmental variables, we ensure that the variables we want to keep hidden are only accessible on a particular machine and to the programs running on it.\nHere's a quick and dirty guide to hiding your secrets (like your API keys or other sensitive information) in your environment!\nTable of Contents\n\nExporting variables in bash\nDefining variables in your bash profile\nUsing the dotenv npm module\n\nExporting variables in bash\nWhat is an \"environment?\"\n\nYour bash environment contains variables that define system properties.\nIt's how your system is configured. It's actually just a set of key\/value pairs we can view by typing printenv in our terminals.\n\n$ printenv\n\n> displays all sorts of key-value pairs!\nWe can add variables to our environment...\n$ export ENV_VAR=\"anna is cool\"\n$ echo $ENV_VAR\n> \"anna is cool\"\n...and delete them.\n$ unset ENV_VAR\n$ echo $ENV_VAR\n>\nOur variable is dead and gone.\nNote: Notice that while we don't set an environmental variable with a \"$\", we do need to add it when we're referring to it again in bash.\nUsing your bash profile\nOur enviromental variables disappear when we close out of our terminal window and reopen it. OH NO. Solution: add it to our bash profile. Our bash profile is a hidden file in our \/users\/YOURUSERNAMEHERE\/ folder that saves our configuration commands and runs them whenever we start up a new terminal window. It's how we set up customizations and defaults.\nOpen up your bash profile...\nsubl ~\/.bash_profile\n\n...add environmental variables at will...\nexport super_secret=\"shh don't tell\"\n...save the file, and back in your terminal...\nsource ~\/.bash_profile\n...load up your new defaults. Your enviromental variable is now accessible!\nNode, process.env, and you\nYou can access your enviromental variables in node through the process.env object:\nconsole.log(process.env);\n...will perform almost the same function as typing printenv in bash.\nAccess particular variables using dot notation:\nconsole.log(process.env.super_secret)\n\/\/ prints \"shh don't tell\"\ndotenv\ndotenv is a module that allows us to store environmental variables in an external file in our project folder, rather than in our bash environments.\nAdvantages of using the npm module\n\nWe don't need to store a potentially infinite amount of sensitive info in our bash profiles.\nWe can store sensitive information in a project-specific location (and store only the keys we need for that project).\n\nUsage is very simple.\n\nCreate a .env file in your app's folder.\nAdd your secrets to the file.\n\nanna=\"awesome\"\nso_very_secret2=\"shh don't tell\"\n\n\nRequire the module and run the load function in your app.\n\nJavascript:\n\/\/ requires the dotenv module & runs its load function immediately\nvar dotenv = require('dotenv').load();\n\n\/\/ now our custom environmental variables have been added to process.env!\nconsole.log(process.env.anna)\n\/\/prints \"awesome\"\nRuby\n# requires dotenv, loads in env vars\nrequire \"dotenv\"\nDotenv.load\n# same deal!\nputs ENV['anna']\n# prints \"awesome\"\nVERY IMPORTANT:\nDo NOT push your .env file to Github!!!!\nMake sure this file is included in your .gitignore, or your secrets will be on display for all to steal.\nDigital Ocean\nYou can either touch a new .env file on your Digital Ocean droplet, or use scp (secure copy).\nHeroku\nOnce your app is deployed, use heroku's config:set, get, and unset methods\nheroku config:set ENV_VAR=\"whatever\"\n# like printenv\nheroku config\nheroku config:get ENV_VAR\nheroku config:unset ENV_VAR\nYou could also secure copy over your .env file:\nheroku run bash\n# Copy the file FROM your machine to the local (heroku) machine\nscp user@mylocalmachine:\/home\/user\/dir\/file.txt .\n\nOR use Foreman apparently iono\n","164":"environmental-hazards\nWebsite for Environmental Hazards Detection LLC\n","165":"eflow-species\nPISCES species analysis by environmental flow (eflow) type\nPublished versions of Rmd files in github pages branch or at\nhttp:\/\/ucd-cws.github.io\/eflows-species\/eflow-distance.html (for example)\n","166":"Welcome to GitHub Pages\nYou can use the editor on GitHub to maintain and preview the content for your website in Markdown files.\nWhenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files.\nMarkdown\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\nFor more details see GitHub Flavored Markdown.\nJekyll Themes\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your repository settings. The name of this theme is saved in the Jekyll _config.yml configuration file.\nSupport or Contact\nHaving trouble with Pages? Check out our documentation or contact support and we\u2019ll help you sort it out.\n","167":"Environmental-Logger\nLog environmental data such as temperature, pressure, humidity, and ambient light using a Tessel 2. Data is then streamed using socket.io and dygraphs.\nBME280 is on pins 0 and 1, and photoresistor is on pin 7, all on port A.\n","168":"\n\n\n\n\n\n\n\n\n\n\nWhat is Svelte?\nSvelte is a new way to build web applications. It's a compiler that takes your declarative components and converts them into efficient JavaScript that surgically updates the DOM.\nLearn more at the Svelte website, or stop by the Discord chatroom.\nDevelopment\nPull requests are encouraged and always welcome. Pick an issue and help us out!\nTo install and work on Svelte locally:\ngit clone https:\/\/github.com\/sveltejs\/svelte.git\ncd svelte\nnpm install\n\nDo not use Yarn to install the dependencies, as the specific package versions in package-lock.json are used to build and test Svelte.\n\nTo build the compiler, and all the other modules included in the package:\nnpm run build\nTo watch for changes and continually rebuild the package (this is useful if you're using npm link to test out changes in a project locally):\nnpm run dev\nThe compiler is written in TypeScript, but don't let that put you off \u2014 it's basically just JavaScript with type annotations. You'll pick it up in no time. If you're using an editor other than Visual Studio Code you may need to install a plugin in order to get syntax highlighting and code hints etc.\nRunning Tests\nnpm run test\nTo filter tests, use -g (aka --grep). For example, to only run tests involving transitions:\nnpm run test -- -g transition\nsvelte.dev\nThe source code for https:\/\/svelte.dev, including all the documentation, lives in the site directory. The site is built with Sapper.\nIs svelte.dev down?\nProbably not, but it's possible. If you can't seem to access any .dev sites, check out this SuperUser question and answer.\nLicense\nMIT\n","169":"Environmental Consulting Web Site\nThis project was an attempt to re-create the web site of an environmental consulting firm using Bootstrap. I think it's a big improvement.\n","170":"Environmental Consulting Web Site\nThis project was an attempt to re-create the web site of an environmental consulting firm using Bootstrap. I think it's a big improvement.\n","171":"Environmental Consulting Web Site\nThis project was an attempt to re-create the web site of an environmental consulting firm using Bootstrap. I think it's a big improvement.\n","172":"amritsar.today\nReal time environmental data for Amritsar city (Backend + Frontend)\n","173":"amritsar.today\nReal time environmental data for Amritsar city (Backend + Frontend)\n","174":"\nenvPred \n\n\n\n\n\n\n\n\n\nenvPred is a package that calculates five statistics from\nenvironmental time-series data: seasonality, the colour of environmental\nnoise (hereafter colour), constancy, contingency and predictability.\nSeasonality entails the regularity in the timing and magnitude of\nfluctuations in the average environmental state over seasons. Colour is\ndefined by how predictable and similar the environment is between\nsuccessive time points, or how far into the future the environmental\nstate is likely to stay the same, independent of the mean environmental\nstate. White noise occurs when there is no correlation between one\nmeasurement and the next, while for reddened noise, there is some\ncorrelation between measurements separated by a finite time-scale.\nSeasonality and colour are calculated following the steps described in\nBarneche et\nal.\u00a0(2018).\nWe first remove linear trends by extracting the residuals from a linear\nregression model fitted to the raw time series. Seasonality is estimated\nin two forms: 1) as the \u201cunbounded\u201d fraction of the total variance that\nis due to predictable seasonal periodicities, \u03b1\/\u03b2, where \u03b1 is the\nvariance of the seasonal trend, and \u03b2 is the variance of the residual\ntime series (i.e.\u00a0the time series after the seasonal trend was removed);\nor 2) as the \u201cbounded\u201d fraction of the total variance that is due to\npredictable seasonal periodicities, \u03b1\/(\u03b1\u2005+\u2005\u03b2). The seasonal trend\nis estimated by binning the time-series data into monthly intervals,\naveraging each month across the duration of the time series, then\nre-creating a seasonal time-series dataset on the same time-scale as the\noriginal data using a linear interpolation between the monthly\nmidpoints. To calculate colour, we first calculate a residual time\nseries by subtracting the corresponding seasonal value from each data\npoint in the time series. The spectral density (i.e.\u00a0variance in the\nresidual time series) was assumed to scale with frequency, f,\naccording to an inverse power law, 1\/f\u03b8 (Halley & Kunin,\n1999;\nVasseur & Yodzis,\n2004).\nThe spectral exponent \u03b8 is then estimated as the negative slope of the\nlinear regression of the natural log of spectral density as a function\nof the natural log of frequency. By definition, white noise means that\n\u03b8\u2004=\u20040, and reddened noise means that \u03b8\u2004>\u20040. Spectral density is\nestimated using the\nspectrum\nfunction from the\nstats R\npackage if the time series is evenly distributed, and the Lomb\u2013Scargle\nfunction\nlsp\nfrom the\nlomb R\npackage if the time series is unevenly distributed (Glynn et\nal.\u00a02006).\nSpectral densities and therefore \u03b8 are calculated between the\nfrequencies of 2\/(n\u00a0\u0394\u00a0t) and 1\/(2\u00a0\u0394\u00a0t) (i.e.\u00a0Nyquist\nfrequency), where \u0394\u00a0t is the time gap between consecutive points in\nthe time series, and n is the number of observations in the time\nseries.\nConstancy, contingency and predictability are calculated following\nColwell\n(1974).\nConstancy measures the extent to which the environment is the same for\nall months in all years. Contingency measures the extent to which the\nenvironmental differences between months are the same in all years.\nPredictability is the sum of constancy and contingency. Maximum\npredictability can be attained as a consequence of either complete\nconstancy, complete contingency, or a combination of constancy and\ncontingency.\nInstallation\nThe envPred package can be installed from GitHub using the\ndevtools package using\ndevtools::install_github.\nIf you do not yet have devtools, install with\ninstall.packages(\"devtools\").\nThen install envPred using the following:\nlibrary(devtools)\ninstall_github(\"dbarneche\/envPred\")\nlibrary(envPred)\nAvailable data sources in envPred\nenvPred provides two test datasets for the user to understand the\nbehaviour and output of package functions: sst (Sea Surface\nTemperature), and npp (ocean Net Primary Productivity). The former\ncontains evenly, complete distributed data at a daily interval; the\nlatter contains unevenly, incomplete (i.e.\u00a0missing data) data. Both\nsample datasets were obtained from a random coordinate using the\nnoaaErddap R package:\n\nNet Primary Productivity (NPP)\ndata\nSea Surface Temperature\n(SST)\n\nThe package can be used for any time-series data, e.g.\u00a0temperature,\nrainfall, light intensity, etc.\nAuthors\nDr.\u00a0Diego Barneche (Australian Institute of Marine Science) and\nDr.\u00a0Scott Burgess (Florida State University Tallahassee)\nFurther Information\nFurther information about envPred, including vignettes and help files,\ncan be seen on the on-line project\npage.\nThis R package is provided for use under the MIT License\n(MIT) by the authors.\nBug reporting\nPlease report any issues or\nbugs.\n","175":"\nenvPred \n\n\n\n\n\n\n\n\n\nenvPred is a package that calculates five statistics from\nenvironmental time-series data: seasonality, the colour of environmental\nnoise (hereafter colour), constancy, contingency and predictability.\nSeasonality entails the regularity in the timing and magnitude of\nfluctuations in the average environmental state over seasons. Colour is\ndefined by how predictable and similar the environment is between\nsuccessive time points, or how far into the future the environmental\nstate is likely to stay the same, independent of the mean environmental\nstate. White noise occurs when there is no correlation between one\nmeasurement and the next, while for reddened noise, there is some\ncorrelation between measurements separated by a finite time-scale.\nSeasonality and colour are calculated following the steps described in\nBarneche et\nal.\u00a0(2018).\nWe first remove linear trends by extracting the residuals from a linear\nregression model fitted to the raw time series. Seasonality is estimated\nin two forms: 1) as the \u201cunbounded\u201d fraction of the total variance that\nis due to predictable seasonal periodicities, \u03b1\/\u03b2, where \u03b1 is the\nvariance of the seasonal trend, and \u03b2 is the variance of the residual\ntime series (i.e.\u00a0the time series after the seasonal trend was removed);\nor 2) as the \u201cbounded\u201d fraction of the total variance that is due to\npredictable seasonal periodicities, \u03b1\/(\u03b1\u2005+\u2005\u03b2). The seasonal trend\nis estimated by binning the time-series data into monthly intervals,\naveraging each month across the duration of the time series, then\nre-creating a seasonal time-series dataset on the same time-scale as the\noriginal data using a linear interpolation between the monthly\nmidpoints. To calculate colour, we first calculate a residual time\nseries by subtracting the corresponding seasonal value from each data\npoint in the time series. The spectral density (i.e.\u00a0variance in the\nresidual time series) was assumed to scale with frequency, f,\naccording to an inverse power law, 1\/f\u03b8 (Halley & Kunin,\n1999;\nVasseur & Yodzis,\n2004).\nThe spectral exponent \u03b8 is then estimated as the negative slope of the\nlinear regression of the natural log of spectral density as a function\nof the natural log of frequency. By definition, white noise means that\n\u03b8\u2004=\u20040, and reddened noise means that \u03b8\u2004>\u20040. Spectral density is\nestimated using the\nspectrum\nfunction from the\nstats R\npackage if the time series is evenly distributed, and the Lomb\u2013Scargle\nfunction\nlsp\nfrom the\nlomb R\npackage if the time series is unevenly distributed (Glynn et\nal.\u00a02006).\nSpectral densities and therefore \u03b8 are calculated between the\nfrequencies of 2\/(n\u00a0\u0394\u00a0t) and 1\/(2\u00a0\u0394\u00a0t) (i.e.\u00a0Nyquist\nfrequency), where \u0394\u00a0t is the time gap between consecutive points in\nthe time series, and n is the number of observations in the time\nseries.\nConstancy, contingency and predictability are calculated following\nColwell\n(1974).\nConstancy measures the extent to which the environment is the same for\nall months in all years. Contingency measures the extent to which the\nenvironmental differences between months are the same in all years.\nPredictability is the sum of constancy and contingency. Maximum\npredictability can be attained as a consequence of either complete\nconstancy, complete contingency, or a combination of constancy and\ncontingency.\nInstallation\nThe envPred package can be installed from GitHub using the\ndevtools package using\ndevtools::install_github.\nIf you do not yet have devtools, install with\ninstall.packages(\"devtools\").\nThen install envPred using the following:\nlibrary(devtools)\ninstall_github(\"dbarneche\/envPred\")\nlibrary(envPred)\nAvailable data sources in envPred\nenvPred provides two test datasets for the user to understand the\nbehaviour and output of package functions: sst (Sea Surface\nTemperature), and npp (ocean Net Primary Productivity). The former\ncontains evenly, complete distributed data at a daily interval; the\nlatter contains unevenly, incomplete (i.e.\u00a0missing data) data. Both\nsample datasets were obtained from a random coordinate using the\nnoaaErddap R package:\n\nNet Primary Productivity (NPP)\ndata\nSea Surface Temperature\n(SST)\n\nThe package can be used for any time-series data, e.g.\u00a0temperature,\nrainfall, light intensity, etc.\nAuthors\nDr.\u00a0Diego Barneche (Australian Institute of Marine Science) and\nDr.\u00a0Scott Burgess (Florida State University Tallahassee)\nFurther Information\nFurther information about envPred, including vignettes and help files,\ncan be seen on the on-line project\npage.\nThis R package is provided for use under the MIT License\n(MIT) by the authors.\nBug reporting\nPlease report any issues or\nbugs.\n","176":"EWATEC\nThis is a web-based platform buit on top ODM (observational data model) for sharing environmental data.\nFeatures\nSetup database\nInstall postgresql\n$ sudo sh -c 'echo \"deb http:\/\/apt.postgresql.org\/pub\/repos\/apt\/ `lsb_release -cs`-pgdg main\" >> \/etc\/apt\/sources.list.d\/pgdg.list'\n$ wget -q https:\/\/www.postgresql.org\/media\/keys\/ACCC4CF8.asc -O - | sudo apt-key add -\n$ sudo apt-get update\n$ sudo apt-get install postgresql postgresql-contrib\n\nInstall postgis\nsudo apt-add-repository ppa:ubuntugis\/ubuntugis-unstable\nsudo apt-get update\nsudo apt-get install postgis\n\nConnect to PostgreSQL\n$ sudo su - postgres\n$ psql\n\nSetup virtual environment\n$ virtualenv env\nInstall GDAL in virtualenv\n\nGDAL library must have been installed.\n\nsudo apt-get install libgdal-dev.\n\nNow install Python binding for GDAL.\n\n$ export CPLUS_INCLUDE_PATH=\/usr\/include\/gdal\n$ export C_INCLUDE_PATH=\/usr\/include\/gdal\n$ (env) pip install GDAL==1.11.2\n\nInstall ibfreetype6-dev libxft-dev (for matplotlib)\n$ sudo apt-get install libfreetype6-dev libxft-dev\nInstall gfortran libblas-dev liblapack-dev libatlas-base-dev  (for scipy numpy)\n$ sudo apt-get install gfortran libblas-dev liblapack-dev libatlas-base-dev\nInstall requirements\n$ (env) pip install -r requirements.txt\nSetup gunicorn\nSetup nginx\n","177":"environmental-economics\n\u73af\u5883\u7ecf\u6d4e\u5b66\u7b14\u8bb0\n","178":" Environmental Scanner\nThis is the code repository for an environmentally aware robot, that uses an Arduino Uno to sense the environment using a variety of analog and digital sensors. This data is then sent to a Raspberry Pi, using the serial (USB) connection.\nArduino sketch for a DIY environmental sensor array\nThis comprehensive scanner uses an Arduino Uno, light sensor, water sensor, gas sensor, smoke sensor and hall sensor and to detect light, water, gas, smoke and electromagnetic fields and then sends an alert over USB to a Raspberry Pi. Currently the Raspberry Pi will only display the current status of each sensor, but future releases will allow the Raspberry Pi to use this information to make intelligent decisions based on each sensor.\nUpload the arduino-raspberry.iso to your Arduino Uno. You will need the following sensors for this project:\n\nHall Sensor (KY-003)\nGas and Alcohol Sensor (MQ3)\nLight Sensor (KY-018)\nSmoke Sensor (MQ2)\nWater Level Sensor\n\nUpload the raspberry-arduino.py script to your Raspberry Pi. In your Raspberry Pi interface, be sure to enable Serial and I2C in PiConfig. Restart your Raspberry Pi and execute the following commands:\nsudo apt-get install python-serial\nsudo pip install pyserial\nNext, connect your Arduino to your Raspberry Pi then execute:\nls \/dev\/tty*\nLook for a line with \/dev\/ttyACMO or something similar (an ACM with any number 0, 1, 2, etc.).\nOpen the raspberry-arduino.py script and update the ser=serial.Serial(\"dev\/ttyACM0\",9600) to the ACM number you found. Next, run the raspberry-arduino.py script in Pyhton3. You will see a the status of each sensor in your Python terminal.\n","179":"METEO\nMETEO - Environmental monitoring platform (Final Master Project)\n","180":"environmental-science\nMy first website developed using html\/css\/javascript\nCreated following the mozilla foundation getting started with the web tutorial\n","181":"AREA-Website\nA website currently being developed for AREA Environmental.\nMade with Bootstrap, JS, HTML, CSS.\nV1 being the first version to be supplied to the client for changes to be suggested.\nV2 being the currently developed version with the client.\n","182":"Environmental Statistics\nThis repository is to contain my work for Stat 614 in Spring of 2020.\n","183":"Environmental Statistics\nThis repository is to contain my work for Stat 614 in Spring of 2020.\n","184":"#Environmental Health Project\n#more beautiful module design\n#module programming\n","185":"Environmental-Forecasting\n","186":"Generated files\nThis repository contains generated files and a checksum.\nDo not edit the files in this repository outside of an instance of ServiceNow.\nIf you find yourself unable to import your repository due to the presence of files edited outside an instance of ServiceNow, merge commits that mix files from different revisions, or other data that does not match the checksum, you may recover using either of the following techniques:\n\n\nRemove the problem commits:\n\nClone your repository to a personal computer with the git command line tools installed and open a git command prompt in the repository root\nRun git log and take note of the SHA1s of the problem commits\nBuild revert commits using git revert SHA1 repeatedly, working backward in time, for each commit that introduced changes not generated by a ServiceNow instance\nRun git push\n\n\n\nOverwrite the problem code snapshot with a known good one:\n\nClone your repository to a personal computer with the git command line tools installed and open a git command prompt in the repository root,\nLocate a known good code snapshot and record its SHA1. For this step, git log can be useful.\nRun git reset --hard SHA1 to a commit that was generated by a ServiceNow instance\nRun git reset HEAD{1}\nRun git add -A\nRun git commit\nRun git push\n\n\n\n","187":"Environmental-Degradation\n","188":"Environmental-Degradation\n","189":"Environmental-Degradation\n","190":"Environmental-Degradation\n","191":"Environmental-Degradation\n","192":"Environmental-Pioneer\n\u4e00\u4e2a\u8ba9\u4f60\u660e\u767d\u5783\u573e\u5206\u7c7b\u7684\u7f51\u7ad9\n\u9879\u76ee\u5f00\u53d1\u56e2\u961f\nUI \u5c0f\u908b\u9062\n\u524d\u7aef marsblue\n\u540e\u7aef Elis_ao\n\u9879\u76ee\u7b80\u4ecb\n\u73b0\u5728\u793e\u4f1a\u8d8a\u6765\u8d8a\u52a0\u91cd\u89c6\u5783\u573e\u56de\u6536\u548c\u5229\u7528\uff0c\u4f46\u4eba\u4eec\u5bf9\u4e8e\u5783\u573e\u7684\u5206\u7c7b\u56de\u6536\u7b49\u77e5\u8bc6\u4e86\u89e3\u7a0b\u5ea6\u4e0d\u9ad8\uff0c\u5bfc\u81f4\u4e86\u6211\u4eec\u7684\u5783\u573e\u6709\u6548\u5206\u7c7b\u7a0b\u5ea6\u4e0d\u9ad8\uff0c\u65e0\u6cd5\u5145\u5206\u6709\u6548\u7684\u5229\u7528\u5783\u573e\u8d44\u6e90\uff0c\u6211\u4eec\u7684\u9879\u76ee\u5c31\u662f\u6293\u4f4f\u4e86\u8fd9\u4e2a\u70b9\uff0c\u5e0c\u671b\u901a\u8fc7\u8fd9\u4e2a\u9879\u76ee\u5bf9\u5927\u5bb6\u7684\u5783\u573e\u5206\u7c7b\u77e5\u8bc6\u8fdb\u884c\u666e\u53ca\uff0c\u63d0\u5347\u56fd\u6c11\u7684\u5bf9\u4e8e\u5783\u573e\u5206\u7c7b\u7684\u8ba4\u77e5\uff0c\u4ece\u800c\u63d0\u5347\u5783\u573e\u5206\u7c7b\u6709\u6548\u6027\uff0c\u4f7f\u5f97\u5783\u573e\u8d44\u6e90\u80fd\u591f\u66f4\u4f4e\u6210\u672c\u7684\u56de\u6536\uff0c\u540c\u65f6\u4fdd\u62a4\u73af\u5883\uff0c\u521b\u5efa\u4e00\u4e2a\u66f4\u7f8e\u597d\u7684\u5730\u7403\u3002\n\u7f51\u7ad9\u7b80\u4ecb\n\u672c\u7ad9\u662f\u81f4\u529b\u4e8e\u63a8\u8fdb\u5783\u573e\u56de\u6536\uff0c\u5ba3\u4f20\u548c\u79d1\u666e\u5783\u573e\u56de\u6536\u77e5\u8bc6\uff0c\u65b9\u4fbf\u5927\u4f17\u8fdb\u884c\u5783\u573e\u5206\u7c7b\u4e3a\u4e00\u4f53\u7684web\u5e73\u53f0\uff0c\u9488\u5bf9\u73b0\u4ee3\u793e\u4f1a\u5bf9\u4e8e\u5783\u573e\u56de\u6536\u7684\u6108\u52a0\u91cd\u89c6\uff0c\u4f46\u5927\u591a\u6570\u7fa4\u4f17\u5bf9\u4e8e\u8fd9\u4e00\u5757\u4e86\u89e3\u8fd8\u662f\u6709\u6240\u6b20\u7f3a\u7684\u73b0\u72b6\uff0c\u672c\u7ad9\u4e13\u95e8\u8bbe\u6709\u5783\u573e\u5206\u7c7b\u79d1\u666e\u77e5\u8bc6\u7684\u677f\u5757\uff0c\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u5e38\u89c1\u5783\u573e\u5206\u7c7b\u7684\u5c0f\u77e5\u8bc6\uff0c\u540c\u65f6\u8fd8\u6709\u4e00\u4e9b\u76f8\u5173\u5783\u573e\u5206\u7c7b\u65b0\u95fb\u7684\u7248\u5757\uff0c\u8fd8\u589e\u8bbe\u4e00\u4e2a\u589e\u5f3a\u5783\u573e\u5206\u7c7b\u77e5\u8bc6\u7684\u5c0f\u6d4b\u8bc4\u529f\u80fd\uff0c\u4ece\u591a\u65b9\u9762\u63a8\u8fdb\u4e86\u5927\u4f17\u5bf9\u4e8e\u5783\u573e\u5206\u7c7b\u7684\u91cd\u89c6\u4e0e\u4e86\u89e3\u3002\nPS\n\u6211\u4eec\u5728GitHub\u4e0a\u5f00\u653e\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u5c31\u662f\u5e0c\u671b\u80fd\u591f\u5f97\u5230\u66f4\u591a\u4eba\u7684\u5efa\u8bae\u4e0e\u6279\u8bc4\uff0c\u5e0c\u671b\u4f60\u7684\u5efa\u8bae\u80fd\u4fc3\u4f7f\u6211\u4eec\u7684\u5f3a\u5927\uff0c\u5982\u679c\u4f60\u5bf9\u6211\u4eec\u7684\u9879\u76ee\u6709\u5174\u8da3\u5e0c\u671b\u52a0\u5165\u6211\u4eec\u7684\u56e2\u961f\u53ef\u4ee5\u5728GitHub\u6216\u8005\u90ae\u7bb1\uff081559830979@qq.com\uff09\u4e0a\u8054\u7cfb\u6211\uff0c\u6211\u4eec\u671f\u5f85\u4f60\u7684\u52a0\u5165\uff01\uff01\n","193":"PMIS_PCIS\nR scripts used to implement the PMIS and PCIS input variable selection (IVS) algorithms as part of the IVS4EM project described in Galelli et. al. (2014). The PMIS algorithm is a filter IVS method developed by Sharma (2000) and later modified by Bowden et al. (2005) and May et al. (2008), where the relevance of potential inputs is evaluated based on the mutual information (MI) between each input variable and the output. The PCIS algorithm (May et al., 2008) is also a filter IVS method, where input relevance is based on partial correlation analysis. Further details of this particular implementation of these algorithms can be found in:\nGalelli S., Humphrey G.B., Maier H.R., Castelletti A., Dandy G.C. and Gibbs M.S. (2014)  An evaluation framework for input variable selection algorithms for environmental data-driven models, Environmental Modelling and Software, 62, 33-51, DOI: 10.1016\/j.envsoft.2014.08.015. (Link to Paper)\nThe purpose of the IVS4EM project is to support a comprehensive framework for the testing and evaluation of IVS algorithms, through the sharing of algorithms (open source code), datasets, and evalution criteria. (Link to IVS4EM Website)\nContents:\n\nPMI_PCIS.R: code to implement the PMIS and PCIS algorithms.\nPMIS_run.R: run the PMIS algorithm to select the most relevent inputs for a given set of input data.\nPCIS_run.R: run the PCIS algorithm to select the most relevant inputs for a given set of input data.\ninp_dat.csv: an example input data file. Column 1 contains an array of data labels or IDs (e.g. dates on which data were recorded); columns 2 to P+1 contain the P candidate input variables; and column P+2 contains the response variable, while the rows are data points. The first row contains the variable names.\n\nTo run the PMIS algorithm, the following command should be used:\nR --args [filename] [out_dir] < PMI_run.R\nwhere filename is the name of the name of the input data file (including path) and out_dir is the name of the output directory (i.e. the directory to which results will be written. This name should NOT include the whole path). The PCIS algorithm is run similarly.\nSharma, A., 2000. Seasonal to interannual rainfall probabilistic forecasts for improved water supply management: Part 1 - a strategy for system predictor identification. Journal of Hydrology 239, 232-239.\nBowden, G.J., Maier, H.R., Dandy, G.C., 2005. Input determination for neural network models in water resources applications. Part 1. Background and methodology. Journal of Hydrology 301, 75-92.\nMay, R.J., Maier, H.R., Dandy, G.C., Fernando, T.M.K.G., 2008. Nonlinear variable selection for artificial neural networks using partial mutual information. Environmental Modelling & Software 23, 1312-1326.\nCopyright 2014 Greer Humphrey.\n","194":"PMIS_PCIS\nR scripts used to implement the PMIS and PCIS input variable selection (IVS) algorithms as part of the IVS4EM project described in Galelli et. al. (2014). The PMIS algorithm is a filter IVS method developed by Sharma (2000) and later modified by Bowden et al. (2005) and May et al. (2008), where the relevance of potential inputs is evaluated based on the mutual information (MI) between each input variable and the output. The PCIS algorithm (May et al., 2008) is also a filter IVS method, where input relevance is based on partial correlation analysis. Further details of this particular implementation of these algorithms can be found in:\nGalelli S., Humphrey G.B., Maier H.R., Castelletti A., Dandy G.C. and Gibbs M.S. (2014)  An evaluation framework for input variable selection algorithms for environmental data-driven models, Environmental Modelling and Software, 62, 33-51, DOI: 10.1016\/j.envsoft.2014.08.015. (Link to Paper)\nThe purpose of the IVS4EM project is to support a comprehensive framework for the testing and evaluation of IVS algorithms, through the sharing of algorithms (open source code), datasets, and evalution criteria. (Link to IVS4EM Website)\nContents:\n\nPMI_PCIS.R: code to implement the PMIS and PCIS algorithms.\nPMIS_run.R: run the PMIS algorithm to select the most relevent inputs for a given set of input data.\nPCIS_run.R: run the PCIS algorithm to select the most relevant inputs for a given set of input data.\ninp_dat.csv: an example input data file. Column 1 contains an array of data labels or IDs (e.g. dates on which data were recorded); columns 2 to P+1 contain the P candidate input variables; and column P+2 contains the response variable, while the rows are data points. The first row contains the variable names.\n\nTo run the PMIS algorithm, the following command should be used:\nR --args [filename] [out_dir] < PMI_run.R\nwhere filename is the name of the name of the input data file (including path) and out_dir is the name of the output directory (i.e. the directory to which results will be written. This name should NOT include the whole path). The PCIS algorithm is run similarly.\nSharma, A., 2000. Seasonal to interannual rainfall probabilistic forecasts for improved water supply management: Part 1 - a strategy for system predictor identification. Journal of Hydrology 239, 232-239.\nBowden, G.J., Maier, H.R., Dandy, G.C., 2005. Input determination for neural network models in water resources applications. Part 1. Background and methodology. Journal of Hydrology 301, 75-92.\nMay, R.J., Maier, H.R., Dandy, G.C., Fernando, T.M.K.G., 2008. Nonlinear variable selection for artificial neural networks using partial mutual information. Environmental Modelling & Software 23, 1312-1326.\nCopyright 2014 Greer Humphrey.\n","195":"environmental-configuration\nAdapter to select an appropriate configuration file based on an Environment\nInstall\nnpm install environmental-configuration --save\n\nUsage\nconfig = require('environmental-configuration')('.\/config')\n\n.\/config should be the path to your configuration relative to the file that is calling it. Within that folder, you need to have at least one file called base.json. For your other environments, they need to be named **env**.json.\n","196":"environmental_law\n","197":"environmental_law\n","198":"Environmental-Health\n","199":"Example sketch for Environmental sensing Workshop on Maker Fest in Novi Sad on 10th June 2018.\nMore info in this doc.\n","200":"Environmental-Recycling\n","201":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","202":"Environmental Monitor\nMy home environmental monitor system.\nBOM\n\nAdafruit HUZZAH32 \u2013 ESP32 Feather Board\nPM2.5 Air Quality Sensor and Breadboard Adapter Kit - PMS5003\nAdafruit Sensirion SHT31-D - Temperature & Humidity Sensor\nAdafruit CCS811 Air Quality Sensor Breakout - VOC and eCO2\n\nLive Monitoring\nAdafruit IO\nWiring\n\nNotes on Sensors\nThis is still WIP, but some early observations.\nCCS811 - eCO2 and VOC\n\nneeds a temperature and humidity measurement to have accuracy.\nthe onboard temperature is no good.\ntends to fluctuate the first 48 hours after startup.\n\nSHT31\n\nHas onboard heater to reduce accumulated condensation\nTemp and Humidity oscillate, symptom of the above?\nFrequency to trigger heater?\n\nPM2.5 Sensor\nHas several readings. Currently using PM standard. Also has environmental and PPM\/0.1L.\nWill it run off the 3.7V battery?\nESP32\nI used some tricks from this article. The ESP32 runs notably cooler\nto the touch with a lower frequency and BT off.\nObservations\nSo far I have tested in my room.\nCO2\nUnfortunately one of my room windows is right below a chimney. We burn natural gas here. There\nare distinct spikes in CO2 detected if this window is open and the boiler is running. There\nare similar rises when the gas stove is in use in the kitchen.\nOvernight C02 levels also rise to around 1500PPM, unless a window is cracked in which case they are around 600-1000PPM.\nWorking from home, I spend easily 18-22 hours a day in my room. Air circulation is quite good and the CO2 levels generally stay\nunder 1000PPM. Nominal values seem to be around 600-800.\nI have 7 house plants; spider, sanservia, bamboo, and pothos varieties. Only two are full size. I'd like to see how watering cycles\nand growth affect CO2, and how quantity of plants affect the rate of drop in CO2.\nPM2.5\nI have a HEPA filter over my AC and a standalone Coway filter. Both seem to keep PM2.5 <0.2.\nI am curious to see how this changes in the winter when only the Coway is running.\n","203":"\nWhat is CodeIgniter\nCodeIgniter is an Application Development Framework - a toolkit - for people\nwho build web sites using PHP. Its goal is to enable you to develop projects\nmuch faster than you could if you were writing code from scratch, by providing\na rich set of libraries for commonly needed tasks, as well as a simple\ninterface and logical structure to access these libraries. CodeIgniter lets\nyou creatively focus on your project by minimizing the amount of code needed\nfor a given task.\n\nWhat is HMVC\nHMVC stands for Hierarchical Model View Controller application design pattern which makes your application modular. It\ngive you chance to separate the controller, model and view in to some module so you can maintenance or improve the application easily.\n\nServer Requirements\nPHP version 5.6 or newer is recommended.\nIt should work on 5.3.7 as well, but we strongly advise you NOT to run\nsuch old versions of PHP, because of potential security and performance\nissues, as well as missing features.\n\nInstallation\n\nDefault CodeIgniter installation: https:\/\/codeigniter.com\/user_guide\/installation\/index.html\nVia Composer : composer create-project alzen8work\/ci_hmvc\n\n","204":"The main part of the Project was created while the Seadevcon Hackathon was taking place (07. - 08.09.18).\nThe FeatureEngeneering notebook contains the cleaning of the seadevcon table, so there will be no NaN values left in it.\nIn Modelling.ipynb I used Random Forrest algorithm to predict what fuel the ship is using depending on all 81 parameters left. I also used Random Forrest for forecasting and got a high accuracy of predicting on what fuel the ship will be going (up to about) 100 min in the future. But the results should be used with caution! The most relevant features are accRunTime and RunNumber, so the Model is clearly overfitting to the history of this ship and engine. Sadly, while the Hackathon, I did not have the time to correct this and after the Hackathon I had to delete all the data.\nThe Some_Data_Analysis merges Seadevcon and Runlog to give an overview about the use of gas\/gasdiesel\/diesel and heavy oil over the years.\nThe solution I was providing won me 1500 Euro in the Zero Emission challenge at the Seadevcon Hackathon.\nhttp:\/\/seadevcon.com\/challenges-hackathon\/ (Challenge 2)\n","205":"\nEnvironmental-Monitoring\nThis repo contains the open-source environmental monitoring hardware designs detailed in our Low-cost electronic sensors for environmental research: pitfalls and opportunities paper. Designs are included in their individual folders:\n1. Basic versatile logger\n2. Water table depth probe\n3. Air quality logger\n4. Water quality logger\n5. Time-sequencing lake sediment trap\n6. High-frequency measurements of wind-blown sand\nSchematics and build instructions for the Freestation Automated Weather Stations and other specific designs can be found at www.freestation.org\nLicense\nThis work is licensed under a Creative Commons Attribution 4.0 International\nLicense - any use of any material here requires attribution.\nCitation Information:\nChan, K., Schillereff, D., Baas, A., Chadwick, M., Main, B., Mulligan, M., O'Shea, F., Pearce, R., Smith, T.E., van Soesbergen, A., Tebbs, E. and Thompson, J., 2020. Low-cost electronic sensors for environmental research: pitfalls and opportunities. Progress in Physical Geography: Earth and Environment DOI:10.1177\/0309133320956567\n\n","206":"Environmental Studies\n\nEnvironmental Studies is statically generated with Jekyll and hosted by The College of Liberal Arts at Temple University.\n\n\n\nLinks\n\n\n\n\n\nDevelopment\nhttps:\/\/develop.cla.temple.edu\/environmental-studies\/\n\n\nProduction\nhttps:\/\/www.cla.temple.edu\/environmental-studies\/\n\n\n\nContent Structure\n\n\n\nDirectory\n\n\n\n\n\n_data\/faculty.yaml\nDatafile for list of faculty.\n\n\n_data\/navigation.yaml\nDatafile for primary   navigation links.\n\n\npages\/*\nPage content, in .md or .html format.\n\n\nmedia\/*\nImages, pdfs, and other uploaded static content.\n\n\n_config.yml\nSite configuration options.\n\n\n\nContributing\nIf you discover typographic errors, bugs, or have problems navigating this site please consider opening a new issue. A brief summary of the problem along with suggestions for improvement are welcome.\nPull requests are also welcome if you would like to contribute or edit page content. Prose.io is a quick and convenient way to edit content in Markdown.\n","207":"Environmentalist\n\n\n\nA simple Node.js app to be used with Slack for managing environment occupancy.\nUsage\nPlease see the wiki for quick start tutorial. Wiki contains all the\ndetails how to install and setup the app.\nInstallation\nInstalling environmentalist.js javascript library is super simple. Use the following command in your terminal:\nnpm install environmentalist.js\n","208":"electric-imp-environmentals\nA demo project to collect, store and chart environmental data from an Electric Imp device using Node.js, Angular and MongoDB.\nRequirements\n\nAn Electric Imp dev kit with an Environmental Tail\nNode.js environment with a MongoDB database.\nKnowledge of Javascript\/Node.js environments\n\nServer Setup\nThe server must be publicly accessible.\n\nnpm update\nbower update\nnode server.js\n\nDevice Setup\n\nLogin at https:\/\/ide.electricimp.com\nRegister your Electric Imp device.\nInstall agent.nut and device.nut.\nUpdate agent.nut for your environment.\n\n\/\/ Begin - Settings\nconst serverUrl = \"http:\/\/yourhost.com\";\nconst WUNDERGROUND_API_KEY = \"xxxxxxxxxx\";\nconst WUNDERGROUND_LOCATION = \"NY\/Albany\";\n\/\/ End - Settings\n\n","209":"electric-imp-environmentals\nA demo project to collect, store and chart environmental data from an Electric Imp device using Node.js, Angular and MongoDB.\nRequirements\n\nAn Electric Imp dev kit with an Environmental Tail\nNode.js environment with a MongoDB database.\nKnowledge of Javascript\/Node.js environments\n\nServer Setup\nThe server must be publicly accessible.\n\nnpm update\nbower update\nnode server.js\n\nDevice Setup\n\nLogin at https:\/\/ide.electricimp.com\nRegister your Electric Imp device.\nInstall agent.nut and device.nut.\nUpdate agent.nut for your environment.\n\n\/\/ Begin - Settings\nconst serverUrl = \"http:\/\/yourhost.com\";\nconst WUNDERGROUND_API_KEY = \"xxxxxxxxxx\";\nconst WUNDERGROUND_LOCATION = \"NY\/Albany\";\n\/\/ End - Settings\n\n","210":"Environmental-instrument\nThe Environmental Instrument translates data from its surroundings into frequencies. This project was conceived when my composition class instructor assigned a \"non-standard notation piece that incorporated and element of randomness\". The instrument does this by using three sensors that control separate waveforms.: a light sensor; a temperature sensor,;and a motion sensor. The light sensor controls the most prevalent frequency, meaning that more light will equal higher frequencies, and less light will equal lower frequencies. Similarly, the temperature sensor changes its frequency positively with with increased temperatures, and negatively with decreased temperatures. The pitch, roll, and yaw all control separate frequencies as well. Again, a negative or positive pitch, roll, and yaw, will control the frequencies respectively. These sensors are read and turned into frequencies by using a Teensy 3.2, and the Teensy audio library.\nAn example of this piece being performed can be seen here on my Soundcloud account, Kokonama.\n"}}